{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 Welcome \u00b6 Welcome to my DOCS website. What \u00b6 In this wiki website, you will find documentation ranging from Linux and Networking to AWS Cloud and DevOps tools that I use on a daily basis. Why \u00b6 While I navigate all the DevOps and Cloud resources online, I intend to document what I learnt as a wiki of all things needed for a Cloud DevOps Engineer. Neither do I claim copyrights to any of the material here as this is only for educational purposes nor I claim this material as my own. You can find setup and support documentation for this website and my other wesbites.","title":"Home"},{"location":"#home","text":"","title":"Home"},{"location":"#welcome","text":"Welcome to my DOCS website.","title":"Welcome"},{"location":"#what","text":"In this wiki website, you will find documentation ranging from Linux and Networking to AWS Cloud and DevOps tools that I use on a daily basis.","title":"What"},{"location":"#why","text":"While I navigate all the DevOps and Cloud resources online, I intend to document what I learnt as a wiki of all things needed for a Cloud DevOps Engineer. Neither do I claim copyrights to any of the material here as this is only for educational purposes nor I claim this material as my own. You can find setup and support documentation for this website and my other wesbites.","title":"Why"},{"location":"AWS/aws-developer-cert/","text":"AWS Certfied Developer Associate (DVA-C01) \u00b6 IAM: Identity and Access Management \u00b6 Allows you to manage users and also manage their level of access to the AWS Console. IAM provides: centralized control over your AWS account. shared access to your AWS account. granular permissions. You can enable different levels of access to different users within your organization Enables identity federation with Active Directory, Facebook, and LinkedIn, et cetera. Allows multifactor authentication. Provides temporary access for users or devices and services as necessary, for example, a web or mobile phone application. Allows you to set up your own password rotation policy. Integrates with many different AWS services. IAM Federation: Integrate their own repository of users with IAM using SAML standard Supports PCI DSS compliance for many applications. Users \u00b6 End user accessing the services Groups \u00b6 Functions (admin, devops) or teams (engineering, design) that contain a group of users. Roles \u00b6 Internal usage within AWS resources to define a set of permissions. Roles are a secure way to grant permissions to entities you trust that are valid for short durations making them more secure. Similar to - IAM User in another account. - Application code running on an EC2 instance that needs to perform actions on AWS resources - An AWS service that needs to act on resources in your account to provide its features. - users from a corporate directory who use identity federation with SAML. Policies (JSON documents) \u00b6 Defines what each of the Users/Groups/Roles can and cannot do and can be attached to either a user, a group, or a role They define the permissions for an action regardless of the method that you use to perform the operation. Policy types \u00b6 Identity-based policies Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. Resource-based policies Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to a principal entity that is specified in the policy. Principals can be in the same account as the resource or in other accounts. Permissions boundaries Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. Organizations SCPs Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions. Access control lists (ACLs) Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal entity. ACLs cannot grant permissions to entities within the same account. Session policies Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user's identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. IAM Policy Simulator \u00b6 When creating new custom policies you can test it here: https://policysim.aws.amazon.com/home/index.jsp This policy tool can you save you time in case your custom policy statement's permission is denied Alternatively, you can use the CLI: Some AWS CLI commands (not all) contain --dry-run option to simulate API calls. This can be used to test permissions. If the command is successful, you'll get the message: Request would have succeeded, but DryRun flag is set Otherwise, you'll be getting the message: An error occurred (UnauthorizedOperation) when calling the {policy_name} operation IAM Best practices \u00b6 One IAM User per person only One IAM Role per Application IAM credentials should never be shared Never write IAM credentials in your code. Never use the ROOT account except for initial setup It's best to give users the minimal amount of permissions to perform their job. EC2 Instances \u00b6 By default, your EC2 instance comes with: A private IP for the internal AWS Network A public IP for the Internet If your machine is stopped and then restarted, the public IP will change unless Elastic IP is used. EC2 User Data \u00b6 Bootstrapping means launching commands when a machine starts It is possible to bootstrap our instances using an EC2 User data script That script is only run once at the instance first start Purpose: Ec2 data is used to automate boot tasks such as: Installing updates Installing software Downloading common files from the internet The EC2 User Data Script runs with the root user EC2 Meta Data \u00b6 Information about your EC2 instance It allows EC2 isntances to learn about themselves without having to use an IAM role for that purpose You can retrieve IAM roles from the metadata but not IAM policies URL: 169.254.169.254/latest/meta-data EC2 Instance Launch Types \u00b6 On Demand Instances : short workload, predictable pricing Reserved Instances : long workloads (>= 1 year) Convertible Reserved Instances : long workloads with flexible instances Scheduled Reserved Instances : launch within time window you reserve Spot Instances : short workloads, for cheap, can lose instances Dedicated Instances : no other customers will share your hardware Dedicated Hosts : book an entire physical server, control instance placement On-Demand Instance \u00b6 Pay for what you use Has the highest cost but no upfront payment No long term commitment Linux instances are by the second whereas Windows instances are currently by the hour. Good for applications with short-term or spiky or unpredictable workloads that cannot be interrupted Reserved Instances \u00b6 Up to 75% off compared to On-demand Pay upfront for what you use with long term commitment Reservation period can be 1 or 3 years Reserve a specific instance type Recommended for steady state usage applications like database systems Convertible Reserved Instances \u00b6 Can change the EC2 instance type Up to 54% discount compared to On-Demand Scheduled Reserved Instances \u00b6 Launch within time window you reserve When you require a fraction of a day / week / month Spot Instances \u00b6 Can get a discount of up to 90% compared to On-demand You bid a price and get the instance as long as its under the price Price varies based on offer and demand Spot instances are reclaimed within a 2 minute notification warning when the spot price goes above your bid Used for batch jobs, Big Data analysis, or workloads that are resilient to failures Not great for critical jobs or databases Dedicated Instances \u00b6 Instances running on hardware that\u2019s dedicated to you May share hardware with other instances in same account No control over instance placement (can move hardware after stop / start) Dedicated Hosts \u00b6 Physical dedicated EC2 server for your use Full control of Ec2 Instance placement Visibility into the underlying sockets / physical cores of the hardware Allocated for your account for a 3 year period reservation Expensive Useful for software that have a complicated licensing model (Bring your own License) Or for a companies that have strong regulatory or compliance needs Which host is right for me \u00b6 On demand: coming and staying in resort whenever we like, we pay the full price Reserved: like planning ahead and if we plan to stay for a long time, we may get a good discount. Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms.You can get kicked out at any time Dedicated Hosts: We book an entire building of the resort EC2 Pricing \u00b6 EC2 instances prices (per hour) varies based on these parameters: Region you are in Instance Type you\u2019re using On-Demand vs Spot vs Reserved vs Dedicated Host Linux vs Windows vs Private OS (RHEL, Windows) You are billed by the second, with a minimum of 60 seconds. You also pay for other factors such as storage, data transfer, fixed IP public addresses, load balancing You do not pay for the instance if the instance is stopped AMI \u00b6 AWS comes with base images such as: Ubuntu Fedora RedHat Windows These images can be customized at runtime using EC2 User data AMI \u2013 an image that can be customized by the user and used to create instances AMIs can be built for Linux or Windows machines AMI are built for a specific AWS region Using a custom built AMI can provide the following advantages: Pre-installed packages needed Faster boot time (no need for long ec2 user data at boot time Machine comes configured with monitoring / enterprise software Security concerns \u2013 control over the machines in the network Control of maintenance and updates of AMIs over time Active Directory Integration out of the box Installing your app ahead of time (for faster deploys when auto-scaling) Using someone else\u2019s AMI that is optimized for running an app, DB, etc. EC2 Instances Overview \u00b6 Instances have 5 distinct characteristics advertised on the website: The RAM (type, amount, generation) The CPU (type, make, frequency, generation, number of cores) The I/O (disk performance, EBS optimisations) The Network (network bandwidth, network latency The Graphical Processing Unit (GPU) It may be daunting to choose the right instance type https://ec2instances.info/ can help with summarizing the types of instances R/C/P/G/H/X/I/F/Z/CR are specialised in RAM, CPU, I/O, Network, GPU Dr. McGift Phx \u00b6 F1 - field programmable gate array use case for this is genomic research, financial analytics, real time video processing big data I3 - high speed storage NosQL databases, data warehousing G3 - Graphic intensive Video encoding, 3D application streaming H1 - High Disk Throughput Mapreduce workloads, distributed filesystems such as HDFS, MapR-FS T2 - Lowest cost general purpose computing Webservers, small database servers D2 - Dense storage File servers, data warehousing, Hadoop R4 - Memory optimized M5 - General purposed and balanced For application servers C5 - Compute optimized CPU intensive apps and database servers P3 - Graphics or general purpose GPU Machine learning, bitcoin mining X1 - Memory optimized SAP HANA, Apache Spark M instance types are balanced (Main purpose) T2/T3 instance types are \u201cburstable\u201d Burstable Instances (T2) \u00b6 AWS has the concept of burstable instances (T2 machines) Burst means that overall, the instance has OK CPU performance. When the machine needs to process something unexpected (a spike in load for example), it can burst, and CPU can be VERY good. If the machine bursts, it utilizes \u201cburst credits\u201d If all the credits are gone, the CPU becomes BAD If the machine stops bursting, credits are accumulated over time Burstable instances can be amazing to handle unexpected traffic and getting the insurance that it will be handled correctly If your instance consistently runs low on credit, you need to move to a different kind of non-burstable instance. T2 Unlimited \u00b6 Nov 2017: It is possible to have an unlimited burst credit balance You pay extra money if you go over your credit balance, but you don\u2019t lose in performance Overall, it is a new offering, so be careful, costs could go high if you\u2019re not monitoring the health of your instances EBS Volume \u00b6 An EC2 machine loses its root device volume (main drive) when it is manually terminated. Unexpected terminations might happen from time to time and you need a way to store your instance data somewhere An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run. It allows your instances to persist data. Once attached, you create a file system on top of these volumes, and you can run a database or you can install applications or you can store files on there, etc. It\u2019s a network drive (Not a physical drive) It uses the network to communicate the instance, which means there might be a bit of latency It can be detached from an EC2 instance and attached to another one quickly It is locked to an Availability Zone (AZ) An EBS Volume in us-east-1a cannot be attached to us-east-1b An EBS volume doesn't exist on just one physical disk. It is actually spread across an availability zone. To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GB and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time EBS Volume Types \u00b6 EBS Volumes come in 4 types and are characterized in Size | Throughput | IOPS General Purpose - GP2 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads. You get a ratio of 3 IOPS per GB with up to 10,000 IOPS Ability to burst up to 3,000 IOPS for extended periods of time for volumes at 3,334 GB and above. Provisioned IOPS - IO1 (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads For things like intensive applications, relational databases, NoSQL databases. Use this when you need more than 10,000 IOPS. Can provision up to 20,000 IOPS per volume. Throughput Optimized - ST1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads Used for BigData, data warehousing, log processing, etc. Cannot be boot volume. Cold Hard disk Drive - SC1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads Used for File server. Cannot be boot volume. EBS Volume Resizing \u00b6 Feb 2017: You can resize your EBS Volumes After resizing an EBS volume, you need to repartition your drive EBS Snapshots \u00b6 EBS Volumes can be backed up using Snapshots. Snapshots only take the actual space of the blocks on the volume If you snapshot a 100 GB drive that only has 5 GB of data, then your EBS snapshot will only be 5 GB Snapshots are used for: Backups : ensuring you can save your data in case of catastrophe Volume migration Resizing a volume down Changing the volume type Encrypting a volume EBS Encryption \u00b6 When you create an encrypted EBS volume, you get the following: Data at rest is encrypted inside the volume All the data in flight moving between the instance and the volume is encrypted All snapshots are encrypted All volumes created from the snapshots are encrypted Encryption and decryption are handled transparently Encryption has a minimal impact on latency EBS Encryption leverages keys from KMS (AES-256) Copying an unencrypted snapshot allows encryption EBS vs. Instance Store \u00b6 Some instance do not come with Root EBS volumes Instead, they come with Instance Store Instance store is physically attached to the machine Pros: Better I/O performance Cons: On termination, the instance store is lost You can\u2019t resize the instance store Backups must be operated by the user Overall, EBS-backed instances should fit most application workloads EBS Summary \u00b6 EBS can be attached to only one instance at a time EBS volumes are locked at the AZ level Migrating an EBS volume across AZ means first backing it up (snapshot), then recreating it in the other AZ EBS backups use IO and you shouldn\u2019t run them while your application is handling a lot of traffic Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated. (You can disable that)","title":"AWS Certfied Developer Associate (DVA-C01)"},{"location":"AWS/aws-developer-cert/#aws-certfied-developer-associate-dva-c01","text":"","title":"AWS Certfied Developer Associate (DVA-C01)"},{"location":"AWS/aws-developer-cert/#iam-identity-and-access-management","text":"Allows you to manage users and also manage their level of access to the AWS Console. IAM provides: centralized control over your AWS account. shared access to your AWS account. granular permissions. You can enable different levels of access to different users within your organization Enables identity federation with Active Directory, Facebook, and LinkedIn, et cetera. Allows multifactor authentication. Provides temporary access for users or devices and services as necessary, for example, a web or mobile phone application. Allows you to set up your own password rotation policy. Integrates with many different AWS services. IAM Federation: Integrate their own repository of users with IAM using SAML standard Supports PCI DSS compliance for many applications.","title":"IAM: Identity and Access Management"},{"location":"AWS/aws-developer-cert/#users","text":"End user accessing the services","title":"Users"},{"location":"AWS/aws-developer-cert/#groups","text":"Functions (admin, devops) or teams (engineering, design) that contain a group of users.","title":"Groups"},{"location":"AWS/aws-developer-cert/#roles","text":"Internal usage within AWS resources to define a set of permissions. Roles are a secure way to grant permissions to entities you trust that are valid for short durations making them more secure. Similar to - IAM User in another account. - Application code running on an EC2 instance that needs to perform actions on AWS resources - An AWS service that needs to act on resources in your account to provide its features. - users from a corporate directory who use identity federation with SAML.","title":"Roles"},{"location":"AWS/aws-developer-cert/#policies-json-documents","text":"Defines what each of the Users/Groups/Roles can and cannot do and can be attached to either a user, a group, or a role They define the permissions for an action regardless of the method that you use to perform the operation.","title":"Policies (JSON documents)"},{"location":"AWS/aws-developer-cert/#policy-types","text":"Identity-based policies Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity. Resource-based policies Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to a principal entity that is specified in the policy. Principals can be in the same account as the resource or in other accounts. Permissions boundaries Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity. Organizations SCPs Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions. Access control lists (ACLs) Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal entity. ACLs cannot grant permissions to entities within the same account. Session policies Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user's identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions.","title":"Policy types"},{"location":"AWS/aws-developer-cert/#iam-policy-simulator","text":"When creating new custom policies you can test it here: https://policysim.aws.amazon.com/home/index.jsp This policy tool can you save you time in case your custom policy statement's permission is denied Alternatively, you can use the CLI: Some AWS CLI commands (not all) contain --dry-run option to simulate API calls. This can be used to test permissions. If the command is successful, you'll get the message: Request would have succeeded, but DryRun flag is set Otherwise, you'll be getting the message: An error occurred (UnauthorizedOperation) when calling the {policy_name} operation","title":"IAM Policy Simulator"},{"location":"AWS/aws-developer-cert/#iam-best-practices","text":"One IAM User per person only One IAM Role per Application IAM credentials should never be shared Never write IAM credentials in your code. Never use the ROOT account except for initial setup It's best to give users the minimal amount of permissions to perform their job.","title":"IAM Best practices"},{"location":"AWS/aws-developer-cert/#ec2-instances","text":"By default, your EC2 instance comes with: A private IP for the internal AWS Network A public IP for the Internet If your machine is stopped and then restarted, the public IP will change unless Elastic IP is used.","title":"EC2 Instances"},{"location":"AWS/aws-developer-cert/#ec2-user-data","text":"Bootstrapping means launching commands when a machine starts It is possible to bootstrap our instances using an EC2 User data script That script is only run once at the instance first start Purpose: Ec2 data is used to automate boot tasks such as: Installing updates Installing software Downloading common files from the internet The EC2 User Data Script runs with the root user","title":"EC2 User Data"},{"location":"AWS/aws-developer-cert/#ec2-meta-data","text":"Information about your EC2 instance It allows EC2 isntances to learn about themselves without having to use an IAM role for that purpose You can retrieve IAM roles from the metadata but not IAM policies URL: 169.254.169.254/latest/meta-data","title":"EC2 Meta Data"},{"location":"AWS/aws-developer-cert/#ec2-instance-launch-types","text":"On Demand Instances : short workload, predictable pricing Reserved Instances : long workloads (>= 1 year) Convertible Reserved Instances : long workloads with flexible instances Scheduled Reserved Instances : launch within time window you reserve Spot Instances : short workloads, for cheap, can lose instances Dedicated Instances : no other customers will share your hardware Dedicated Hosts : book an entire physical server, control instance placement","title":"EC2 Instance Launch Types"},{"location":"AWS/aws-developer-cert/#on-demand-instance","text":"Pay for what you use Has the highest cost but no upfront payment No long term commitment Linux instances are by the second whereas Windows instances are currently by the hour. Good for applications with short-term or spiky or unpredictable workloads that cannot be interrupted","title":"On-Demand Instance"},{"location":"AWS/aws-developer-cert/#reserved-instances","text":"Up to 75% off compared to On-demand Pay upfront for what you use with long term commitment Reservation period can be 1 or 3 years Reserve a specific instance type Recommended for steady state usage applications like database systems","title":"Reserved Instances"},{"location":"AWS/aws-developer-cert/#convertible-reserved-instances","text":"Can change the EC2 instance type Up to 54% discount compared to On-Demand","title":"Convertible Reserved Instances"},{"location":"AWS/aws-developer-cert/#scheduled-reserved-instances","text":"Launch within time window you reserve When you require a fraction of a day / week / month","title":"Scheduled Reserved Instances"},{"location":"AWS/aws-developer-cert/#spot-instances","text":"Can get a discount of up to 90% compared to On-demand You bid a price and get the instance as long as its under the price Price varies based on offer and demand Spot instances are reclaimed within a 2 minute notification warning when the spot price goes above your bid Used for batch jobs, Big Data analysis, or workloads that are resilient to failures Not great for critical jobs or databases","title":"Spot Instances"},{"location":"AWS/aws-developer-cert/#dedicated-instances","text":"Instances running on hardware that\u2019s dedicated to you May share hardware with other instances in same account No control over instance placement (can move hardware after stop / start)","title":"Dedicated Instances"},{"location":"AWS/aws-developer-cert/#dedicated-hosts","text":"Physical dedicated EC2 server for your use Full control of Ec2 Instance placement Visibility into the underlying sockets / physical cores of the hardware Allocated for your account for a 3 year period reservation Expensive Useful for software that have a complicated licensing model (Bring your own License) Or for a companies that have strong regulatory or compliance needs","title":"Dedicated Hosts"},{"location":"AWS/aws-developer-cert/#which-host-is-right-for-me","text":"On demand: coming and staying in resort whenever we like, we pay the full price Reserved: like planning ahead and if we plan to stay for a long time, we may get a good discount. Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms.You can get kicked out at any time Dedicated Hosts: We book an entire building of the resort","title":"Which host is right for me"},{"location":"AWS/aws-developer-cert/#ec2-pricing","text":"EC2 instances prices (per hour) varies based on these parameters: Region you are in Instance Type you\u2019re using On-Demand vs Spot vs Reserved vs Dedicated Host Linux vs Windows vs Private OS (RHEL, Windows) You are billed by the second, with a minimum of 60 seconds. You also pay for other factors such as storage, data transfer, fixed IP public addresses, load balancing You do not pay for the instance if the instance is stopped","title":"EC2 Pricing"},{"location":"AWS/aws-developer-cert/#ami","text":"AWS comes with base images such as: Ubuntu Fedora RedHat Windows These images can be customized at runtime using EC2 User data AMI \u2013 an image that can be customized by the user and used to create instances AMIs can be built for Linux or Windows machines AMI are built for a specific AWS region Using a custom built AMI can provide the following advantages: Pre-installed packages needed Faster boot time (no need for long ec2 user data at boot time Machine comes configured with monitoring / enterprise software Security concerns \u2013 control over the machines in the network Control of maintenance and updates of AMIs over time Active Directory Integration out of the box Installing your app ahead of time (for faster deploys when auto-scaling) Using someone else\u2019s AMI that is optimized for running an app, DB, etc.","title":"AMI"},{"location":"AWS/aws-developer-cert/#ec2-instances-overview","text":"Instances have 5 distinct characteristics advertised on the website: The RAM (type, amount, generation) The CPU (type, make, frequency, generation, number of cores) The I/O (disk performance, EBS optimisations) The Network (network bandwidth, network latency The Graphical Processing Unit (GPU) It may be daunting to choose the right instance type https://ec2instances.info/ can help with summarizing the types of instances R/C/P/G/H/X/I/F/Z/CR are specialised in RAM, CPU, I/O, Network, GPU","title":"EC2 Instances Overview"},{"location":"AWS/aws-developer-cert/#dr-mcgift-phx","text":"F1 - field programmable gate array use case for this is genomic research, financial analytics, real time video processing big data I3 - high speed storage NosQL databases, data warehousing G3 - Graphic intensive Video encoding, 3D application streaming H1 - High Disk Throughput Mapreduce workloads, distributed filesystems such as HDFS, MapR-FS T2 - Lowest cost general purpose computing Webservers, small database servers D2 - Dense storage File servers, data warehousing, Hadoop R4 - Memory optimized M5 - General purposed and balanced For application servers C5 - Compute optimized CPU intensive apps and database servers P3 - Graphics or general purpose GPU Machine learning, bitcoin mining X1 - Memory optimized SAP HANA, Apache Spark M instance types are balanced (Main purpose) T2/T3 instance types are \u201cburstable\u201d","title":"Dr. McGift Phx"},{"location":"AWS/aws-developer-cert/#burstable-instances-t2","text":"AWS has the concept of burstable instances (T2 machines) Burst means that overall, the instance has OK CPU performance. When the machine needs to process something unexpected (a spike in load for example), it can burst, and CPU can be VERY good. If the machine bursts, it utilizes \u201cburst credits\u201d If all the credits are gone, the CPU becomes BAD If the machine stops bursting, credits are accumulated over time Burstable instances can be amazing to handle unexpected traffic and getting the insurance that it will be handled correctly If your instance consistently runs low on credit, you need to move to a different kind of non-burstable instance.","title":"Burstable Instances (T2)"},{"location":"AWS/aws-developer-cert/#t2-unlimited","text":"Nov 2017: It is possible to have an unlimited burst credit balance You pay extra money if you go over your credit balance, but you don\u2019t lose in performance Overall, it is a new offering, so be careful, costs could go high if you\u2019re not monitoring the health of your instances","title":"T2 Unlimited"},{"location":"AWS/aws-developer-cert/#ebs-volume","text":"An EC2 machine loses its root device volume (main drive) when it is manually terminated. Unexpected terminations might happen from time to time and you need a way to store your instance data somewhere An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run. It allows your instances to persist data. Once attached, you create a file system on top of these volumes, and you can run a database or you can install applications or you can store files on there, etc. It\u2019s a network drive (Not a physical drive) It uses the network to communicate the instance, which means there might be a bit of latency It can be detached from an EC2 instance and attached to another one quickly It is locked to an Availability Zone (AZ) An EBS Volume in us-east-1a cannot be attached to us-east-1b An EBS volume doesn't exist on just one physical disk. It is actually spread across an availability zone. To move a volume across, you first need to snapshot it Have a provisioned capacity (size in GB and IOPS) You get billed for all the provisioned capacity You can increase the capacity of the drive over time","title":"EBS Volume"},{"location":"AWS/aws-developer-cert/#ebs-volume-types","text":"EBS Volumes come in 4 types and are characterized in Size | Throughput | IOPS General Purpose - GP2 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads. You get a ratio of 3 IOPS per GB with up to 10,000 IOPS Ability to burst up to 3,000 IOPS for extended periods of time for volumes at 3,334 GB and above. Provisioned IOPS - IO1 (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads For things like intensive applications, relational databases, NoSQL databases. Use this when you need more than 10,000 IOPS. Can provision up to 20,000 IOPS per volume. Throughput Optimized - ST1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads Used for BigData, data warehousing, log processing, etc. Cannot be boot volume. Cold Hard disk Drive - SC1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads Used for File server. Cannot be boot volume.","title":"EBS Volume Types"},{"location":"AWS/aws-developer-cert/#ebs-volume-resizing","text":"Feb 2017: You can resize your EBS Volumes After resizing an EBS volume, you need to repartition your drive","title":"EBS Volume Resizing"},{"location":"AWS/aws-developer-cert/#ebs-snapshots","text":"EBS Volumes can be backed up using Snapshots. Snapshots only take the actual space of the blocks on the volume If you snapshot a 100 GB drive that only has 5 GB of data, then your EBS snapshot will only be 5 GB Snapshots are used for: Backups : ensuring you can save your data in case of catastrophe Volume migration Resizing a volume down Changing the volume type Encrypting a volume","title":"EBS Snapshots"},{"location":"AWS/aws-developer-cert/#ebs-encryption","text":"When you create an encrypted EBS volume, you get the following: Data at rest is encrypted inside the volume All the data in flight moving between the instance and the volume is encrypted All snapshots are encrypted All volumes created from the snapshots are encrypted Encryption and decryption are handled transparently Encryption has a minimal impact on latency EBS Encryption leverages keys from KMS (AES-256) Copying an unencrypted snapshot allows encryption","title":"EBS Encryption"},{"location":"AWS/aws-developer-cert/#ebs-vs-instance-store","text":"Some instance do not come with Root EBS volumes Instead, they come with Instance Store Instance store is physically attached to the machine Pros: Better I/O performance Cons: On termination, the instance store is lost You can\u2019t resize the instance store Backups must be operated by the user Overall, EBS-backed instances should fit most application workloads","title":"EBS vs. Instance Store"},{"location":"AWS/aws-developer-cert/#ebs-summary","text":"EBS can be attached to only one instance at a time EBS volumes are locked at the AZ level Migrating an EBS volume across AZ means first backing it up (snapshot), then recreating it in the other AZ EBS backups use IO and you shouldn\u2019t run them while your application is handling a lot of traffic Root EBS Volumes of instances get terminated by default if the EC2 instance gets terminated. (You can disable that)","title":"EBS Summary"},{"location":"AWS/aws-ecs/","text":"Amazon Elastic Container Service \u00b6 ECS Clusters \u00b6 To create a cluster aws> ecs create-cluster --cluster-name deepdive Here is the output ------------------------------------------------------------------------------------------------ | CreateCluster | +----------------------------------------------------------------------------------------------+ || cluster || |+------------------------------------+-------------------------------------------------------+| || activeServicesCount | 0 || || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || clusterName | deepdive || || pendingTasksCount | 0 || || registeredContainerInstancesCount | 0 || || runningTasksCount | 0 || || status | ACTIVE || |+------------------------------------+-------------------------------------------------------+| ||| settings ||| ||+--------------------------+---------------------------------------------------------------+|| ||| name | containerInsights ||| ||| value | disabled ||| ||+--------------------------+---------------------------------------------------------------+|| To list clusters aws> ecs list-clusters Here is the output ----------------------------------------------------------- | ListClusters | +---------------------------------------------------------+ || clusterArns || |+-------------------------------------------------------+| || arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || |+-------------------------------------------------------+| To describe a specific cluster aws> ecs describe-clusters --clusters deepdive Here is the output ------------------------------------------------------------------------------------------------ | DescribeClusters | +----------------------------------------------------------------------------------------------+ || clusters || |+------------------------------------+-------------------------------------------------------+| || activeServicesCount | 0 || || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || clusterName | deepdive || || pendingTasksCount | 0 || || registeredContainerInstancesCount | 0 || || runningTasksCount | 0 || || status | ACTIVE || |+------------------------------------+-------------------------------------------------------+| ||| settings ||| ||+--------------------------+---------------------------------------------------------------+|| ||| name | containerInsights ||| ||| value | disabled ||| ||+--------------------------+---------------------------------------------------------------+|| To delete a cluster aws> ecs delete-cluster --cluster deepdive ECS Container Agent \u00b6 Here is an outline of how the AWS ECS objects relate: A container agent is ran on an EC2 instance and it helps EC2 instance in joining a cluster. The container agent is open-source and can be found here . The agent runs on ECS optimized or custom AMIs. On DockerHub - here There are optional configuration values . Status reports are available when ecs container agent is installed on the EC2 instance. Create s3 bucket aws> s3api create-bucket --bucket deepu-bucket-ecs-deepdive -------------------------------------------- | CreateBucket | +-----------+------------------------------+ | Location | /deepu-bucket-ecs-deepdive | +-----------+------------------------------+ aws> s3 cp ecs.config s3://deepu-bucket-ecs-deepdive upload: ./ecs.config to s3://deepu-bucket-ecs-deepdive/ecs.config aws> s3 ls s3://deepu-bucket-ecs-deepdive 2019 -09-03 22 :17:09 21 ecs.config ECS Container Instance \u00b6 A container instance is an EC2 instance registered to a cluster. Connects via a container agent. LifeCycle States - ACTIVE and connected (registered container instance and connection status is TRUE - Container instance is UP.) - ACTIVE and disconnected (Stopped container instance) - INACTIVE (de-registered container instance and will not be seen as part of the cluster) aws> ec2 run-instances --image-id ami-0b16d80945b1a9c7d --count 1 --instance-type t2.micro --iam-instance-profile Name = ecsInstanceRole --key-name ~/Documents/pem-keys/elk-stack.pem --security-group-ids sg-02909c0c9fb1ace0e --user-data file://copy-ecs-config-to-s3 aws> ecs list-container-instances --cluster deepdive -------------------------------------------------------------------------------------------------- | ListContainerInstances | +------------------------------------------------------------------------------------------------+ || containerInstanceArns || |+----------------------------------------------------------------------------------------------+| || arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 || |+----------------------------------------------------------------------------------------------+| aws> ecs describe-container-instances --cluster deepdive --container-instances arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | DescribeContainerInstances | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || containerInstances || |+----------------+----------------------------------------------------------------------------------------------+----------------------+--------------------+-----------------+--------------------+---------+-----------+| || agentConnected | containerInstanceArn | ec2InstanceId | pendingTasksCount | registeredAt | runningTasksCount | status | version || |+----------------+----------------------------------------------------------------------------------------------+----------------------+--------------------+-----------------+--------------------+---------+-----------+| || True | arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 | i-0bccc3b7f5f2fa1bf | 0 | 1567565218.739 | 0 | ACTIVE | 3 || |+----------------+----------------------------------------------------------------------------------------------+----------------------+--------------------+-----------------+--------------------+---------+-----------+| ||| attributes ||| ||+----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+|| ||| name | value ||| ||+----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+|| ||| ecs.capability.secrets.asm.environment-variables | ||| ||| ecs.capability.branch-cni-plugin-version | cdd89b92- ||| ||| ecs.ami-id | ami-0b16d80945b1a9c7d ||| ||| ecs.capability.secrets.asm.bootstrap.log-driver | ||| ||| ecs.capability.task-eia.optimized-cpu | ||| ||| com.amazonaws.ecs.capability.logging-driver.none | ||| ||| ecs.capability.ecr-endpoint | ||| ||| ecs.capability.docker-plugin.local | ||| ||| ecs.capability.task-cpu-mem-limit | ||| ||| ecs.capability.secrets.ssm.bootstrap.log-driver | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.30 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.31 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.32 | ||| ||| ecs.availability-zone | us-east-1d ||| ||| ecs.capability.aws-appmesh | ||| ||| com.amazonaws.ecs.capability.logging-driver.awslogs | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.24 | ||| ||| ecs.capability.task-eni-trunking | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.25 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.26 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.27 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.28 | ||| ||| com.amazonaws.ecs.capability.privileged-container | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.29 | ||| ||| ecs.cpu-architecture | x86_64 ||| ||| ecs.capability.firelens.fluentbit | ||| ||| com.amazonaws.ecs.capability.ecr-auth | ||| ||| ecs.os-type | linux ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.20 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.21 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.22 | ||| ||| ecs.capability.private-registry-authentication.secretsmanager | ||| ||| ecs.capability.task-eia | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.23 | ||| ||| com.amazonaws.ecs.capability.logging-driver.syslog | ||| ||| com.amazonaws.ecs.capability.logging-driver.awsfirelens | ||| ||| com.amazonaws.ecs.capability.logging-driver.json-file | ||| ||| ecs.capability.execution-role-awslogs | ||| ||| ecs.vpc-id | vpc-8954fcf3 ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.17 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.18 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.19 | ||| ||| ecs.capability.task-eni | ||| ||| ecs.capability.firelens.fluentd | ||| ||| ecs.capability.execution-role-ecr-pull | ||| ||| ecs.capability.container-health-check | ||| ||| ecs.subnet-id | subnet-82c45eac ||| ||| ecs.instance-type | t2.micro ||| ||| com.amazonaws.ecs.capability.task-iam-role-network-host | ||| ||| ecs.capability.container-ordering | ||| ||| ecs.capability.cni-plugin-version | 91ccefc8-2019.06.0 ||| ||| ecs.capability.pid-ipc-namespace-sharing | ||| ||| ecs.capability.secrets.ssm.environment-variables | ||| ||| com.amazonaws.ecs.capability.task-iam-role | ||| ||+----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+|| ||| registeredResources ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| 0.0 | 1024 | 0 | CPU | INTEGER ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| registeredResources ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| 0.0 | 983 | 0 | MEMORY | INTEGER ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| registeredResources ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS | STRINGSET ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| |||| stringSetValue |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| |||| 22 |||| |||| 2376 |||| |||| 2375 |||| |||| 51678 |||| |||| 51679 |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| ||| registeredResources ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS_UDP | STRINGSET ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| remainingResources ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| 0.0 | 1024 | 0 | CPU | INTEGER ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| remainingResources ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| 0.0 | 983 | 0 | MEMORY | INTEGER ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| remainingResources ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS | STRINGSET ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| |||| stringSetValue |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| |||| 22 |||| |||| 2376 |||| |||| 2375 |||| |||| 51678 |||| |||| 51679 |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| ||| remainingResources ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS_UDP | STRINGSET ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| versionInfo ||| ||+----------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+|| ||| agentHash | agentVersion | dockerVersion ||| ||+----------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+|| ||| 02ff320c | 1.30.0 | DockerVersion: 18.06.1-ce ||| ||+----------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+|| ECS Tasks & Services \u00b6 A Task Definition is a collection of 1 or more container configurations. Some Tasks may need only one container, while other Tasks may need 2 or more potentially linked containers running concurrently. The Task definition allows you to specify which Docker image to use, which ports to expose, how much CPU and memory to allot, how to collect logs, and define environment variables. A Task is created when you run a Task directly, which launches container(s) (defined in the task definition) until they are stopped or exit on their own, at which point they are not replaced automatically. Running Tasks directly is ideal for short running jobs, perhaps as an example things that were accomplished via CRON. A Service is used to guarantee that you always have some number of Tasks running at all times. If a Task's container exits due to error, or the underlying EC2 instance fails and is replaced, the ECS Service will replace the failed Task. This is why we create Clusters so that the Service has plenty of resources in terms of CPU, Memory and Network ports to use. To us it doesn't really matter which instance Tasks run on so long as they run. A Service configuration references a Task definition. A Service is responsible for creating Tasks. Services are typically used for long running applications like web servers. For example, if I deployed my website powered by Node.JS in Oregon (us-west-2) I would want say at least three Tasks running across the three Availability Zones (AZ) for the sake of High-Availability; if one fails I have another two and the failed one will be replaced (read that as self-healing!). Creating a Service is the way to do this. If I had 6 EC2 instances in my cluster, 2 per AZ, the Service will automatically balance Tasks across zones as best it can while also considering cpu, memory, and network resources. Another very important point is that a Service can be configured to use a load balancer , so that as it creates the Tasks\u2014that is it launches containers defined in the Task Defintion\u2014the Service will automatically register the container's EC2 instance with the load balancer. Tasks cannot be configured to use a load balancer, only Services can. ECS Scheduler \u00b6 3 ways to schedule a task on your cluster The scheduler helps you utilize your resources with least effort on the user's part and in the best way possible. Scheduler is flexible and we can use the instance as well if needed and we can even use third-party scheduler. Services Services are long lived and stateless (like a Web application) Define how many service instances you want (like Autoscaling, it will sclae up whenever any instance is down.) Works with Elastic Load Balancing as well (Hook up the service to ELB) 3 steps for placing a service into your cluster Compare the task definition's attributes to the state of the cluster and then check if the container instance can run the required attributes (cpu, memory, etc.) The scheduler has a list of container instances and check how many service instances are running in an availability zone. Keeping this in mind, it will place a new instance in an availability zone that has least amount of tasks running on it Check how many service instances are running on container instances and place a new task on the container instance that has least amount of tasks running on it. Running Tasks Tasks that are short lived or 1 off tasks that exit when done can be run with Scheduler. (Video encoding or DB migration for example) We can use RunTask which randomly distributes tasks on your cluster. But, it minimizes specific instances from getting overloaded. Starting Tasks Running tasks and starting tasks are different StartTask lets you pick where you want to run a task StartTasks lets you build or use your own scheduler Services and tasks have 3 states: - PENDING - RUNNING - STOPPED The container agent is responsible for state tracking Task LifeCycle aws> ecs create-service --cluster deepdive --service-name web --task-definition web --desired-count 1 ---------------------------------------------------------------------------------------- | CreateService | +--------------------------------------------------------------------------------------+ || service || | +-----------------------+------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 1 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 0 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | +-----------------------+------------------------------------------------------------+ | || | deploymentConfiguration || | || +----------------------------------------------------------------+-----------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +----------------------------------------------------------------+-----------------+ || || | deployments || | || +-----------------+----------------------------------------------------------------+ || || | createdAt | 1567608636 .767 || | || | desiredCount | 1 || | || | id | ecs-svc/9223370469246139040 || | || | launchType | EC2 || | || | pendingCount | 0 || | || | runningCount | 0 || | || | status | PRIMARY || | || | taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | || | updatedAt | 1567608636 .767 || | || +-----------------+----------------------------------------------------------------+ || aws> ecs list-services --cluster deepdive ------------------------------------------------------ | ListServices | +----------------------------------------------------+ || serviceArns || | +--------------------------------------------------+ | || arn:aws:ecs:us-east-1:289503391411:service/web || | +--------------------------------------------------+ | aws> ecs describe-services --cluster deepdive --services web --------------------------------------------------------------------------------------------------------------------------------------------------- | DescribeServices | +-------------------------------------------------------------------------------------------------------------------------------------------------+ || services || | +---------------------------------------+-------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 1 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 1 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | +---------------------------------------+-------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +--------------------------------------------------------------------------------------------------------------+------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +--------------------------------------------------------------------------------------------------------------+------------------------------+ || || | deployments || | || +-------------------------------+-------------------------------------------------------------------------------------------------------------+ || || | createdAt | 1567608636 .767 || | || | desiredCount | 1 || | || | id | ecs-svc/9223370469246139040 || | || | launchType | EC2 || | || | pendingCount | 0 || | || | runningCount | 1 || | || | status | PRIMARY || | || | taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | || | updatedAt | 1567608650 .671 || | || +-------------------------------+-------------------------------------------------------------------------------------------------------------+ || || | events || | || +----------------+----------------------------------------+-----------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +----------------+----------------------------------------+-----------------------------------------------------------------------------------+ || || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +----------------+----------------------------------------+-----------------------------------------------------------------------------------+ || aws> ecs update-service --cluster deepdive --service web --task-definition web --desired-count 2 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | UpdateService | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || service || | +--------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 2 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 1 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || | +--------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+ || || | deployments || | || +----------------+---------------+------------------------------+-------------+---------------+---------------+----------+------------------------------------------------------------+------------------+ || || | createdAt | desiredCount | id | launchType | pendingCount | runningCount | status | taskDefinition | updatedAt || | || +----------------+---------------+------------------------------+-------------+---------------+---------------+----------+------------------------------------------------------------+------------------+ || || | 1567610184 .168 | 2 | ecs-svc/9223370469244591639 | EC2 | 0 | 0 | PRIMARY | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 | 1567610184 .168 || | || | 1567608636 .767 | 1 | ecs-svc/9223370469246139040 | EC2 | 0 | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 | 1567608650 .671 || | || +----------------+---------------+------------------------------+-------------+---------------+---------------+----------+------------------------------------------------------------+------------------+ || || | events || | || +------------------------+--------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +------------------------+--------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+ || || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +------------------------+--------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+ || To stop the service aws> ecs update-service --cluster deepdive --service web --task-definition web --desired-count 0 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | UpdateService | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || service || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 0 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 1 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | deployments || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | createdAt | desiredCount | id | launchType | pendingCount | runningCount | status | taskDefinition | updatedAt || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | 1567610184 .168 | 0 | ecs-svc/9223370469244591639 | EC2 | 0 | 0 | PRIMARY | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 | 1567610184 .168 || | || | 1567608636 .767 | 1 | ecs-svc/9223370469246139040 | EC2 | 0 | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 | 1567610184 .168 || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | events || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | 1567610200 .241 | edbdedf6-951a-4a93-aa62-358e9af36e4a | ( service web ) was unable to place a task because no container instance met all of its requirements. The closest matching ( container-instance 6368caa4-6386-4a5b-9c60-ac752591b819 ) is already using a port required by your task. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide. || | || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || aws> ecs delete-service --cluster deepdive --service web ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | DeleteService | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || service || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 0 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 0 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | DRAINING || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | deployments || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | createdAt | desiredCount | id | launchType | pendingCount | runningCount | status | taskDefinition | updatedAt || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | 1567610184 .168 | 0 | ecs-svc/9223370469244591639 | EC2 | 0 | 0 | PRIMARY | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 | 1567610345 .337 || | || | 1567608636 .767 | 1 | ecs-svc/9223370469246139040 | EC2 | 0 | 0 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 | 1567610184 .168 || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | events || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | 1567610345 .38 | affb2376-1662-48db-9289-97c56c373cc7 | ( service web ) has stopped 1 running tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || | 1567610200 .241 | edbdedf6-951a-4a93-aa62-358e9af36e4a | ( service web ) was unable to place a task because no container instance met all of its requirements. The closest matching ( container-instance 6368caa4-6386-4a5b-9c60-ac752591b819 ) is already using a port required by your task. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide. || | || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || ECS Task Definitions \u00b6 Task Definitions describe how Docker images should be ran. Each Task Definition can control 1 or more containers (with optional volumes) Applications can be 1 or more task definitions. Grouped containers run on the same instance. This can be used for minimum latency. Services are created through task definitions. Components of Task definition. - Family -> Used for versioning the Task Definition when it is modified. - Container definition - Volumes -> A way to share data between containers. aws> ecs register-task-definition --cli-input-json file://web-task-definition.json ------------------------------------------------------------------------------------------------ | RegisterTaskDefinition | +----------------------------------------------------------------------------------------------+ || taskDefinition || |+--------+-----------+---------+-------------------------------------------------------------+| || family | revision | status | taskDefinitionArn || |+--------+-----------+---------+-------------------------------------------------------------+| || web | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || |+--------+-----------+---------+-------------------------------------------------------------+| ||| compatibilities ||| ||+------------------------------------------------------------------------------------------+|| ||| EC2 ||| ||+------------------------------------------------------------------------------------------+|| ||| containerDefinitions ||| ||+-----------+------------------------+----------------+------------------+-----------------+|| ||| cpu | essential | image | memory | name ||| ||+-----------+------------------------+----------------+------------------+-----------------+|| ||| 102 | True | nginx | 50 | nginx ||| ||+-----------+------------------------+----------------+------------------+-----------------+|| |||| portMappings |||| |||+-----------------------------------+-------------------------+--------------------------+||| |||| containerPort | hostPort | protocol |||| |||+-----------------------------------+-------------------------+--------------------------+||| |||| 80 | 80 | tcp |||| |||+-----------------------------------+-------------------------+--------------------------+||| aws> ecs list-task-definition-families ---------------------------- | ListTaskDefinitionFamilies | +--------------------------+ || families || | +------------------------+ | || web || | +------------------------+ | aws> ecs list-task-definitions ---------------------------------------------------------------- | ListTaskDefinitions | +--------------------------------------------------------------+ || taskDefinitionArns || | +------------------------------------------------------------+ | || arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || | +------------------------------------------------------------+ | aws> ecs describe-task-definition --task-definition web:1 ------------------------------------------------------------------------------------------------ | DescribeTaskDefinition | +----------------------------------------------------------------------------------------------+ || taskDefinition || | +--------+-----------+---------+-------------------------------------------------------------+ | || family | revision | status | taskDefinitionArn || | +--------+-----------+---------+-------------------------------------------------------------+ | || web | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || | +--------+-----------+---------+-------------------------------------------------------------+ | || | compatibilities || | || +------------------------------------------------------------------------------------------+ || || | EC2 || | || +------------------------------------------------------------------------------------------+ || || | containerDefinitions || | || +-----------+------------------------+----------------+------------------+-----------------+ || || | cpu | essential | image | memory | name || | || +-----------+------------------------+----------------+------------------+-----------------+ || || | 102 | True | nginx | 50 | nginx || | || +-----------+------------------------+----------------+------------------+-----------------+ || |||| portMappings |||| || | +-----------------------------------+-------------------------+--------------------------+ || | |||| containerPort | hostPort | protocol |||| || | +-----------------------------------+-------------------------+--------------------------+ || | |||| 80 | 80 | tcp |||| || | +-----------------------------------+-------------------------+--------------------------+ || | After registering task definition under family web for 4 times aws> ecs list-task-definitions ---------------------------------------------------------------- | ListTaskDefinitions | +--------------------------------------------------------------+ || taskDefinitionArns || | +------------------------------------------------------------+ | || arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:2 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:3 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | +------------------------------------------------------------+ | You can deregister any task definition aws> ecs deregister-task-definition --task-definition web:3 aws> ecs list-task-definitions ---------------------------------------------------------------- | ListTaskDefinitions | +--------------------------------------------------------------+ || taskDefinitionArns || |+------------------------------------------------------------+| || arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:2 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || |+------------------------------------------------------------+| To create a task definition template aws> ecs register-task-definition --generate-cli-skeleton aws> ecs run-task --cluster deepdive --task-definition web --count 1 -------------------------------------------------------------------------------------------------------------------------- | RunTask | +------------------------------------------------------------------------------------------------------------------------+ || tasks || | +----------------------+-----------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || containerInstanceArn | arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 || || cpu | 102 || || createdAt | 1567610699 .64 || || desiredStatus | RUNNING || || group | family:web || || lastStatus | PENDING || || launchType | EC2 || || memory | 50 || || taskArn | arn:aws:ecs:us-east-1:289503391411:task/798b90f2-ffaf-4bd0-a2dd-10842dfd1d9f || || taskDefinitionArn | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || || version | 1 || | +----------------------+-----------------------------------------------------------------------------------------------+ | || | containers || | || +-----------------+--------------------------------------------------------------------------------------------------+ || || | containerArn | arn:aws:ecs:us-east-1:289503391411:container/4ccb5949-e4c6-47d7-92a8-066b9d7c4cf4 || | || | cpu | 102 || | || | lastStatus | PENDING || | || | memory | 50 || | || | name | nginx || | || | taskArn | arn:aws:ecs:us-east-1:289503391411:task/798b90f2-ffaf-4bd0-a2dd-10842dfd1d9f || | || +-----------------+--------------------------------------------------------------------------------------------------+ || || | overrides || | || +--------------------------------------------------------------------------------------------------------------------+ || |||| containerOverrides |||| || | +-----------------------------------------------------+------------------------------------------------------------+ || | |||| name | nginx |||| || | +-----------------------------------------------------+------------------------------------------------------------+ || |","title":"Amazon Elastic Container Service"},{"location":"AWS/aws-ecs/#amazon-elastic-container-service","text":"","title":"Amazon Elastic Container Service"},{"location":"AWS/aws-ecs/#ecs-clusters","text":"To create a cluster aws> ecs create-cluster --cluster-name deepdive Here is the output ------------------------------------------------------------------------------------------------ | CreateCluster | +----------------------------------------------------------------------------------------------+ || cluster || |+------------------------------------+-------------------------------------------------------+| || activeServicesCount | 0 || || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || clusterName | deepdive || || pendingTasksCount | 0 || || registeredContainerInstancesCount | 0 || || runningTasksCount | 0 || || status | ACTIVE || |+------------------------------------+-------------------------------------------------------+| ||| settings ||| ||+--------------------------+---------------------------------------------------------------+|| ||| name | containerInsights ||| ||| value | disabled ||| ||+--------------------------+---------------------------------------------------------------+|| To list clusters aws> ecs list-clusters Here is the output ----------------------------------------------------------- | ListClusters | +---------------------------------------------------------+ || clusterArns || |+-------------------------------------------------------+| || arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || |+-------------------------------------------------------+| To describe a specific cluster aws> ecs describe-clusters --clusters deepdive Here is the output ------------------------------------------------------------------------------------------------ | DescribeClusters | +----------------------------------------------------------------------------------------------+ || clusters || |+------------------------------------+-------------------------------------------------------+| || activeServicesCount | 0 || || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || clusterName | deepdive || || pendingTasksCount | 0 || || registeredContainerInstancesCount | 0 || || runningTasksCount | 0 || || status | ACTIVE || |+------------------------------------+-------------------------------------------------------+| ||| settings ||| ||+--------------------------+---------------------------------------------------------------+|| ||| name | containerInsights ||| ||| value | disabled ||| ||+--------------------------+---------------------------------------------------------------+|| To delete a cluster aws> ecs delete-cluster --cluster deepdive","title":"ECS Clusters"},{"location":"AWS/aws-ecs/#ecs-container-agent","text":"Here is an outline of how the AWS ECS objects relate: A container agent is ran on an EC2 instance and it helps EC2 instance in joining a cluster. The container agent is open-source and can be found here . The agent runs on ECS optimized or custom AMIs. On DockerHub - here There are optional configuration values . Status reports are available when ecs container agent is installed on the EC2 instance. Create s3 bucket aws> s3api create-bucket --bucket deepu-bucket-ecs-deepdive -------------------------------------------- | CreateBucket | +-----------+------------------------------+ | Location | /deepu-bucket-ecs-deepdive | +-----------+------------------------------+ aws> s3 cp ecs.config s3://deepu-bucket-ecs-deepdive upload: ./ecs.config to s3://deepu-bucket-ecs-deepdive/ecs.config aws> s3 ls s3://deepu-bucket-ecs-deepdive 2019 -09-03 22 :17:09 21 ecs.config","title":"ECS Container Agent"},{"location":"AWS/aws-ecs/#ecs-container-instance","text":"A container instance is an EC2 instance registered to a cluster. Connects via a container agent. LifeCycle States - ACTIVE and connected (registered container instance and connection status is TRUE - Container instance is UP.) - ACTIVE and disconnected (Stopped container instance) - INACTIVE (de-registered container instance and will not be seen as part of the cluster) aws> ec2 run-instances --image-id ami-0b16d80945b1a9c7d --count 1 --instance-type t2.micro --iam-instance-profile Name = ecsInstanceRole --key-name ~/Documents/pem-keys/elk-stack.pem --security-group-ids sg-02909c0c9fb1ace0e --user-data file://copy-ecs-config-to-s3 aws> ecs list-container-instances --cluster deepdive -------------------------------------------------------------------------------------------------- | ListContainerInstances | +------------------------------------------------------------------------------------------------+ || containerInstanceArns || |+----------------------------------------------------------------------------------------------+| || arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 || |+----------------------------------------------------------------------------------------------+| aws> ecs describe-container-instances --cluster deepdive --container-instances arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | DescribeContainerInstances | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || containerInstances || |+----------------+----------------------------------------------------------------------------------------------+----------------------+--------------------+-----------------+--------------------+---------+-----------+| || agentConnected | containerInstanceArn | ec2InstanceId | pendingTasksCount | registeredAt | runningTasksCount | status | version || |+----------------+----------------------------------------------------------------------------------------------+----------------------+--------------------+-----------------+--------------------+---------+-----------+| || True | arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 | i-0bccc3b7f5f2fa1bf | 0 | 1567565218.739 | 0 | ACTIVE | 3 || |+----------------+----------------------------------------------------------------------------------------------+----------------------+--------------------+-----------------+--------------------+---------+-----------+| ||| attributes ||| ||+----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+|| ||| name | value ||| ||+----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+|| ||| ecs.capability.secrets.asm.environment-variables | ||| ||| ecs.capability.branch-cni-plugin-version | cdd89b92- ||| ||| ecs.ami-id | ami-0b16d80945b1a9c7d ||| ||| ecs.capability.secrets.asm.bootstrap.log-driver | ||| ||| ecs.capability.task-eia.optimized-cpu | ||| ||| com.amazonaws.ecs.capability.logging-driver.none | ||| ||| ecs.capability.ecr-endpoint | ||| ||| ecs.capability.docker-plugin.local | ||| ||| ecs.capability.task-cpu-mem-limit | ||| ||| ecs.capability.secrets.ssm.bootstrap.log-driver | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.30 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.31 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.32 | ||| ||| ecs.availability-zone | us-east-1d ||| ||| ecs.capability.aws-appmesh | ||| ||| com.amazonaws.ecs.capability.logging-driver.awslogs | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.24 | ||| ||| ecs.capability.task-eni-trunking | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.25 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.26 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.27 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.28 | ||| ||| com.amazonaws.ecs.capability.privileged-container | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.29 | ||| ||| ecs.cpu-architecture | x86_64 ||| ||| ecs.capability.firelens.fluentbit | ||| ||| com.amazonaws.ecs.capability.ecr-auth | ||| ||| ecs.os-type | linux ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.20 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.21 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.22 | ||| ||| ecs.capability.private-registry-authentication.secretsmanager | ||| ||| ecs.capability.task-eia | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.23 | ||| ||| com.amazonaws.ecs.capability.logging-driver.syslog | ||| ||| com.amazonaws.ecs.capability.logging-driver.awsfirelens | ||| ||| com.amazonaws.ecs.capability.logging-driver.json-file | ||| ||| ecs.capability.execution-role-awslogs | ||| ||| ecs.vpc-id | vpc-8954fcf3 ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.17 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.18 | ||| ||| com.amazonaws.ecs.capability.docker-remote-api.1.19 | ||| ||| ecs.capability.task-eni | ||| ||| ecs.capability.firelens.fluentd | ||| ||| ecs.capability.execution-role-ecr-pull | ||| ||| ecs.capability.container-health-check | ||| ||| ecs.subnet-id | subnet-82c45eac ||| ||| ecs.instance-type | t2.micro ||| ||| com.amazonaws.ecs.capability.task-iam-role-network-host | ||| ||| ecs.capability.container-ordering | ||| ||| ecs.capability.cni-plugin-version | 91ccefc8-2019.06.0 ||| ||| ecs.capability.pid-ipc-namespace-sharing | ||| ||| ecs.capability.secrets.ssm.environment-variables | ||| ||| com.amazonaws.ecs.capability.task-iam-role | ||| ||+----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------+|| ||| registeredResources ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| 0.0 | 1024 | 0 | CPU | INTEGER ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| registeredResources ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| 0.0 | 983 | 0 | MEMORY | INTEGER ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| registeredResources ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS | STRINGSET ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| |||| stringSetValue |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| |||| 22 |||| |||| 2376 |||| |||| 2375 |||| |||| 51678 |||| |||| 51679 |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| ||| registeredResources ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS_UDP | STRINGSET ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| remainingResources ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| 0.0 | 1024 | 0 | CPU | INTEGER ||| ||+-------------------------------------------------+------------------------------------------------------+--------------------------------------------+--------------------------+-------------------------------------+|| ||| remainingResources ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| 0.0 | 983 | 0 | MEMORY | INTEGER ||| ||+------------------------------------------------+----------------------------------------------------+------------------------------------------+--------------------------------+------------------------------------+|| ||| remainingResources ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS | STRINGSET ||| ||+-----------------------------------------------+---------------------------------------------------+------------------------------------------+----------------------------+------------------------------------------+|| |||| stringSetValue |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| |||| 22 |||| |||| 2376 |||| |||| 2375 |||| |||| 51678 |||| |||| 51679 |||| |||+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+||| ||| remainingResources ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| doubleValue | integerValue | longValue | name | type ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| 0.0 | 0 | 0 | PORTS_UDP | STRINGSET ||| ||+--------------------------------------------+------------------------------------------------+---------------------------------------+---------------------------------------+----------------------------------------+|| ||| versionInfo ||| ||+----------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+|| ||| agentHash | agentVersion | dockerVersion ||| ||+----------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+|| ||| 02ff320c | 1.30.0 | DockerVersion: 18.06.1-ce ||| ||+----------------------------------------------+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------+||","title":"ECS Container Instance"},{"location":"AWS/aws-ecs/#ecs-tasks-services","text":"A Task Definition is a collection of 1 or more container configurations. Some Tasks may need only one container, while other Tasks may need 2 or more potentially linked containers running concurrently. The Task definition allows you to specify which Docker image to use, which ports to expose, how much CPU and memory to allot, how to collect logs, and define environment variables. A Task is created when you run a Task directly, which launches container(s) (defined in the task definition) until they are stopped or exit on their own, at which point they are not replaced automatically. Running Tasks directly is ideal for short running jobs, perhaps as an example things that were accomplished via CRON. A Service is used to guarantee that you always have some number of Tasks running at all times. If a Task's container exits due to error, or the underlying EC2 instance fails and is replaced, the ECS Service will replace the failed Task. This is why we create Clusters so that the Service has plenty of resources in terms of CPU, Memory and Network ports to use. To us it doesn't really matter which instance Tasks run on so long as they run. A Service configuration references a Task definition. A Service is responsible for creating Tasks. Services are typically used for long running applications like web servers. For example, if I deployed my website powered by Node.JS in Oregon (us-west-2) I would want say at least three Tasks running across the three Availability Zones (AZ) for the sake of High-Availability; if one fails I have another two and the failed one will be replaced (read that as self-healing!). Creating a Service is the way to do this. If I had 6 EC2 instances in my cluster, 2 per AZ, the Service will automatically balance Tasks across zones as best it can while also considering cpu, memory, and network resources. Another very important point is that a Service can be configured to use a load balancer , so that as it creates the Tasks\u2014that is it launches containers defined in the Task Defintion\u2014the Service will automatically register the container's EC2 instance with the load balancer. Tasks cannot be configured to use a load balancer, only Services can.","title":"ECS Tasks &amp; Services"},{"location":"AWS/aws-ecs/#ecs-scheduler","text":"3 ways to schedule a task on your cluster The scheduler helps you utilize your resources with least effort on the user's part and in the best way possible. Scheduler is flexible and we can use the instance as well if needed and we can even use third-party scheduler. Services Services are long lived and stateless (like a Web application) Define how many service instances you want (like Autoscaling, it will sclae up whenever any instance is down.) Works with Elastic Load Balancing as well (Hook up the service to ELB) 3 steps for placing a service into your cluster Compare the task definition's attributes to the state of the cluster and then check if the container instance can run the required attributes (cpu, memory, etc.) The scheduler has a list of container instances and check how many service instances are running in an availability zone. Keeping this in mind, it will place a new instance in an availability zone that has least amount of tasks running on it Check how many service instances are running on container instances and place a new task on the container instance that has least amount of tasks running on it. Running Tasks Tasks that are short lived or 1 off tasks that exit when done can be run with Scheduler. (Video encoding or DB migration for example) We can use RunTask which randomly distributes tasks on your cluster. But, it minimizes specific instances from getting overloaded. Starting Tasks Running tasks and starting tasks are different StartTask lets you pick where you want to run a task StartTasks lets you build or use your own scheduler Services and tasks have 3 states: - PENDING - RUNNING - STOPPED The container agent is responsible for state tracking Task LifeCycle aws> ecs create-service --cluster deepdive --service-name web --task-definition web --desired-count 1 ---------------------------------------------------------------------------------------- | CreateService | +--------------------------------------------------------------------------------------+ || service || | +-----------------------+------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 1 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 0 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | +-----------------------+------------------------------------------------------------+ | || | deploymentConfiguration || | || +----------------------------------------------------------------+-----------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +----------------------------------------------------------------+-----------------+ || || | deployments || | || +-----------------+----------------------------------------------------------------+ || || | createdAt | 1567608636 .767 || | || | desiredCount | 1 || | || | id | ecs-svc/9223370469246139040 || | || | launchType | EC2 || | || | pendingCount | 0 || | || | runningCount | 0 || | || | status | PRIMARY || | || | taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | || | updatedAt | 1567608636 .767 || | || +-----------------+----------------------------------------------------------------+ || aws> ecs list-services --cluster deepdive ------------------------------------------------------ | ListServices | +----------------------------------------------------+ || serviceArns || | +--------------------------------------------------+ | || arn:aws:ecs:us-east-1:289503391411:service/web || | +--------------------------------------------------+ | aws> ecs describe-services --cluster deepdive --services web --------------------------------------------------------------------------------------------------------------------------------------------------- | DescribeServices | +-------------------------------------------------------------------------------------------------------------------------------------------------+ || services || | +---------------------------------------+-------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 1 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 1 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | +---------------------------------------+-------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +--------------------------------------------------------------------------------------------------------------+------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +--------------------------------------------------------------------------------------------------------------+------------------------------+ || || | deployments || | || +-------------------------------+-------------------------------------------------------------------------------------------------------------+ || || | createdAt | 1567608636 .767 || | || | desiredCount | 1 || | || | id | ecs-svc/9223370469246139040 || | || | launchType | EC2 || | || | pendingCount | 0 || | || | runningCount | 1 || | || | status | PRIMARY || | || | taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | || | updatedAt | 1567608650 .671 || | || +-------------------------------+-------------------------------------------------------------------------------------------------------------+ || || | events || | || +----------------+----------------------------------------+-----------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +----------------+----------------------------------------+-----------------------------------------------------------------------------------+ || || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +----------------+----------------------------------------+-----------------------------------------------------------------------------------+ || aws> ecs update-service --cluster deepdive --service web --task-definition web --desired-count 2 -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | UpdateService | +------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || service || | +--------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 2 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 1 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || | +--------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------+ || || | deployments || | || +----------------+---------------+------------------------------+-------------+---------------+---------------+----------+------------------------------------------------------------+------------------+ || || | createdAt | desiredCount | id | launchType | pendingCount | runningCount | status | taskDefinition | updatedAt || | || +----------------+---------------+------------------------------+-------------+---------------+---------------+----------+------------------------------------------------------------+------------------+ || || | 1567610184 .168 | 2 | ecs-svc/9223370469244591639 | EC2 | 0 | 0 | PRIMARY | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 | 1567610184 .168 || | || | 1567608636 .767 | 1 | ecs-svc/9223370469246139040 | EC2 | 0 | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 | 1567608650 .671 || | || +----------------+---------------+------------------------------+-------------+---------------+---------------+----------+------------------------------------------------------------+------------------+ || || | events || | || +------------------------+--------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +------------------------+--------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+ || || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +------------------------+--------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+ || To stop the service aws> ecs update-service --cluster deepdive --service web --task-definition web --desired-count 0 ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | UpdateService | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || service || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 0 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 1 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | ACTIVE || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | deployments || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | createdAt | desiredCount | id | launchType | pendingCount | runningCount | status | taskDefinition | updatedAt || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | 1567610184 .168 | 0 | ecs-svc/9223370469244591639 | EC2 | 0 | 0 | PRIMARY | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 | 1567610184 .168 || | || | 1567608636 .767 | 1 | ecs-svc/9223370469246139040 | EC2 | 0 | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 | 1567610184 .168 || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | events || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | 1567610200 .241 | edbdedf6-951a-4a93-aa62-358e9af36e4a | ( service web ) was unable to place a task because no container instance met all of its requirements. The closest matching ( container-instance 6368caa4-6386-4a5b-9c60-ac752591b819 ) is already using a port required by your task. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide. || | || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || aws> ecs delete-service --cluster deepdive --service web ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | DeleteService | +--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || service || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || createdAt | 1567608636 .767 || || desiredCount | 0 || || enableECSManagedTags | False || || launchType | EC2 || || pendingCount | 0 || || propagateTags | NONE || || runningCount | 0 || || schedulingStrategy | REPLICA || || serviceArn | arn:aws:ecs:us-east-1:289503391411:service/web || || serviceName | web || || status | DRAINING || || taskDefinition | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || | +-----------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | || | deploymentConfiguration || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | maximumPercent | 200 || | || | minimumHealthyPercent | 100 || | || +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+ || || | deployments || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | createdAt | desiredCount | id | launchType | pendingCount | runningCount | status | taskDefinition | updatedAt || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | 1567610184 .168 | 0 | ecs-svc/9223370469244591639 | EC2 | 0 | 0 | PRIMARY | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 | 1567610345 .337 || | || | 1567608636 .767 | 1 | ecs-svc/9223370469246139040 | EC2 | 0 | 0 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 | 1567610184 .168 || | || +--------------------------------+-----------------------------+----------------------------------------------------------+-------------------------+-----------------------------+-----------------------------+--------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------+ || || | events || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | createdAt | id | message || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ || || | 1567610345 .38 | affb2376-1662-48db-9289-97c56c373cc7 | ( service web ) has stopped 1 running tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || | 1567610200 .241 | edbdedf6-951a-4a93-aa62-358e9af36e4a | ( service web ) was unable to place a task because no container instance met all of its requirements. The closest matching ( container-instance 6368caa4-6386-4a5b-9c60-ac752591b819 ) is already using a port required by your task. For more information, see the Troubleshooting section of the Amazon ECS Developer Guide. || | || | 1567608650 .678 | 8db7eb66-89f8-4bd0-bb9d-1a3c5f7b1d41 | ( service web ) has reached a steady state. || | || | 1567608640 .905 | 20d60fcb-e5af-41dc-a37c-b692a68c9618 | ( service web ) has started 1 tasks: ( task 6b939534-e746-4375-ab3e-6c12e17b55d9 ) . || | || +----------------+---------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ ||","title":"ECS Scheduler"},{"location":"AWS/aws-ecs/#ecs-task-definitions","text":"Task Definitions describe how Docker images should be ran. Each Task Definition can control 1 or more containers (with optional volumes) Applications can be 1 or more task definitions. Grouped containers run on the same instance. This can be used for minimum latency. Services are created through task definitions. Components of Task definition. - Family -> Used for versioning the Task Definition when it is modified. - Container definition - Volumes -> A way to share data between containers. aws> ecs register-task-definition --cli-input-json file://web-task-definition.json ------------------------------------------------------------------------------------------------ | RegisterTaskDefinition | +----------------------------------------------------------------------------------------------+ || taskDefinition || |+--------+-----------+---------+-------------------------------------------------------------+| || family | revision | status | taskDefinitionArn || |+--------+-----------+---------+-------------------------------------------------------------+| || web | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || |+--------+-----------+---------+-------------------------------------------------------------+| ||| compatibilities ||| ||+------------------------------------------------------------------------------------------+|| ||| EC2 ||| ||+------------------------------------------------------------------------------------------+|| ||| containerDefinitions ||| ||+-----------+------------------------+----------------+------------------+-----------------+|| ||| cpu | essential | image | memory | name ||| ||+-----------+------------------------+----------------+------------------+-----------------+|| ||| 102 | True | nginx | 50 | nginx ||| ||+-----------+------------------------+----------------+------------------+-----------------+|| |||| portMappings |||| |||+-----------------------------------+-------------------------+--------------------------+||| |||| containerPort | hostPort | protocol |||| |||+-----------------------------------+-------------------------+--------------------------+||| |||| 80 | 80 | tcp |||| |||+-----------------------------------+-------------------------+--------------------------+||| aws> ecs list-task-definition-families ---------------------------- | ListTaskDefinitionFamilies | +--------------------------+ || families || | +------------------------+ | || web || | +------------------------+ | aws> ecs list-task-definitions ---------------------------------------------------------------- | ListTaskDefinitions | +--------------------------------------------------------------+ || taskDefinitionArns || | +------------------------------------------------------------+ | || arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || | +------------------------------------------------------------+ | aws> ecs describe-task-definition --task-definition web:1 ------------------------------------------------------------------------------------------------ | DescribeTaskDefinition | +----------------------------------------------------------------------------------------------+ || taskDefinition || | +--------+-----------+---------+-------------------------------------------------------------+ | || family | revision | status | taskDefinitionArn || | +--------+-----------+---------+-------------------------------------------------------------+ | || web | 1 | ACTIVE | arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || | +--------+-----------+---------+-------------------------------------------------------------+ | || | compatibilities || | || +------------------------------------------------------------------------------------------+ || || | EC2 || | || +------------------------------------------------------------------------------------------+ || || | containerDefinitions || | || +-----------+------------------------+----------------+------------------+-----------------+ || || | cpu | essential | image | memory | name || | || +-----------+------------------------+----------------+------------------+-----------------+ || || | 102 | True | nginx | 50 | nginx || | || +-----------+------------------------+----------------+------------------+-----------------+ || |||| portMappings |||| || | +-----------------------------------+-------------------------+--------------------------+ || | |||| containerPort | hostPort | protocol |||| || | +-----------------------------------+-------------------------+--------------------------+ || | |||| 80 | 80 | tcp |||| || | +-----------------------------------+-------------------------+--------------------------+ || | After registering task definition under family web for 4 times aws> ecs list-task-definitions ---------------------------------------------------------------- | ListTaskDefinitions | +--------------------------------------------------------------+ || taskDefinitionArns || | +------------------------------------------------------------+ | || arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:2 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:3 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || | +------------------------------------------------------------+ | You can deregister any task definition aws> ecs deregister-task-definition --task-definition web:3 aws> ecs list-task-definitions ---------------------------------------------------------------- | ListTaskDefinitions | +--------------------------------------------------------------+ || taskDefinitionArns || |+------------------------------------------------------------+| || arn:aws:ecs:us-east-1:289503391411:task-definition/web:1 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:2 || || arn:aws:ecs:us-east-1:289503391411:task-definition/web:4 || |+------------------------------------------------------------+| To create a task definition template aws> ecs register-task-definition --generate-cli-skeleton aws> ecs run-task --cluster deepdive --task-definition web --count 1 -------------------------------------------------------------------------------------------------------------------------- | RunTask | +------------------------------------------------------------------------------------------------------------------------+ || tasks || | +----------------------+-----------------------------------------------------------------------------------------------+ | || clusterArn | arn:aws:ecs:us-east-1:289503391411:cluster/deepdive || || containerInstanceArn | arn:aws:ecs:us-east-1:289503391411:container-instance/6368caa4-6386-4a5b-9c60-ac752591b819 || || cpu | 102 || || createdAt | 1567610699 .64 || || desiredStatus | RUNNING || || group | family:web || || lastStatus | PENDING || || launchType | EC2 || || memory | 50 || || taskArn | arn:aws:ecs:us-east-1:289503391411:task/798b90f2-ffaf-4bd0-a2dd-10842dfd1d9f || || taskDefinitionArn | arn:aws:ecs:us-east-1:289503391411:task-definition/web:5 || || version | 1 || | +----------------------+-----------------------------------------------------------------------------------------------+ | || | containers || | || +-----------------+--------------------------------------------------------------------------------------------------+ || || | containerArn | arn:aws:ecs:us-east-1:289503391411:container/4ccb5949-e4c6-47d7-92a8-066b9d7c4cf4 || | || | cpu | 102 || | || | lastStatus | PENDING || | || | memory | 50 || | || | name | nginx || | || | taskArn | arn:aws:ecs:us-east-1:289503391411:task/798b90f2-ffaf-4bd0-a2dd-10842dfd1d9f || | || +-----------------+--------------------------------------------------------------------------------------------------+ || || | overrides || | || +--------------------------------------------------------------------------------------------------------------------+ || |||| containerOverrides |||| || | +-----------------------------------------------------+------------------------------------------------------------+ || | |||| name | nginx |||| || | +-----------------------------------------------------+------------------------------------------------------------+ || |","title":"ECS Task Definitions"},{"location":"AWS/awslogs/","text":"AWSLogs \u00b6 AWSLogs can be used in place of Logstash when working with AWS Cloud and ELK stack. To install awslogs, we can use: pip install awslogs Groups can be listed using: awslogs groups Streams can be listed using: awslogs streams To capture from a specific stream, we can use: awslogs get ${group name} ALL --start=\"1h\" To capture to a file, we can use: awslogs get aws_vpc_log_groups ALL --start='12/05/2019 11:00' --end='12/05/2019 16:00' > vpcflowlog To remove first 2 columns, use: cat vpcflowlog | awk '{print $3 \" \" $4 \" \" $5 \" \" $6 \" \" $7 \" \" $8 \" \" $9 \" \" $10 \" \" $11 \" \" $12 \" \" $13 \" \" $14 \" \" $15 \" \" $16}' > tmp Capturing inter VPC communication and to drop intra VPC communication logs, use: cat tmp | awk '{ if( $4 ~ /10\\.100/ && $5 ~ /10\\.100/ ) {} else { print } }' > vpcflowlog","title":"AWSLogs"},{"location":"AWS/awslogs/#awslogs","text":"AWSLogs can be used in place of Logstash when working with AWS Cloud and ELK stack. To install awslogs, we can use: pip install awslogs Groups can be listed using: awslogs groups Streams can be listed using: awslogs streams To capture from a specific stream, we can use: awslogs get ${group name} ALL --start=\"1h\" To capture to a file, we can use: awslogs get aws_vpc_log_groups ALL --start='12/05/2019 11:00' --end='12/05/2019 16:00' > vpcflowlog To remove first 2 columns, use: cat vpcflowlog | awk '{print $3 \" \" $4 \" \" $5 \" \" $6 \" \" $7 \" \" $8 \" \" $9 \" \" $10 \" \" $11 \" \" $12 \" \" $13 \" \" $14 \" \" $15 \" \" $16}' > tmp Capturing inter VPC communication and to drop intra VPC communication logs, use: cat tmp | awk '{ if( $4 ~ /10\\.100/ && $5 ~ /10\\.100/ ) {} else { print } }' > vpcflowlog","title":"AWSLogs"},{"location":"AWS/ec2-security-groups-notes/","text":"EC2 Security Groups \u00b6 Security Group Name - sg-02909c0c9fb1ace0e To create security groups, run the following in aws-shell aws> ec2 create-security-group --group-name deepu_SG_useast1 --description \"Security Group for deepu on us-east-1\" And here is the output ------------------------------------- | CreateSecurityGroup | +----------+------------------------+ | GroupId | sg-02909c0c9fb1ace0e | +----------+------------------------+ To add ingress rule ffor SSH and HTTP aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 22 --cidr 0 .0.0.0/0 aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 80 --cidr 0 .0.0.0/0 To add ingress rule for PostgreSQL and Redis aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 5432 --cidr 0 .0.0.0/0 --source-group sg-02909c0c9fb1ace0e aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 6379 --cidr 0 .0.0.0/0 --source-group sg-02909c0c9fb1ace0e To display security groups aws> ec2 describe-security-groups --group-id sg-02909c0c9fb1ace0e And here is the output ------------------------------------------------------------------------------------------------------------------------- | DescribeSecurityGroups | +-----------------------------------------------------------------------------------------------------------------------+ || SecurityGroups || |+----------------------------------------+-----------------------+-------------------+---------------+----------------+| || Description | GroupId | GroupName | OwnerId | VpcId || |+----------------------------------------+-----------------------+-------------------+---------------+----------------+| || Security Group for deepu on us-east-1 | sg-02909c0c9fb1ace0e | deepu_SG_useast1 | 289503391411 | vpc-8954fcf3 || |+----------------------------------------+-----------------------+-------------------+---------------+----------------+| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 80 | tcp | 80 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 5432 | tcp | 5432 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| UserIdGroupPairs |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| GroupId | UserId |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| sg-02909c0c9fb1ace0e | 289503391411 |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 22 | tcp | 22 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 6379 | tcp | 6379 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| UserIdGroupPairs |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| GroupId | UserId |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| sg-02909c0c9fb1ace0e | 289503391411 |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| ||| IpPermissionsEgress ||| ||+-------------------------------------------------------------------------------------------------------------------+|| ||| IpProtocol ||| ||+-------------------------------------------------------------------------------------------------------------------+|| ||| -1 ||| ||+-------------------------------------------------------------------------------------------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| To delete a security group aws> ec2 delete-security-group --group-id sg-02909c0c9fb1ace0e","title":"EC2 Security Groups"},{"location":"AWS/ec2-security-groups-notes/#ec2-security-groups","text":"Security Group Name - sg-02909c0c9fb1ace0e To create security groups, run the following in aws-shell aws> ec2 create-security-group --group-name deepu_SG_useast1 --description \"Security Group for deepu on us-east-1\" And here is the output ------------------------------------- | CreateSecurityGroup | +----------+------------------------+ | GroupId | sg-02909c0c9fb1ace0e | +----------+------------------------+ To add ingress rule ffor SSH and HTTP aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 22 --cidr 0 .0.0.0/0 aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 80 --cidr 0 .0.0.0/0 To add ingress rule for PostgreSQL and Redis aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 5432 --cidr 0 .0.0.0/0 --source-group sg-02909c0c9fb1ace0e aws> ec2 authorize-security-group-ingress --group-id sg-02909c0c9fb1ace0e --protocol tcp --port 6379 --cidr 0 .0.0.0/0 --source-group sg-02909c0c9fb1ace0e To display security groups aws> ec2 describe-security-groups --group-id sg-02909c0c9fb1ace0e And here is the output ------------------------------------------------------------------------------------------------------------------------- | DescribeSecurityGroups | +-----------------------------------------------------------------------------------------------------------------------+ || SecurityGroups || |+----------------------------------------+-----------------------+-------------------+---------------+----------------+| || Description | GroupId | GroupName | OwnerId | VpcId || |+----------------------------------------+-----------------------+-------------------+---------------+----------------+| || Security Group for deepu on us-east-1 | sg-02909c0c9fb1ace0e | deepu_SG_useast1 | 289503391411 | vpc-8954fcf3 || |+----------------------------------------+-----------------------+-------------------+---------------+----------------+| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 80 | tcp | 80 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 5432 | tcp | 5432 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| UserIdGroupPairs |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| GroupId | UserId |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| sg-02909c0c9fb1ace0e | 289503391411 |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 22 | tcp | 22 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| ||| IpPermissions ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| FromPort | IpProtocol | ToPort ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| ||| 6379 | tcp | 6379 ||| ||+-------------------------------------+---------------------------------------------+-------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| UserIdGroupPairs |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| GroupId | UserId |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| |||| sg-02909c0c9fb1ace0e | 289503391411 |||| |||+-------------------------------------------------------------------+---------------------------------------------+||| ||| IpPermissionsEgress ||| ||+-------------------------------------------------------------------------------------------------------------------+|| ||| IpProtocol ||| ||+-------------------------------------------------------------------------------------------------------------------+|| ||| -1 ||| ||+-------------------------------------------------------------------------------------------------------------------+|| |||| IpRanges |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| CidrIp |||| |||+-----------------------------------------------------------------------------------------------------------------+||| |||| 0.0.0.0/0 |||| |||+-----------------------------------------------------------------------------------------------------------------+||| To delete a security group aws> ec2 delete-security-group --group-id sg-02909c0c9fb1ace0e","title":"EC2 Security Groups"},{"location":"AWS/elk-on-ecs/","text":"ELK stack on AWS ECS \u00b6 Deploying Elastic Stack on AWS Elastic Container Service I will outline the procedure I followed to deploy Elastic Stack on AWS ECS. Note: The following arch has been deprecated as Docker released Here is the architecture I followed to build this In the diagram above, we have the following containers: - Three ElasticSearch containers to ensure high availability. Odd number to minimize split brain scenarios. - Two Kibana containers for UI - Two logstash containers to handle traffic Split Brain Scenario is a situation where a network failure caused the network to split. The split means that the parts can no longer communicate with each other. They need to decide what to do next on their own based on the latest membership data of the cluster. This is being handled by using 3 ECS clusters. Working with immutable infrastructure to replace defective componenets of this architecture. After adding DockerFile and elasticsearch.yml configuration files, run the aws-ecr-bake-and-push.sh file to push a new docker image into the private docker repo on AWS ECR. ./elk-stack-aws-ecr-bake-and-push.sh elasticsearch After we have the customized docker image with elasticsearch on ECR, we need to create a cluster in ECS with 3 EC2 instances. Few points that need to be addressed: - We might not need all 3 instances all the time. For this reason, we will be using Autoscaling to start with 1 instance and scale to 3 whenever needed. - There is no way to disable public IP assignments from the ECS wizard. Instead, we need to manually ensure our private cluster stays private, safely tucked inside our VPC. - ElasticSearch will fail due to a low mmap count; we need to increase that. - We need to allocate more space to our ElasticSearch nodes than the standard 8GB granted by Amazon. - We need to customize the docker daemon to take advantage of the extra space. Cluster dev-elk-cluster Instance type t2.micro Desired number of instances 3 Key pair elk-stack ECS AMI ID ami-066ce9bb9f4cbb03d VPC vpc-071e76a617d759d04 Subnet 1 subnet-02f18e573300175b6 Subnet 1 route table association rtbassoc-06119201929019675 Subnet 2 subnet-0b8babda5dab2c0b7 Subnet 2 route table association rtbassoc-0881ec1492ec2c853 VPC Availability Zones us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1e, us-east-1f Security group sg-0cfae2130059a5738 Internet gateway igw-0a96cb67118059687 Route table rtb-0de91746702e12029 Amazon EC2 route EC2Co-Publi-VF872SCT2020 Virtual private gateway attachment EC2Co-Attac-17Z48YC2S8CVW Launch configuration EC2ContainerService-dev-elk-cluster-EcsInstanceLc-154J2FEYWM6UX Auto Scaling group EC2ContainerService-dev-elk-cluster-EcsInstanceAsg-1OEDR2NDUIUHE --- Adding the following content to User Data section of the EC2 instances in CloudFormation Changeset. Content-Type: multipart/mixed; boundary=\"//\" MIME-Version: 1.0 --// Content-Type: text/cloud-boothook; charset=\"us-ascii\" # Set Docker daemon options cloud-init-per once docker_options echo 'OPTIONS=\"${OPTIONS} --storage-opt dm.basesize=250G\"' >> /etc/sysconfig/docker --// Content-Type: text/x-shellscript; charset=\"us-ascii\" #!/bin/bash echo ECS_CLUSTER=dev-elk-cluster >> /etc/ecs/ecs.config; echo ECS_BACKEND_HOST=dev-elk-cluster >> /etc/ecs/ecs.config; echo ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=15m >> /etc/ecs/ecs.config; echo ECS_IMAGE_CLEANUP_INTERVAL=10m >> /etc/ecs/ecs.config; sysctl -w vm.max_map_count=262144; mkdir -p /usr/share/elasticsearch/data/; chown -R 1000.1000 /usr/share/elasticsearch/data/; --//-- Storage \u00b6 Adding EBS stores for storage and using PIOPS as advised on elastic's documentation. Or we can go with using Instance Store that has the advantage of hard disk physical attached to the host and also benefit of avoiding to pay extra for EBS. Note: EBS Storage option in ECS Cluster configuration is not the storage for containers to store their data. This volume is strictly for docker images. Amazon explains this: The volume is configured as a Logical Volume Management (LVM) device and it is accessed directly by Docker via the devicemapper backend. Because the volume is not mounted, you cannot use standard storage information commands (such as df -h) to determine the available storage. AutoScaling \u00b6 We will be using Autoscaling to scale up or down the EC2 instances as required. Autoscaling Launch config can be used as reference. We can use CloudFormation to deploy ECS containers with ElasticSearch and Kibana. To create custom ElasticSearch Docker images, I am using AWS ECR push script using the CloudFormation stack with AWS ECR params and this CloudFormation template . I am using this bash script to push custom elasticsearch images to AWS ECR when needed. Once the custom images are updated in ECR, we can work on deploying ECS instances. This can be done using CloudFormation as well. Note: CloudFormation will require privileges to create IAM roles. To create EC2 instances with ECS and using the images from ECR repo, I am using AWS ECS bash script and the CloudFormation template that will use these parameters . The need for Logstash has been deprecated since AWS ECS can utilize awslogs Log driver.","title":"ELK stack on AWS ECS"},{"location":"AWS/elk-on-ecs/#elk-stack-on-aws-ecs","text":"Deploying Elastic Stack on AWS Elastic Container Service I will outline the procedure I followed to deploy Elastic Stack on AWS ECS. Note: The following arch has been deprecated as Docker released Here is the architecture I followed to build this In the diagram above, we have the following containers: - Three ElasticSearch containers to ensure high availability. Odd number to minimize split brain scenarios. - Two Kibana containers for UI - Two logstash containers to handle traffic Split Brain Scenario is a situation where a network failure caused the network to split. The split means that the parts can no longer communicate with each other. They need to decide what to do next on their own based on the latest membership data of the cluster. This is being handled by using 3 ECS clusters. Working with immutable infrastructure to replace defective componenets of this architecture. After adding DockerFile and elasticsearch.yml configuration files, run the aws-ecr-bake-and-push.sh file to push a new docker image into the private docker repo on AWS ECR. ./elk-stack-aws-ecr-bake-and-push.sh elasticsearch After we have the customized docker image with elasticsearch on ECR, we need to create a cluster in ECS with 3 EC2 instances. Few points that need to be addressed: - We might not need all 3 instances all the time. For this reason, we will be using Autoscaling to start with 1 instance and scale to 3 whenever needed. - There is no way to disable public IP assignments from the ECS wizard. Instead, we need to manually ensure our private cluster stays private, safely tucked inside our VPC. - ElasticSearch will fail due to a low mmap count; we need to increase that. - We need to allocate more space to our ElasticSearch nodes than the standard 8GB granted by Amazon. - We need to customize the docker daemon to take advantage of the extra space. Cluster dev-elk-cluster Instance type t2.micro Desired number of instances 3 Key pair elk-stack ECS AMI ID ami-066ce9bb9f4cbb03d VPC vpc-071e76a617d759d04 Subnet 1 subnet-02f18e573300175b6 Subnet 1 route table association rtbassoc-06119201929019675 Subnet 2 subnet-0b8babda5dab2c0b7 Subnet 2 route table association rtbassoc-0881ec1492ec2c853 VPC Availability Zones us-east-1a, us-east-1b, us-east-1c, us-east-1d, us-east-1e, us-east-1f Security group sg-0cfae2130059a5738 Internet gateway igw-0a96cb67118059687 Route table rtb-0de91746702e12029 Amazon EC2 route EC2Co-Publi-VF872SCT2020 Virtual private gateway attachment EC2Co-Attac-17Z48YC2S8CVW Launch configuration EC2ContainerService-dev-elk-cluster-EcsInstanceLc-154J2FEYWM6UX Auto Scaling group EC2ContainerService-dev-elk-cluster-EcsInstanceAsg-1OEDR2NDUIUHE --- Adding the following content to User Data section of the EC2 instances in CloudFormation Changeset. Content-Type: multipart/mixed; boundary=\"//\" MIME-Version: 1.0 --// Content-Type: text/cloud-boothook; charset=\"us-ascii\" # Set Docker daemon options cloud-init-per once docker_options echo 'OPTIONS=\"${OPTIONS} --storage-opt dm.basesize=250G\"' >> /etc/sysconfig/docker --// Content-Type: text/x-shellscript; charset=\"us-ascii\" #!/bin/bash echo ECS_CLUSTER=dev-elk-cluster >> /etc/ecs/ecs.config; echo ECS_BACKEND_HOST=dev-elk-cluster >> /etc/ecs/ecs.config; echo ECS_ENGINE_TASK_CLEANUP_WAIT_DURATION=15m >> /etc/ecs/ecs.config; echo ECS_IMAGE_CLEANUP_INTERVAL=10m >> /etc/ecs/ecs.config; sysctl -w vm.max_map_count=262144; mkdir -p /usr/share/elasticsearch/data/; chown -R 1000.1000 /usr/share/elasticsearch/data/; --//--","title":"ELK stack on AWS ECS"},{"location":"AWS/elk-on-ecs/#storage","text":"Adding EBS stores for storage and using PIOPS as advised on elastic's documentation. Or we can go with using Instance Store that has the advantage of hard disk physical attached to the host and also benefit of avoiding to pay extra for EBS. Note: EBS Storage option in ECS Cluster configuration is not the storage for containers to store their data. This volume is strictly for docker images. Amazon explains this: The volume is configured as a Logical Volume Management (LVM) device and it is accessed directly by Docker via the devicemapper backend. Because the volume is not mounted, you cannot use standard storage information commands (such as df -h) to determine the available storage.","title":"Storage"},{"location":"AWS/elk-on-ecs/#autoscaling","text":"We will be using Autoscaling to scale up or down the EC2 instances as required. Autoscaling Launch config can be used as reference. We can use CloudFormation to deploy ECS containers with ElasticSearch and Kibana. To create custom ElasticSearch Docker images, I am using AWS ECR push script using the CloudFormation stack with AWS ECR params and this CloudFormation template . I am using this bash script to push custom elasticsearch images to AWS ECR when needed. Once the custom images are updated in ECR, we can work on deploying ECS instances. This can be done using CloudFormation as well. Note: CloudFormation will require privileges to create IAM roles. To create EC2 instances with ECS and using the images from ECR repo, I am using AWS ECS bash script and the CloudFormation template that will use these parameters . The need for Logstash has been deprecated since AWS ECS can utilize awslogs Log driver.","title":"AutoScaling"},{"location":"AWS/lambda-programming-model/","text":"Lambda Programming Model \u00b6 Handler \u00b6 1 Handler is the function AWS Lambda calls to start execution of your Lambda function. You identify the handler when you create your Lambda function. When a Lambda function is invoked, AWS Lambda starts executing your code by calling the handler function. Context \u00b6 AWS Lambda also passes a context object to the handler function, as the second parameter. Via this context object your code can interact with AWS Lambda. Logging \u00b6 Your Lambda function can contain logging statements. AWS Lambda writes these logs to CloudWatch Logs. Log data can be lost due to throttling or, in some cases, when the execution context is terminated. Exceptions \u00b6 Your Lambda function needs to communicate the result of the function execution to AWS Lambda. Concurrency \u00b6 When your function is invoked more quickly than a single instance of your function can process events, Lambda scales by running additional instances. Bootstrapping \u00b6 Your Lambda function code must be written in a stateless style, and have no affinity with the underlying compute infrastructure. 2 Bootstrapping refers to the process of preparing the environment before an application starts to resolve and process an incoming request. Bootstrapping is done in two places: in the entry script in the application AWS Lambda Docs \u21a9 Runtime bootstrapping \u21a9","title":"Lambda Programming Model"},{"location":"AWS/lambda-programming-model/#lambda-programming-model","text":"","title":"Lambda Programming Model"},{"location":"AWS/lambda-programming-model/#handler","text":"1 Handler is the function AWS Lambda calls to start execution of your Lambda function. You identify the handler when you create your Lambda function. When a Lambda function is invoked, AWS Lambda starts executing your code by calling the handler function.","title":"Handler"},{"location":"AWS/lambda-programming-model/#context","text":"AWS Lambda also passes a context object to the handler function, as the second parameter. Via this context object your code can interact with AWS Lambda.","title":"Context"},{"location":"AWS/lambda-programming-model/#logging","text":"Your Lambda function can contain logging statements. AWS Lambda writes these logs to CloudWatch Logs. Log data can be lost due to throttling or, in some cases, when the execution context is terminated.","title":"Logging"},{"location":"AWS/lambda-programming-model/#exceptions","text":"Your Lambda function needs to communicate the result of the function execution to AWS Lambda.","title":"Exceptions"},{"location":"AWS/lambda-programming-model/#concurrency","text":"When your function is invoked more quickly than a single instance of your function can process events, Lambda scales by running additional instances.","title":"Concurrency"},{"location":"AWS/lambda-programming-model/#bootstrapping","text":"Your Lambda function code must be written in a stateless style, and have no affinity with the underlying compute infrastructure. 2 Bootstrapping refers to the process of preparing the environment before an application starts to resolve and process an incoming request. Bootstrapping is done in two places: in the entry script in the application AWS Lambda Docs \u21a9 Runtime bootstrapping \u21a9","title":"Bootstrapping"},{"location":"AWS/python-boto3/","text":"Python Boto3 \u00b6 Boto3 is the AWS SDK for Python. It provides easy to use object-oriented API and low-level access to AWS services. Botocore provices the low-level clients, session, credentials and configuration data. Boto3 is built on top of Botocore by providing its own sessions, resources and collections. Session \u00b6 A session manages state about a particular configuration. By default, a session is created for you when needed. However, it is possible and recommended to maintain your own sessions in some scenarios. Sessions typically store: Credentials Region Other configurations Configuration \u00b6 In addition to credentials, you can also configure non-credential values. In general, boto3 follows the same approach used in credential lookup: try various locations until a value is found. Boto3 uses these sources for configuration: Explicitly passed as the config parameter when creating a client. Environment variables ~/.aws/config Resources \u00b6 Resources represent an object-oriented interface to AWS services. They provide a higher-level abstraction than the raw, low-level calls made by service clients. import boto3 s3 = boto3 . resource ( 's3' ) sqs = boto3 . resource ( 'sqs' ) Clients \u00b6 Clients provide a low-level interface to AWS whose methods map close to one-to-one with service APIs. All service operations are supported by clients. Clients are generated from a JSON service definition file. import boto3 sqs = boto3 . client ( 'sqs' ) Botocore \u00b6 Botocore session is encapsulated by Boto3 session. Botocore also maintains credentials and Boto3 can be configured in multiple ways. Regardless of the source that we choose, AWS credentials and region must be set in order to make requests. ACCESS_KEY: The access key for AWS account SECRET_KEY: The secret key for AWS account SESSION_TOKEN: Used with temporary credentials Botocore package provides a low-level interface to AWS services. It is responsible for: Providing access to all available services. Providing access to all operations within a service. Marshaling all parameters for a particular operation in the correct format (serialization) Signing the request with the correct authentication signature. Receiving the response and returning the data in native python data structures (deserialization). Handling higher level abstractions on top of these services is left to the application layer. Botocore package is mainly data-driven. Each service has a JSON description which specifies: all of the operations the services supports, all of the parameters the operations accepts, all of the documentaion related to the service, information about supported regions and endpoints. Client vs Resource \u00b6 Client Resource - Exposes Botocore clients to developers - Generated from JSON resource descriptions - Generated from JSON service descriptions - Object-oriented API - Provides low-level service access - Identifiers and attributes - Typically maps one-to-one with service API - Actions, References, Sub-resources - Method names are sname-cases, list_buckets - Collections","title":"Python Boto3"},{"location":"AWS/python-boto3/#python-boto3","text":"Boto3 is the AWS SDK for Python. It provides easy to use object-oriented API and low-level access to AWS services. Botocore provices the low-level clients, session, credentials and configuration data. Boto3 is built on top of Botocore by providing its own sessions, resources and collections.","title":"Python Boto3"},{"location":"AWS/python-boto3/#session","text":"A session manages state about a particular configuration. By default, a session is created for you when needed. However, it is possible and recommended to maintain your own sessions in some scenarios. Sessions typically store: Credentials Region Other configurations","title":"Session"},{"location":"AWS/python-boto3/#configuration","text":"In addition to credentials, you can also configure non-credential values. In general, boto3 follows the same approach used in credential lookup: try various locations until a value is found. Boto3 uses these sources for configuration: Explicitly passed as the config parameter when creating a client. Environment variables ~/.aws/config","title":"Configuration"},{"location":"AWS/python-boto3/#resources","text":"Resources represent an object-oriented interface to AWS services. They provide a higher-level abstraction than the raw, low-level calls made by service clients. import boto3 s3 = boto3 . resource ( 's3' ) sqs = boto3 . resource ( 'sqs' )","title":"Resources"},{"location":"AWS/python-boto3/#clients","text":"Clients provide a low-level interface to AWS whose methods map close to one-to-one with service APIs. All service operations are supported by clients. Clients are generated from a JSON service definition file. import boto3 sqs = boto3 . client ( 'sqs' )","title":"Clients"},{"location":"AWS/python-boto3/#botocore","text":"Botocore session is encapsulated by Boto3 session. Botocore also maintains credentials and Boto3 can be configured in multiple ways. Regardless of the source that we choose, AWS credentials and region must be set in order to make requests. ACCESS_KEY: The access key for AWS account SECRET_KEY: The secret key for AWS account SESSION_TOKEN: Used with temporary credentials Botocore package provides a low-level interface to AWS services. It is responsible for: Providing access to all available services. Providing access to all operations within a service. Marshaling all parameters for a particular operation in the correct format (serialization) Signing the request with the correct authentication signature. Receiving the response and returning the data in native python data structures (deserialization). Handling higher level abstractions on top of these services is left to the application layer. Botocore package is mainly data-driven. Each service has a JSON description which specifies: all of the operations the services supports, all of the parameters the operations accepts, all of the documentaion related to the service, information about supported regions and endpoints.","title":"Botocore"},{"location":"AWS/python-boto3/#client-vs-resource","text":"Client Resource - Exposes Botocore clients to developers - Generated from JSON resource descriptions - Generated from JSON service descriptions - Object-oriented API - Provides low-level service access - Identifiers and attributes - Typically maps one-to-one with service API - Actions, References, Sub-resources - Method names are sname-cases, list_buckets - Collections","title":"Client vs Resource"},{"location":"AWS/vpc-best-practices/","text":"VPC Best Practices \u00b6 Choose proper VPC configuration based on present and future needs It is advisable to design your Amazon VPC implementation based on your expansion requirements looking ahead at least two years. Choose proper CIDR block for VPC. The permissible size of the CIDR block ranges between /16 netmask and a /28 netmask. While designing your Amazon VPC architecture to communicate with the on-premises data center, it is required that the CIDR range used in Amazon VPC does not overlap or cause a conflict with the CIDR block in the on-premises data center. Isolate dev, testing and prod environments It is always better to create a distinct Amazon VPC for development, production, and staging \u2013 or one Amazon VPC with Separate Security/Subnets/isolated NW groups for staging, production, and development. Securing your AWS VPC implementation web application firewall or a firewall virtual appliance. configure intrusion detection systems and intrusion prevention virtual appliances. IAM, you can audit and monitor Administrator access to your VPC. Create Disaster recovery plan Developing a disaster recovery plan with respect to your VPC implementation is of critical importance Traffic control and Security VPC peering Interconnected applications requiring private and secure access inside AWS. Systems have been deployed in different AWS accounts by some business units and are required to be either shared or consumed privately Better integrated access of systems, such as when a customer can peer their VPC with their core suppliers. EC2 instances in private subnets While using ELB for web applications, ensure that you place all other EC2 instances in private subnets wherever possible. secure practice dictates that only ELBs must be in the public subnet. Tagging use a tagging policy to efficiently organize resources for reporting.","title":"VPC Best Practices"},{"location":"AWS/vpc-best-practices/#vpc-best-practices","text":"Choose proper VPC configuration based on present and future needs It is advisable to design your Amazon VPC implementation based on your expansion requirements looking ahead at least two years. Choose proper CIDR block for VPC. The permissible size of the CIDR block ranges between /16 netmask and a /28 netmask. While designing your Amazon VPC architecture to communicate with the on-premises data center, it is required that the CIDR range used in Amazon VPC does not overlap or cause a conflict with the CIDR block in the on-premises data center. Isolate dev, testing and prod environments It is always better to create a distinct Amazon VPC for development, production, and staging \u2013 or one Amazon VPC with Separate Security/Subnets/isolated NW groups for staging, production, and development. Securing your AWS VPC implementation web application firewall or a firewall virtual appliance. configure intrusion detection systems and intrusion prevention virtual appliances. IAM, you can audit and monitor Administrator access to your VPC. Create Disaster recovery plan Developing a disaster recovery plan with respect to your VPC implementation is of critical importance Traffic control and Security VPC peering Interconnected applications requiring private and secure access inside AWS. Systems have been deployed in different AWS accounts by some business units and are required to be either shared or consumed privately Better integrated access of systems, such as when a customer can peer their VPC with their core suppliers. EC2 instances in private subnets While using ELB for web applications, ensure that you place all other EC2 instances in private subnets wherever possible. secure practice dictates that only ELBs must be in the public subnet. Tagging use a tagging policy to efficiently organize resources for reporting.","title":"VPC Best Practices"},{"location":"CKAD/Containers/","text":"Container \u00b6 What is a container \u00b6 A container is a self-contained ready-to-run application. Containers have all on board that is required to start the application. To start a container, a container runtime is required. The container runtime is running on a host platform and establishes communication between the localhost kernel and the container. All containers, no matter what they do, run on top of the same local host kernel. On top of container runtime, there are isolated namespaces that help in running different containers. Every virtual machine sits on its own Kernel. On the other hand, Containers run on Container runtime that in turn talks to/ sits on the kernel. Containers are based on features offered by the Linux operating system. Linux kernel namespaces provide strict isolation between system components at different levels: network file users processes IPCs Linux cgroups offer resource allocation and limitation. Container runtimes \u00b6 The container runtime allows for starting and running the container on top of the host OS The container runtime is responsible for all parts of running the container which are not already a part of the running container program itself Different container runtime solutions exist: docker lxc runc cri-o rkt containerd These runtimes are included in different container solutions. Understanding OCI \u00b6 OCI is the Open Containers Initiative that standardizes the use of containers The image-spec defines how to package a container in a filesystem bundle The runtime-spec defines how to run that filesystem in a container OCI standardization ensures compatability between containers, no matter which environment they originally come from. Understanding Container Components \u00b6 Images are read-only environments that contain the runtime environment which includes the application and all libraries it requires. Registries are used to store images. Docker Hub is a common registry, but private registries can be also created. Containers are the isolated runtime environments where the application is running. By using namespaces , the containers can be offered as a strictly isolated environment. Starting Containers \u00b6 On CentOS/RHEL 7, Docker is available from the repositories. Use yum install docker to install Run docker --version to verify installation One CentOS/RHEL 8, Docker is no longer available use docker-ce from Docker.io instead. After install, use systemctl enable --now docker to start and enable the service","title":"Container"},{"location":"CKAD/Containers/#container","text":"","title":"Container"},{"location":"CKAD/Containers/#what-is-a-container","text":"A container is a self-contained ready-to-run application. Containers have all on board that is required to start the application. To start a container, a container runtime is required. The container runtime is running on a host platform and establishes communication between the localhost kernel and the container. All containers, no matter what they do, run on top of the same local host kernel. On top of container runtime, there are isolated namespaces that help in running different containers. Every virtual machine sits on its own Kernel. On the other hand, Containers run on Container runtime that in turn talks to/ sits on the kernel. Containers are based on features offered by the Linux operating system. Linux kernel namespaces provide strict isolation between system components at different levels: network file users processes IPCs Linux cgroups offer resource allocation and limitation.","title":"What is a container"},{"location":"CKAD/Containers/#container-runtimes","text":"The container runtime allows for starting and running the container on top of the host OS The container runtime is responsible for all parts of running the container which are not already a part of the running container program itself Different container runtime solutions exist: docker lxc runc cri-o rkt containerd These runtimes are included in different container solutions.","title":"Container runtimes"},{"location":"CKAD/Containers/#understanding-oci","text":"OCI is the Open Containers Initiative that standardizes the use of containers The image-spec defines how to package a container in a filesystem bundle The runtime-spec defines how to run that filesystem in a container OCI standardization ensures compatability between containers, no matter which environment they originally come from.","title":"Understanding OCI"},{"location":"CKAD/Containers/#understanding-container-components","text":"Images are read-only environments that contain the runtime environment which includes the application and all libraries it requires. Registries are used to store images. Docker Hub is a common registry, but private registries can be also created. Containers are the isolated runtime environments where the application is running. By using namespaces , the containers can be offered as a strictly isolated environment.","title":"Understanding Container Components"},{"location":"CKAD/Containers/#starting-containers","text":"On CentOS/RHEL 7, Docker is available from the repositories. Use yum install docker to install Run docker --version to verify installation One CentOS/RHEL 8, Docker is no longer available use docker-ce from Docker.io instead. After install, use systemctl enable --now docker to start and enable the service","title":"Starting Containers"},{"location":"CKAD/exam-prep/","text":"About the exam \u00b6 The exam is primarily about speed. With that in mind, the best way to approach the moderate to complex questions is to generate the initial YAML via the --dry-run flag. Then, edit the file with either vi or nano, and then create the required resource. The steps are outlined below. $ kubectl run nginx --image = nginx --restart = Never --dry-run -o yaml > mypod.yaml $ nano mypod.yaml $ kubectl create -f mypod.yaml pod \"nginx\" created # There you go. If you're not satisfied with the results. Delete the resource, re-edit the declaritive yaml file, and redo. $ kubectl delete -f mypod.yaml pod \"nginx\" deleted $ nano mypod.yaml $ kubectl create -f mypod.yaml pod \"nginx\" created","title":"About the exam"},{"location":"CKAD/exam-prep/#about-the-exam","text":"The exam is primarily about speed. With that in mind, the best way to approach the moderate to complex questions is to generate the initial YAML via the --dry-run flag. Then, edit the file with either vi or nano, and then create the required resource. The steps are outlined below. $ kubectl run nginx --image = nginx --restart = Never --dry-run -o yaml > mypod.yaml $ nano mypod.yaml $ kubectl create -f mypod.yaml pod \"nginx\" created # There you go. If you're not satisfied with the results. Delete the resource, re-edit the declaritive yaml file, and redo. $ kubectl delete -f mypod.yaml pod \"nginx\" deleted $ nano mypod.yaml $ kubectl create -f mypod.yaml pod \"nginx\" created","title":"About the exam"},{"location":"DevOps/ansible/","text":"Ansible \u00b6 Intro \u00b6 Ansible is an open source tool that enables autoamtion, configuration and orchestration of infrastructure. Helps in: Automation of app deployment Manage multi server systems Reduce complexity When using Ansible, we build out the entire system in code and store the code in source control. Rollbacks will be available when needed and the code can be shared with other team members. Produces reliable and repeatable systems and reduces human error in spinning up the infrastructure multiple times. Written in Python and the scripting language used is YAML. Commands are sent to nodes (in parallel) via SSH and executed sequentially on each respective node. Ansible can be used in: Mass deployments As a configuration management tool to ensure identical environments when scaling to meet demands (during high spikes of traffic) Migrating environments from integration, testing and production in a reliable and dependable way. Failure prevention - As a tool for reviewing change logs and rolling back applications if failures do occur. Setup inventory machines \u00b6 Create sandbox servers that will need to be configured and managed. Inventory, child and node servers naming can be used interchangeably. These are the servers we will configure using Ansible. Can create 3 VMs using this template. Install Ansible \u00b6 Ansible can be installed via pip. $ sudo pip3 install ansible $ ansible --version ansible 2 .9.2 config file = None configured module search path = [ '~/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python3.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 3 .7.5 ( default, Nov 1 2019 , 02 :16:23 ) [ Clang 11 .0.0 ( clang-1100.0.33.8 )] To check that the control machine (Laptop) has access to the node VMs, we can SSH into those systems and confirm access to the machines. $ ssh -i ~/.ssh/ansible-2020.pem ec2-user@54.152.194.112 $ ssh -i ~/.ssh/ansible-2020.pem ec2-user@3.211.181.182 $ ssh -i ~/.ssh/ansible-2020.pem ec2-user@54.89.101.67 Setup inventory file \u00b6 Ansible must be given information on the inventory servers before we can execute commands on the node machines. The file can either be static or dynamic. Static inventory file: Create an inventory file containing the node/inventory server information. This file lists the hostnames and groups. # hosts-dev [ webservers ] 54 .152.194.112 3 .211.181.182 [ loadbalancers ] lb ansible_host = 54 .89.101.67 [ local ] control ansible_connection = local # above 2 lines code is the way Ansible communicates back to the control host. Also, this tells Ansible not to SSH into it as it is the localhost Dynamic inventory file: If you use Amazon Web Services EC2, maintaining an inventory file might not be the best approach, because hosts may come and go over time, be managed by external applications, or you might even be using AWS autoscaling. For this reason, you can use the EC2 external inventory script. You can use this script in one of two ways: The easiest is to use Ansible\u2019s -i command line option and specify the path to the script after marking it executable: $ ansible -i ec2.py -u ubuntu us-east-1d -m ping The second option is to copy the script to /etc/ansible/hosts and chmod +x it. You must also copy the ec2.ini file to /etc/ansible/ec2.ini . Then you can run ansible as you would normally. The inventory file can include inventory specific parameters like non-standard SSH port numbers or aliases using <name> ansible_host=<IP address> . Default Ansible inventory is located in /etc/ansible/hosts Reference a different inventory by using -i <path> in CLI. $ ansible --list-hosts all -i hosts-dev hosts ( 4 ) : 54 .152.194.112 3 .211.181.182 lb control Ansible Configuration \u00b6 We might need to configure our local Ansible environment with global specific properties associated with our setup. - This can be done by creating a configuration file (ansible.cfg) to control the local Ansible environmental settings. - Configuration file will search in the following order: - ANSIBLE_CONFIG (environment variable if set) - ansible.cfg (in the current directory) - ~/.ansible.cfg (in the home directory) - /etc/ansible/ansible.cfg (in the default ansible directory) - The ansible.cfg file contains important information like the location of the inventory file, default SSH key to use, default remote users to use with SSH and more. Patterns can be used to targeted hosts and groups. Here are some examples: $ ansible --list-hosts all hosts ( 4 ) : app1 app2 lb control $ ansible --list-hosts app* hosts ( 2 ) : app1 app2 $ ansible --list-hosts \"*\" hosts ( 4 ) : app1 app2 lb control $ ansible --list-hosts webservers:loadbalancers hosts ( 3 ) : app1 app2 lb $ ansible --list-hosts \\! control hosts ( 3 ) : app1 app2 lb $ ansible --list-hosts webservers: \\! app1 hosts ( 1 ) : app2 $ ansible --list-hosts webservers [ 0 ] hosts ( 1 ) : app1 Ansible Tasks \u00b6 Tasks are a way to run adhoc commands against our inventory in a one-line single executable. Tasks are the basic building blocks of Ansible's execution and configuration. Running AdHoc commands are great for troubleshooting and quick testing against the inventory. Ansible modules Ansible command Commands consist of command, options and host-pattern. As the remote user and other configuration settings are not specified, it will result in ping failure. $ ansible options <host-pattern> $ ansible -m ping all # ansible <module-flag> <module-name> <inventory> app1 | UNREACHABLE! = > { \"changed\" : false, \"msg\" : \"Failed to connect to the host via ssh: pradeepgorthi@54.152.194.112: Permission denied (publickey).\" , \"unreachable\" : true } app2 | UNREACHABLE! = > { \"changed\" : false, \"msg\" : \"Failed to connect to the host via ssh: pradeepgorthi@3.211.181.182: Permission denied (publickey).\" , \"unreachable\" : true } lb | UNREACHABLE! = > { \"changed\" : false, \"msg\" : \"Failed to connect to the host via ssh: pradeepgorthi@54.89.101.67: Permission denied (publickey).\" , \"unreachable\" : true } control | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } The results returned will give the important information about the execution on the end hosts. After adding the remote_user , private_key_file and host_key_checking information in ansible.cfg file, the pings are successful as it now knows which user and ssh keypair to use when logging in. $ ansible -m ping all control | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } lb | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } app1 | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } app2 | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } We can run shell commands using the following: $ ansible -m shell -a \"uname\" webservers:loadbalancers app1 | CHANGED | rc = 0 >> Linux app2 | CHANGED | rc = 0 >> Linux lb | CHANGED | rc = 0 >> Linux # rc translates to Return code which is 0 -> successful. # As a command ran on the remote node, status is shown as CHANGED. Playbooks \u00b6 Playbooks are a way to combine ordered processess and manage configuration needed to build out a remote system. Makes configuration management easy and gives us the ability to deploy a multi-machine setup. Playbooks can declare configurations and orchestrate steps and can ensure our remote system is configured as expected when run. The written tasks in playbook can be run synchronously or asynchronously. Gives us the ability to infra as code and manage in source control. To contruct a system using playbooks: - Package Management - packages the system needs. - package manager - patching - Example playbook: --- - hosts : loadbalancers tasks : - name : Install apache yum : name=httpd state=latest - Configuration - configure the system with necessary application files/configuration files needed. - copy files or folders from control machine to nodes - edit configuration files - Example playbook: --- - hosts : loadbalancers tasks : - name : Copy config file copy : src=./config.cfg dest=/etc/config.cfg - hosts : webservers tasks : - name : Sync folders synchronize : src=./app dest=/var/www/html/app - Service Handlers - we can create service handlers to start, stop or restart out system when changes are made. - command - service - handlers - Example playbook: --- - hosts : loadbalancers tasks : - name : Configure port number lineinfile : path=/etc/config.cfg regexp='^port' line='port=80' notify : Restart apache # apache will be restarted only if the line is changed to port=80 handlers : - name : Restart apache service : name=httpd status=restarted To do yum update : # yum-update.yml --- - hosts : webservers:loadbalancers become : true # to become sudo tasks : - name : Updating yum packages yum : name=* state=latest To install apache and php : # install-services.yml --- # install-services.yml --- - hosts : loadbalancers become : true tasks : - name : Installing apache yum : name=httpd state=present # It will check the system to see if apache is already installed, if it is nothing will be done and if it is not, apache will be installed. - name : Ensure apache starts service : name=httpd state=started enabled=yes - hosts : webservers become : true tasks : - name : Installing services yum : name : - httpd - php state : present - name : Ensure apache starts service : name : httpd state : started # start httpd service enabled : yes # to configure it to start on every boot $ ansible-playbook playbooks/install-services.yml PLAY [ loadbalancers ] *********************************************************************************************************************************************************** TASK [ Gathering Facts ] ********************************************************************************************************************************************************* ok: [ lb ] TASK [ Installing apache ] ******************************************************************************************************************************************************* ok: [ lb ] TASK [ Ensure apache starts ] **************************************************************************************************************************************************** changed: [ lb ] PLAY [ webservers ] ************************************************************************************************************************************************************** TASK [ Gathering Facts ] ********************************************************************************************************************************************************* ok: [ app1 ] ok: [ app2 ] TASK [ Installing services ] ***************************************************************************************************************************************************** ok: [ app1 ] ok: [ app2 ] TASK [ Ensure apache starts ] **************************************************************************************************************************************************** changed: [ app1 ] changed: [ app2 ] PLAY RECAP ********************************************************************************************************************************************************************* app1 : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0 app2 : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0 lb : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0 To upload files to node servers: # setup-app.yml --- - hosts : webservers become : true tasks : - name : Upload index file copy : src : ../index.php dest : /var/www/html mode : 0755 - name : Configure php.ini file lineinfile : # Change a line in a config file matching the regex to the line value. path : /etc/php.ini regexp : ^short_open_tag line : 'short_open_tag=Off' notify : restart-apache # restart apache handlers : # Declaring restart apache task as a handler and adding notify in configure task. - name : restart-apache service : name : httpd state : restarted Load balance playbook using the template file: # setup-lb.yml --- - hosts : loadbalancers become : true tasks : - name : Creating template template : src : ../config/lb-config.j2 dest : /etc/httpd/conf.d/lb.conf owner : bin group : wheel mode : 064 notify : restart apache handlers : # Declaring restart apache task as a handler and adding notify in configure task. - name : restart apache service : name=httpd state=restarted Template file to be used by Ansible to dynamically configure the Loadbalancer: # Loadbalancer config that can be read by Ansible is written in Jinja2 ProxyRequests off <Proxy balancer://webcluster > {% for hosts in groups [ 'webservers' ] %} BalancerMember http:// {{ hostvars [ hosts ][ 'ansible_host' ] }} {% endfor %} ProxySet lbmethod=byrequests </Proxy> Running all playbooks in a single file from start to finish for a cleaner approach. # all-playbooks.yml --- - import_playbook : yum-update.yml - import_playbook : install-services.yml - import_playbook : setup-app.yml - import_playbook : setup-lb.yml To check status of httpd service: # check-status.yml --- - hosts : webservers:loadbalancers become : true tasks : - name : Check apache status command : service httpd status Ansible provides variables and metadata about the host that we interact with when running playbooks. - During the TASK[Gathering facts] step, variables get populated. - Gathers useful facts about the host and can be used in playbooks. - Use the status module to see all of the facts gathered during TASK[Gathering facts] step. - Uses jinja2 templating to evaluate these expressions. Example Adhoc command: $ ansible -m setup app1 # Gives all the variables associated with it and we can use them in playbooks $ ansible -m setup app1 app 1 | SUCCESS => { \"ansible_facts\" : { \"ansible_all_ipv4_addresses\" : [ \"172.31.94.190\" ], \"ansible_all_ipv6_addresses\" : [ \"fe80::10f3:50ff:fea3:9151\" ], \"ansible_apparmor\" : { \"status\" : \"disabled\" }, \"ansible_architecture\" : \"x86_64\" , \"ansible_bios_date\" : \"08/24/2006\" , \"ansible_bios_version\" : \"4.2.amazon\" , \"ansible_cmdline\" : { \"console\" : \"ttyS0\" , \"nvme_core.io_timeout\" : \"4294967295\" , \"root\" : \"LABEL=/\" , \"selinux\" : \"0\" }, \"ansible_date_time\" : { \"date\" : \"2019-12-17\" , \"day\" : \"17\" , \"epoch\" : \"1576552252\" , \"hour\" : \"03\" , \"iso8601\" : \"2019-12-17T03:10:52Z\" , \"iso8601_basic\" : \"20191217T031052467561\" , \"iso8601_basic_short\" : \"20191217T031052\" , \"iso8601_micro\" : \"2019-12-17T03:10:52.467633Z\" , \"minute\" : \"10\" , \"month\" : \"12\" , \"second\" : \"52\" , \"time\" : \"03:10:52\" , \"tz\" : \"UTC\" , \"tz_offset\" : \"+0000\" , \"weekday\" : \"Tuesday\" , \"weekday_number\" : \"2\" , \"weeknumber\" : \"50\" , \"year\" : \"2019\" }, \"ansible_default_ipv4\" : { \"address\" : \"172.31.94.190\" , \"alias\" : \"eth0\" , \"broadcast\" : \"172.31.95.255\" , \"gateway\" : \"172.31.80.1\" , \"interface\" : \"eth0\" , \"macaddress\" : \"12:f3:50:a3:91:51\" , \"mtu\" : 9001 , \"netmask\" : \"255.255.240.0\" , \"network\" : \"172.31.80.0\" , \"type\" : \"ether\" }, \"ansible_default_ipv6\" : {}, \"ansible_device_links\" : { \"ids\" : {}, \"labels\" : { \"xvda1\" : [ \"\\\\x2f\" ] }, \"masters\" : {}, \"uuids\" : { \"xvda1\" : [ \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" ] } }, \"ansible_devices\" : { \"xvda\" : { \"holders\" : [], \"host\" : \"\" , \"links\" : { \"ids\" : [], \"labels\" : [], \"masters\" : [], \"uuids\" : [] }, \"model\" : null , \"partitions\" : { \"xvda1\" : { \"holders\" : [], \"links\" : { \"ids\" : [], \"labels\" : [ \"\\\\x2f\" ], \"masters\" : [], \"uuids\" : [ \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" ] }, \"sectors\" : \"16773087\" , \"sectorsize\" : 512 , \"size\" : \"8.00 GB\" , \"start\" : \"4096\" , \"uuid\" : \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" } }, \"removable\" : \"0\" , \"rotational\" : \"0\" , \"sas_address\" : null , \"sas_device_handle\" : null , \"scheduler_mode\" : \"noop\" , \"sectors\" : \"16777216\" , \"sectorsize\" : \"512\" , \"size\" : \"8.00 GB\" , \"support_discard\" : \"0\" , \"vendor\" : null , \"virtual\" : 1 } }, \"ansible_distribution\" : \"Amazon\" , \"ansible_distribution_file_parsed\" : true , \"ansible_distribution_file_path\" : \"/etc/system-release\" , \"ansible_distribution_file_variety\" : \"Amazon\" , \"ansible_distribution_major_version\" : \"2018\" , \"ansible_distribution_release\" : \"NA\" , \"ansible_distribution_version\" : \"NA\" , \"ansible_dns\" : { \"nameservers\" : [ \"172.31.0.2\" ], \"options\" : { \"attempts\" : \"5\" , \"timeout\" : \"2\" }, \"search\" : [ \"ec2.internal\" ] }, \"ansible_domain\" : \"ec2.internal\" , \"ansible_effective_group_id\" : 500 , \"ansible_effective_user_id\" : 500 , \"ansible_env\" : { \"AWS_AUTO_SCALING_HOME\" : \"/opt/aws/apitools/as\" , \"AWS_CLOUDWATCH_HOME\" : \"/opt/aws/apitools/mon\" , \"AWS_ELB_HOME\" : \"/opt/aws/apitools/elb\" , \"AWS_PATH\" : \"/opt/aws\" , \"EC2_AMITOOL_HOME\" : \"/opt/aws/amitools/ec2\" , \"EC2_HOME\" : \"/opt/aws/apitools/ec2\" , \"HOME\" : \"/home/ec2-user\" , \"JAVA_HOME\" : \"/usr/lib/jvm/jre\" , \"LANG\" : \"en_US.UTF-8\" , \"LESSOPEN\" : \"||/usr/bin/lesspipe.sh %s\" , \"LESS_TERMCAP_mb\" : \"\\u001b[01;31m\" , \"LESS_TERMCAP_md\" : \"\\u001b[01;38;5;208m\" , \"LESS_TERMCAP_me\" : \"\\u001b[0m\" , \"LESS_TERMCAP_se\" : \"\\u001b[0m\" , \"LESS_TERMCAP_ue\" : \"\\u001b[0m\" , \"LESS_TERMCAP_us\" : \"\\u001b[04;38;5;111m\" , \"LOGNAME\" : \"ec2-user\" , \"MAIL\" : \"/var/mail/ec2-user\" , \"PATH\" : \"/usr/local/bin:/bin:/usr/bin:/opt/aws/bin\" , \"PWD\" : \"/home/ec2-user\" , \"SHELL\" : \"/bin/bash\" , \"SHLVL\" : \"2\" , \"SSH_CLIENT\" : \"24.187.17.118 60904 22\" , \"SSH_CONNECTION\" : \"24.187.17.118 60904 172.31.94.190 22\" , \"SSH_TTY\" : \"/dev/pts/0\" , \"TERM\" : \"xterm-256color\" , \"USER\" : \"ec2-user\" , \"_\" : \"/usr/bin/python\" }, \"ansible_eth0\" : { \"active\" : true , \"device\" : \"eth0\" , \"features\" : { \"esp_hw_offload\" : \"off [fixed]\" , \"esp_tx_csum_hw_offload\" : \"off [fixed]\" , \"fcoe_mtu\" : \"off [fixed]\" , \"generic_receive_offload\" : \"on\" , \"generic_segmentation_offload\" : \"on\" , \"highdma\" : \"off [fixed]\" , \"hw_tc_offload\" : \"off [fixed]\" , \"l2_fwd_offload\" : \"off [fixed]\" , \"large_receive_offload\" : \"off [fixed]\" , \"loopback\" : \"off [fixed]\" , \"netns_local\" : \"off [fixed]\" , \"ntuple_filters\" : \"off [fixed]\" , \"receive_hashing\" : \"off [fixed]\" , \"rx_all\" : \"off [fixed]\" , \"rx_checksumming\" : \"on [fixed]\" , \"rx_fcs\" : \"off [fixed]\" , \"rx_udp_tunnel_port_offload\" : \"off [fixed]\" , \"rx_vlan_filter\" : \"off [fixed]\" , \"rx_vlan_offload\" : \"off [fixed]\" , \"rx_vlan_stag_filter\" : \"off [fixed]\" , \"rx_vlan_stag_hw_parse\" : \"off [fixed]\" , \"scatter_gather\" : \"on\" , \"tcp_segmentation_offload\" : \"on\" , \"tx_checksum_fcoe_crc\" : \"off [fixed]\" , \"tx_checksum_ip_generic\" : \"off [fixed]\" , \"tx_checksum_ipv4\" : \"on [fixed]\" , \"tx_checksum_ipv6\" : \"off [requested on]\" , \"tx_checksum_sctp\" : \"off [fixed]\" , \"tx_checksumming\" : \"on\" , \"tx_esp_segmentation\" : \"off [fixed]\" , \"tx_fcoe_segmentation\" : \"off [fixed]\" , \"tx_gre_csum_segmentation\" : \"off [fixed]\" , \"tx_gre_segmentation\" : \"off [fixed]\" , \"tx_gso_partial\" : \"off [fixed]\" , \"tx_gso_robust\" : \"on [fixed]\" , \"tx_ipxip4_segmentation\" : \"off [fixed]\" , \"tx_ipxip6_segmentation\" : \"off [fixed]\" , \"tx_lockless\" : \"off [fixed]\" , \"tx_nocache_copy\" : \"off\" , \"tx_scatter_gather\" : \"on\" , \"tx_scatter_gather_fraglist\" : \"off [fixed]\" , \"tx_sctp_segmentation\" : \"off [fixed]\" , \"tx_tcp6_segmentation\" : \"off [requested on]\" , \"tx_tcp_ecn_segmentation\" : \"off [fixed]\" , \"tx_tcp_mangleid_segmentation\" : \"off\" , \"tx_tcp_segmentation\" : \"on\" , \"tx_udp_tnl_csum_segmentation\" : \"off [fixed]\" , \"tx_udp_tnl_segmentation\" : \"off [fixed]\" , \"tx_vlan_offload\" : \"off [fixed]\" , \"tx_vlan_stag_hw_insert\" : \"off [fixed]\" , \"udp_fragmentation_offload\" : \"off\" , \"vlan_challenged\" : \"off [fixed]\" }, \"hw_timestamp_filters\" : [], \"ipv4\" : { \"address\" : \"172.31.94.190\" , \"broadcast\" : \"172.31.95.255\" , \"netmask\" : \"255.255.240.0\" , \"network\" : \"172.31.80.0\" }, \"ipv6\" : [ { \"address\" : \"fe80::10f3:50ff:fea3:9151\" , \"prefix\" : \"64\" , \"scope\" : \"link\" } ], \"macaddress\" : \"12:f3:50:a3:91:51\" , \"module\" : \"xen_netfront\" , \"mtu\" : 9001 , \"pciid\" : \"vif-0\" , \"promisc\" : false , \"timestamping\" : [ \"rx_software\" , \"software\" ], \"type\" : \"ether\" }, \"ansible_fibre_channel_wwn\" : [], \"ansible_fips\" : false , \"ansible_form_factor\" : \"Other\" , \"ansible_fqdn\" : \"ip-172-31-94-190.ec2.internal\" , \"ansible_hostname\" : \"ip-172-31-94-190\" , \"ansible_hostnqn\" : \"\" , \"ansible_interfaces\" : [ \"lo\" , \"eth0\" ], \"ansible_is_chroot\" : false , \"ansible_iscsi_iqn\" : \"\" , \"ansible_kernel\" : \"4.14.62-65.117.amzn1.x86_64\" , \"ansible_kernel_version\" : \"#1 SMP Fri Aug 10 20:03:52 UTC 2018\" , \"ansible_lo\" : { \"active\" : true , \"device\" : \"lo\" , \"features\" : { \"esp_hw_offload\" : \"off [fixed]\" , \"esp_tx_csum_hw_offload\" : \"off [fixed]\" , \"fcoe_mtu\" : \"off [fixed]\" , \"generic_receive_offload\" : \"on\" , \"generic_segmentation_offload\" : \"on\" , \"highdma\" : \"on [fixed]\" , \"hw_tc_offload\" : \"off [fixed]\" , \"l2_fwd_offload\" : \"off [fixed]\" , \"large_receive_offload\" : \"off [fixed]\" , \"loopback\" : \"on [fixed]\" , \"netns_local\" : \"on [fixed]\" , \"ntuple_filters\" : \"off [fixed]\" , \"receive_hashing\" : \"off [fixed]\" , \"rx_all\" : \"off [fixed]\" , \"rx_checksumming\" : \"on [fixed]\" , \"rx_fcs\" : \"off [fixed]\" , \"rx_udp_tunnel_port_offload\" : \"off [fixed]\" , \"rx_vlan_filter\" : \"off [fixed]\" , \"rx_vlan_offload\" : \"off [fixed]\" , \"rx_vlan_stag_filter\" : \"off [fixed]\" , \"rx_vlan_stag_hw_parse\" : \"off [fixed]\" , \"scatter_gather\" : \"on\" , \"tcp_segmentation_offload\" : \"on\" , \"tx_checksum_fcoe_crc\" : \"off [fixed]\" , \"tx_checksum_ip_generic\" : \"on [fixed]\" , \"tx_checksum_ipv4\" : \"off [fixed]\" , \"tx_checksum_ipv6\" : \"off [fixed]\" , \"tx_checksum_sctp\" : \"on [fixed]\" , \"tx_checksumming\" : \"on\" , \"tx_esp_segmentation\" : \"off [fixed]\" , \"tx_fcoe_segmentation\" : \"off [fixed]\" , \"tx_gre_csum_segmentation\" : \"off [fixed]\" , \"tx_gre_segmentation\" : \"off [fixed]\" , \"tx_gso_partial\" : \"off [fixed]\" , \"tx_gso_robust\" : \"off [fixed]\" , \"tx_ipxip4_segmentation\" : \"off [fixed]\" , \"tx_ipxip6_segmentation\" : \"off [fixed]\" , \"tx_lockless\" : \"on [fixed]\" , \"tx_nocache_copy\" : \"off [fixed]\" , \"tx_scatter_gather\" : \"on [fixed]\" , \"tx_scatter_gather_fraglist\" : \"on [fixed]\" , \"tx_sctp_segmentation\" : \"on\" , \"tx_tcp6_segmentation\" : \"on\" , \"tx_tcp_ecn_segmentation\" : \"on\" , \"tx_tcp_mangleid_segmentation\" : \"on\" , \"tx_tcp_segmentation\" : \"on\" , \"tx_udp_tnl_csum_segmentation\" : \"off [fixed]\" , \"tx_udp_tnl_segmentation\" : \"off [fixed]\" , \"tx_vlan_offload\" : \"off [fixed]\" , \"tx_vlan_stag_hw_insert\" : \"off [fixed]\" , \"udp_fragmentation_offload\" : \"off\" , \"vlan_challenged\" : \"on [fixed]\" }, \"hw_timestamp_filters\" : [], \"ipv4\" : { \"address\" : \"127.0.0.1\" , \"broadcast\" : \"host\" , \"netmask\" : \"255.0.0.0\" , \"network\" : \"127.0.0.0\" }, \"ipv6\" : [ { \"address\" : \"::1\" , \"prefix\" : \"128\" , \"scope\" : \"host\" } ], \"mtu\" : 65536 , \"promisc\" : false , \"timestamping\" : [ \"tx_software\" , \"rx_software\" , \"software\" ], \"type\" : \"loopback\" }, \"ansible_local\" : {}, \"ansible_lsb\" : {}, \"ansible_machine\" : \"x86_64\" , \"ansible_machine_id\" : \"fef67c45adfb1d65225893255df2b04b\" , \"ansible_memfree_mb\" : 224 , \"ansible_memory_mb\" : { \"nocache\" : { \"free\" : 868 , \"used\" : 117 }, \"real\" : { \"free\" : 224 , \"total\" : 985 , \"used\" : 761 }, \"swap\" : { \"cached\" : 0 , \"free\" : 0 , \"total\" : 0 , \"used\" : 0 } }, \"ansible_memtotal_mb\" : 985 , \"ansible_mounts\" : [ { \"block_available\" : 1632826 , \"block_size\" : 4096 , \"block_total\" : 2030953 , \"block_used\" : 398127 , \"device\" : \"/dev/xvda1\" , \"fstype\" : \"ext4\" , \"inode_available\" : 483722 , \"inode_total\" : 524288 , \"inode_used\" : 40566 , \"mount\" : \"/\" , \"options\" : \"rw,noatime,data=ordered\" , \"size_available\" : 6688055296 , \"size_total\" : 8318783488 , \"uuid\" : \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" } ], \"ansible_nodename\" : \"ip-172-31-94-190\" , \"ansible_os_family\" : \"RedHat\" , \"ansible_pkg_mgr\" : \"yum\" , \"ansible_proc_cmdline\" : { \"console\" : [ \"tty1\" , \"ttyS0\" ], \"nvme_core.io_timeout\" : \"4294967295\" , \"root\" : \"LABEL=/\" , \"selinux\" : \"0\" }, \"ansible_processor\" : [ \"0\" , \"GenuineIntel\" , \"Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz\" ], \"ansible_processor_cores\" : 1 , \"ansible_processor_count\" : 1 , \"ansible_processor_threads_per_core\" : 1 , \"ansible_processor_vcpus\" : 1 , \"ansible_product_name\" : \"HVM domU\" , \"ansible_product_serial\" : \"NA\" , \"ansible_product_uuid\" : \"NA\" , \"ansible_product_version\" : \"4.2.amazon\" , \"ansible_python\" : { \"executable\" : \"/usr/bin/python\" , \"has_sslcontext\" : true , \"type\" : \"CPython\" , \"version\" : { \"major\" : 2 , \"micro\" : 16 , \"minor\" : 7 , \"releaselevel\" : \"final\" , \"serial\" : 0 }, \"version_info\" : [ 2 , 7 , 16 , \"final\" , 0 ] }, \"ansible_python_version\" : \"2.7.16\" , \"ansible_real_group_id\" : 500 , \"ansible_real_user_id\" : 500 , \"ansible_selinux\" : { \"status\" : \"Missing selinux Python library\" }, \"ansible_selinux_python_present\" : false , \"ansible_service_mgr\" : \"upstart\" , \"ansible_ssh_host_key_dsa_public\" : \"AAAAB3NzaC1kc3MAAACBAKmcc4NBxWTiOqX/jdg01Ywl1xYmHKFES13l3gPDstl1jOkFkCjJTZhY1hnCywOXC/yYWd1dlBAf3U1rnrGsEWG98/gGHamBsR6ykuPe2pxI9znh46V1xYpRpY1RfjZZ/8D7aEPHXIBDpCsUxHlF41gvX2EhVpIIioJeOwF5uz8zAAAAFQCRBwQz9E5URC/umtsp+si9VNsGjwAAAIBpxNzHbhGIab4+YpLc1Mt1EucWH202OcaYPHN8Rzyxx0BMp4Pqh0yvlAo8tfwOzDUxVYCPimSzXKRHwobeXy/hmv9DMekUT+DYFFSpDYTQn23rurihuHxTM3StGourHhq3iTk00B+X1qKF69V9O1HwPKR8VYbQnSNgnJ81NdJN6AAAAIAc+TnOyu3TXLHw0IXJEPmUqi2sOBgHNEWbtqEybhFIJKmXGrfYF6CPJLlwdtd0rIR3iPz0du1CKvBXAKY50+y29v8gDHTQdYbnDLeYxqpMf+xFEjfuSU3rmbRA/ealoG3P+dpR5DhhOuIN1beRMZFarArqtMBmTRiCqAioRIEz5A==\" , \"ansible_ssh_host_key_ecdsa_public\" : \"AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBI95vsHxkXdL4JjueQ/F8E7tfr5dUhH8kdvnnXiu0QvEKo8wJVV6cBGnBKuvckEesAoOc+55QWwn8RviHp/LfyU=\" , \"ansible_ssh_host_key_ed25519_public\" : \"AAAAC3NzaC1lZDI1NTE5AAAAIBCcSbXTmYudwPkmhmShysni/n0CRfdcPvY9l6NN095B\" , \"ansible_ssh_host_key_rsa_public\" : \"AAAAB3NzaC1yc2EAAAADAQABAAABAQDn03D+cJVjB7XOMu62K7xTTwoYNeMWa2lSmgoFJSlxvCYw9eCqlt1XPFAAoHmlYvBhPmCXATu2v6f1XWQRaD4VYR1uopDh6Ku9WhBOyYk5FGkJWgyM5Vm+TTrd6Btx5d/+w8tWUIT1BQ4V0pQYSeYRwZilfs+AA8mYRFrlf6+Bqvzn1hGGlRnC5PD4JTbjCT6ys22Il1RSIwWYHtx+zHfCJTrcP8qUcYB9mftJspWrwyOSw1zTCQBs8yQazlxLlzy3XsGK8zgJEtA5vQvosibPGYckfp/TM7SIYzY/pLz2IhQiaeHwdmInlvvSssUlJ6taTBiLga7LDQDmDH7G9sLn\" , \"ansible_swapfree_mb\" : 0 , \"ansible_swaptotal_mb\" : 0 , \"ansible_system\" : \"Linux\" , \"ansible_system_capabilities\" : [ \"\" ], \"ansible_system_capabilities_enforced\" : \"True\" , \"ansible_system_vendor\" : \"Xen\" , \"ansible_uptime_seconds\" : 11917 , \"ansible_user_dir\" : \"/home/ec2-user\" , \"ansible_user_gecos\" : \"EC2 Default User\" , \"ansible_user_gid\" : 500 , \"ansible_user_id\" : \"ec2-user\" , \"ansible_user_shell\" : \"/bin/bash\" , \"ansible_user_uid\" : 500 , \"ansible_userspace_architecture\" : \"x86_64\" , \"ansible_userspace_bits\" : \"64\" , \"ansible_virtualization_role\" : \"guest\" , \"ansible_virtualization_type\" : \"xen\" , \"discovered_interpreter_python\" : \"/usr/bin/python\" , \"gather_subset\" : [ \"all\" ], \"module_setup\" : true }, \"changed\" : false } Example playbook: - name : Add webserver info copy : dest : /var/www/html/info.php content : \"{{ ansible_hostname }}\" We can create local variables within the playbooks. Create playbook variables using vars to create key/value pairs and dictionary/map of variables. Can reference variables directly in playbooks. Can create variable files and import them into the playbooks. Example playbook: vars : html_path : \"/var/www/html\" new_var : \"repeated information\" tasks : - name : Add webserver information copy : dest : \"{{ html_path }}/info.php\" content : \"{{ new_var }}\" Ansible also gives us the ability to register variables from tasks that run to get information about its execution. Create variables from info returned from tasks ran using register . Call registered variables for later use. Use debug module anytime to see variables and debug our playbooks. Example playbook: vars : html_path : \"/var/www/html\" tasks : - name : See directory contents command : ls -la {{ html_path }} register : dir_contents # Output/information of execution of the above command is stored in this variable - name : Debug dir contents debug : msg : \"{{ dir_contents }}\" Variables can also be stored and used from command line, variables in external files or in inventory files. Roles \u00b6 Ansible provides roles framework that makes each part of variables, tasks, templates and modules fully independent. Helps group tasks together in a way that is self containing. Clean and predefined directory structure. Break up the configurations into files. Re-use code by others who need similar configurations. Easy to modify and reduces syntax errors. Ansible helps in creating the directory structure using ansible-galaxy init <dir_path> ansible-galaxy init |dir_path| $ ansible-galaxy init roles/webservers roles/ \u2514\u2500\u2500 webservers \u251c\u2500\u2500 README.md \u251c\u2500\u2500 defaults \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 files \u251c\u2500\u2500 handlers \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 templates \u251c\u2500\u2500 tests \u2502 \u251c\u2500\u2500 inventory \u2502 \u2514\u2500\u2500 test.yml \u2514\u2500\u2500 vars \u2514\u2500\u2500 main.yml . \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 hosts-dev \u251c\u2500\u2500 index.php \u251c\u2500\u2500 roles \u2502 \u2514\u2500\u2500 webservers \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 defaults \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 files \u2502 \u2502 \u2514\u2500\u2500 index.php \u2502 \u251c\u2500\u2500 handlers \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 meta \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 tasks \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 tests \u2502 \u2502 \u251c\u2500\u2500 inventory \u2502 \u2502 \u2514\u2500\u2500 test.yml \u2502 \u2514\u2500\u2500 vars \u2502 \u2514\u2500\u2500 main.yml \u2514\u2500\u2500 setup-app.yml # ansible.cfg [defaults] inventory = ./hosts-dev remote_user = ec2-user private_key_file = ~/.ssh/ansible-2020.pem host_key_checking = False # hosts-dev [webservers] app1 ansible_host=54.152.194.112 app2 ansible_host=3.211.181.182 [loadbalancers] lb ansible_host=54.89.101.67 [local] control ansible_connection=local # above 2 lines code is the way Ansible communicates back to the control host and so this is important # index.php <?php echo \"<h1>Hello, World! This is my Ansible page.</h1>\"; ?> # setup-app.yml --- - hosts: webservers become: true roles: - webservers Check Mode (Dry run) \u00b6 Reports changes that Ansible would have to make on the end hosts rather than applying the changes. Commands are run without affecting the remote system and reports changes. Great for one node at a time and used for basic configuration management use cases. $ ansible-playbook setup-app.yml --check Error Handling in playbooks \u00b6 Change the default behavior of Ansible when certain events happen that may or may not need to report as a failure or changed status. Sometimes non-zero exit code is fine. Sometimes commands might not need to report a changed status. Explicitly force Ansible to ignore errors or changes that occur. Shell Module and Command Module always return a changed status when run and even when no changes are made to the node server # check-status.yml --- - hosts : webservers:loadbalancers become : true tasks : - name : Check apache status command : service httpd status changed_when : false # Ignores if changes occur - name : this will not fail command : /bin/false # always returns non-zero exit code (fail status) ignore_errors : yes # Ignore errors Tags \u00b6 Assigning tags to specific tasks in playbooks allows you to only call certain tasks in a very long playbook. It only runs specific parts of a playbook rather than all of the plays. Add tags to any tasks and re-use if needed. Specify the tags you want to run (or not to) on the command line. After adding tags: <tag_name> to any of the tasks, it can be called (or not) using: $ ansible-playbook setup-app.yml --tags <tag_name> $ ansible-playbook setup-app.yml --skip-tags <tag_name> Ansible Vault \u00b6 Ansible vault is a way to keep sensitive information in encrypted files and not plain text in playbooks. Keeps passwords, keys and other sensitive variables in encrypted vault files. Vault files can be shared through source control. Password protected and default cipher is AES. Encrypted data file can be created using: $ ansible-vault create secret-variables.yml # create encrypted file $ ansible-vault edit secret-variables.yml # edit encrypted file $ ansible-playbook setup-app.yml --ask-vault-pass # prompt for password to use the encrypted secret variables file","title":"Ansible"},{"location":"DevOps/ansible/#ansible","text":"","title":"Ansible"},{"location":"DevOps/ansible/#intro","text":"Ansible is an open source tool that enables autoamtion, configuration and orchestration of infrastructure. Helps in: Automation of app deployment Manage multi server systems Reduce complexity When using Ansible, we build out the entire system in code and store the code in source control. Rollbacks will be available when needed and the code can be shared with other team members. Produces reliable and repeatable systems and reduces human error in spinning up the infrastructure multiple times. Written in Python and the scripting language used is YAML. Commands are sent to nodes (in parallel) via SSH and executed sequentially on each respective node. Ansible can be used in: Mass deployments As a configuration management tool to ensure identical environments when scaling to meet demands (during high spikes of traffic) Migrating environments from integration, testing and production in a reliable and dependable way. Failure prevention - As a tool for reviewing change logs and rolling back applications if failures do occur.","title":"Intro"},{"location":"DevOps/ansible/#setup-inventory-machines","text":"Create sandbox servers that will need to be configured and managed. Inventory, child and node servers naming can be used interchangeably. These are the servers we will configure using Ansible. Can create 3 VMs using this template.","title":"Setup inventory machines"},{"location":"DevOps/ansible/#install-ansible","text":"Ansible can be installed via pip. $ sudo pip3 install ansible $ ansible --version ansible 2 .9.2 config file = None configured module search path = [ '~/.ansible/plugins/modules' , '/usr/share/ansible/plugins/modules' ] ansible python module location = /usr/local/lib/python3.7/site-packages/ansible executable location = /usr/local/bin/ansible python version = 3 .7.5 ( default, Nov 1 2019 , 02 :16:23 ) [ Clang 11 .0.0 ( clang-1100.0.33.8 )] To check that the control machine (Laptop) has access to the node VMs, we can SSH into those systems and confirm access to the machines. $ ssh -i ~/.ssh/ansible-2020.pem ec2-user@54.152.194.112 $ ssh -i ~/.ssh/ansible-2020.pem ec2-user@3.211.181.182 $ ssh -i ~/.ssh/ansible-2020.pem ec2-user@54.89.101.67","title":"Install Ansible"},{"location":"DevOps/ansible/#setup-inventory-file","text":"Ansible must be given information on the inventory servers before we can execute commands on the node machines. The file can either be static or dynamic. Static inventory file: Create an inventory file containing the node/inventory server information. This file lists the hostnames and groups. # hosts-dev [ webservers ] 54 .152.194.112 3 .211.181.182 [ loadbalancers ] lb ansible_host = 54 .89.101.67 [ local ] control ansible_connection = local # above 2 lines code is the way Ansible communicates back to the control host. Also, this tells Ansible not to SSH into it as it is the localhost Dynamic inventory file: If you use Amazon Web Services EC2, maintaining an inventory file might not be the best approach, because hosts may come and go over time, be managed by external applications, or you might even be using AWS autoscaling. For this reason, you can use the EC2 external inventory script. You can use this script in one of two ways: The easiest is to use Ansible\u2019s -i command line option and specify the path to the script after marking it executable: $ ansible -i ec2.py -u ubuntu us-east-1d -m ping The second option is to copy the script to /etc/ansible/hosts and chmod +x it. You must also copy the ec2.ini file to /etc/ansible/ec2.ini . Then you can run ansible as you would normally. The inventory file can include inventory specific parameters like non-standard SSH port numbers or aliases using <name> ansible_host=<IP address> . Default Ansible inventory is located in /etc/ansible/hosts Reference a different inventory by using -i <path> in CLI. $ ansible --list-hosts all -i hosts-dev hosts ( 4 ) : 54 .152.194.112 3 .211.181.182 lb control","title":"Setup inventory file"},{"location":"DevOps/ansible/#ansible-configuration","text":"We might need to configure our local Ansible environment with global specific properties associated with our setup. - This can be done by creating a configuration file (ansible.cfg) to control the local Ansible environmental settings. - Configuration file will search in the following order: - ANSIBLE_CONFIG (environment variable if set) - ansible.cfg (in the current directory) - ~/.ansible.cfg (in the home directory) - /etc/ansible/ansible.cfg (in the default ansible directory) - The ansible.cfg file contains important information like the location of the inventory file, default SSH key to use, default remote users to use with SSH and more. Patterns can be used to targeted hosts and groups. Here are some examples: $ ansible --list-hosts all hosts ( 4 ) : app1 app2 lb control $ ansible --list-hosts app* hosts ( 2 ) : app1 app2 $ ansible --list-hosts \"*\" hosts ( 4 ) : app1 app2 lb control $ ansible --list-hosts webservers:loadbalancers hosts ( 3 ) : app1 app2 lb $ ansible --list-hosts \\! control hosts ( 3 ) : app1 app2 lb $ ansible --list-hosts webservers: \\! app1 hosts ( 1 ) : app2 $ ansible --list-hosts webservers [ 0 ] hosts ( 1 ) : app1","title":"Ansible Configuration"},{"location":"DevOps/ansible/#ansible-tasks","text":"Tasks are a way to run adhoc commands against our inventory in a one-line single executable. Tasks are the basic building blocks of Ansible's execution and configuration. Running AdHoc commands are great for troubleshooting and quick testing against the inventory. Ansible modules Ansible command Commands consist of command, options and host-pattern. As the remote user and other configuration settings are not specified, it will result in ping failure. $ ansible options <host-pattern> $ ansible -m ping all # ansible <module-flag> <module-name> <inventory> app1 | UNREACHABLE! = > { \"changed\" : false, \"msg\" : \"Failed to connect to the host via ssh: pradeepgorthi@54.152.194.112: Permission denied (publickey).\" , \"unreachable\" : true } app2 | UNREACHABLE! = > { \"changed\" : false, \"msg\" : \"Failed to connect to the host via ssh: pradeepgorthi@3.211.181.182: Permission denied (publickey).\" , \"unreachable\" : true } lb | UNREACHABLE! = > { \"changed\" : false, \"msg\" : \"Failed to connect to the host via ssh: pradeepgorthi@54.89.101.67: Permission denied (publickey).\" , \"unreachable\" : true } control | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } The results returned will give the important information about the execution on the end hosts. After adding the remote_user , private_key_file and host_key_checking information in ansible.cfg file, the pings are successful as it now knows which user and ssh keypair to use when logging in. $ ansible -m ping all control | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } lb | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } app1 | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } app2 | SUCCESS = > { \"ansible_facts\" : { \"discovered_interpreter_python\" : \"/usr/bin/python\" } , \"changed\" : false, \"ping\" : \"pong\" } We can run shell commands using the following: $ ansible -m shell -a \"uname\" webservers:loadbalancers app1 | CHANGED | rc = 0 >> Linux app2 | CHANGED | rc = 0 >> Linux lb | CHANGED | rc = 0 >> Linux # rc translates to Return code which is 0 -> successful. # As a command ran on the remote node, status is shown as CHANGED.","title":"Ansible Tasks"},{"location":"DevOps/ansible/#playbooks","text":"Playbooks are a way to combine ordered processess and manage configuration needed to build out a remote system. Makes configuration management easy and gives us the ability to deploy a multi-machine setup. Playbooks can declare configurations and orchestrate steps and can ensure our remote system is configured as expected when run. The written tasks in playbook can be run synchronously or asynchronously. Gives us the ability to infra as code and manage in source control. To contruct a system using playbooks: - Package Management - packages the system needs. - package manager - patching - Example playbook: --- - hosts : loadbalancers tasks : - name : Install apache yum : name=httpd state=latest - Configuration - configure the system with necessary application files/configuration files needed. - copy files or folders from control machine to nodes - edit configuration files - Example playbook: --- - hosts : loadbalancers tasks : - name : Copy config file copy : src=./config.cfg dest=/etc/config.cfg - hosts : webservers tasks : - name : Sync folders synchronize : src=./app dest=/var/www/html/app - Service Handlers - we can create service handlers to start, stop or restart out system when changes are made. - command - service - handlers - Example playbook: --- - hosts : loadbalancers tasks : - name : Configure port number lineinfile : path=/etc/config.cfg regexp='^port' line='port=80' notify : Restart apache # apache will be restarted only if the line is changed to port=80 handlers : - name : Restart apache service : name=httpd status=restarted To do yum update : # yum-update.yml --- - hosts : webservers:loadbalancers become : true # to become sudo tasks : - name : Updating yum packages yum : name=* state=latest To install apache and php : # install-services.yml --- # install-services.yml --- - hosts : loadbalancers become : true tasks : - name : Installing apache yum : name=httpd state=present # It will check the system to see if apache is already installed, if it is nothing will be done and if it is not, apache will be installed. - name : Ensure apache starts service : name=httpd state=started enabled=yes - hosts : webservers become : true tasks : - name : Installing services yum : name : - httpd - php state : present - name : Ensure apache starts service : name : httpd state : started # start httpd service enabled : yes # to configure it to start on every boot $ ansible-playbook playbooks/install-services.yml PLAY [ loadbalancers ] *********************************************************************************************************************************************************** TASK [ Gathering Facts ] ********************************************************************************************************************************************************* ok: [ lb ] TASK [ Installing apache ] ******************************************************************************************************************************************************* ok: [ lb ] TASK [ Ensure apache starts ] **************************************************************************************************************************************************** changed: [ lb ] PLAY [ webservers ] ************************************************************************************************************************************************************** TASK [ Gathering Facts ] ********************************************************************************************************************************************************* ok: [ app1 ] ok: [ app2 ] TASK [ Installing services ] ***************************************************************************************************************************************************** ok: [ app1 ] ok: [ app2 ] TASK [ Ensure apache starts ] **************************************************************************************************************************************************** changed: [ app1 ] changed: [ app2 ] PLAY RECAP ********************************************************************************************************************************************************************* app1 : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0 app2 : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0 lb : ok = 3 changed = 1 unreachable = 0 failed = 0 skipped = 0 rescued = 0 ignored = 0 To upload files to node servers: # setup-app.yml --- - hosts : webservers become : true tasks : - name : Upload index file copy : src : ../index.php dest : /var/www/html mode : 0755 - name : Configure php.ini file lineinfile : # Change a line in a config file matching the regex to the line value. path : /etc/php.ini regexp : ^short_open_tag line : 'short_open_tag=Off' notify : restart-apache # restart apache handlers : # Declaring restart apache task as a handler and adding notify in configure task. - name : restart-apache service : name : httpd state : restarted Load balance playbook using the template file: # setup-lb.yml --- - hosts : loadbalancers become : true tasks : - name : Creating template template : src : ../config/lb-config.j2 dest : /etc/httpd/conf.d/lb.conf owner : bin group : wheel mode : 064 notify : restart apache handlers : # Declaring restart apache task as a handler and adding notify in configure task. - name : restart apache service : name=httpd state=restarted Template file to be used by Ansible to dynamically configure the Loadbalancer: # Loadbalancer config that can be read by Ansible is written in Jinja2 ProxyRequests off <Proxy balancer://webcluster > {% for hosts in groups [ 'webservers' ] %} BalancerMember http:// {{ hostvars [ hosts ][ 'ansible_host' ] }} {% endfor %} ProxySet lbmethod=byrequests </Proxy> Running all playbooks in a single file from start to finish for a cleaner approach. # all-playbooks.yml --- - import_playbook : yum-update.yml - import_playbook : install-services.yml - import_playbook : setup-app.yml - import_playbook : setup-lb.yml To check status of httpd service: # check-status.yml --- - hosts : webservers:loadbalancers become : true tasks : - name : Check apache status command : service httpd status Ansible provides variables and metadata about the host that we interact with when running playbooks. - During the TASK[Gathering facts] step, variables get populated. - Gathers useful facts about the host and can be used in playbooks. - Use the status module to see all of the facts gathered during TASK[Gathering facts] step. - Uses jinja2 templating to evaluate these expressions. Example Adhoc command: $ ansible -m setup app1 # Gives all the variables associated with it and we can use them in playbooks $ ansible -m setup app1 app 1 | SUCCESS => { \"ansible_facts\" : { \"ansible_all_ipv4_addresses\" : [ \"172.31.94.190\" ], \"ansible_all_ipv6_addresses\" : [ \"fe80::10f3:50ff:fea3:9151\" ], \"ansible_apparmor\" : { \"status\" : \"disabled\" }, \"ansible_architecture\" : \"x86_64\" , \"ansible_bios_date\" : \"08/24/2006\" , \"ansible_bios_version\" : \"4.2.amazon\" , \"ansible_cmdline\" : { \"console\" : \"ttyS0\" , \"nvme_core.io_timeout\" : \"4294967295\" , \"root\" : \"LABEL=/\" , \"selinux\" : \"0\" }, \"ansible_date_time\" : { \"date\" : \"2019-12-17\" , \"day\" : \"17\" , \"epoch\" : \"1576552252\" , \"hour\" : \"03\" , \"iso8601\" : \"2019-12-17T03:10:52Z\" , \"iso8601_basic\" : \"20191217T031052467561\" , \"iso8601_basic_short\" : \"20191217T031052\" , \"iso8601_micro\" : \"2019-12-17T03:10:52.467633Z\" , \"minute\" : \"10\" , \"month\" : \"12\" , \"second\" : \"52\" , \"time\" : \"03:10:52\" , \"tz\" : \"UTC\" , \"tz_offset\" : \"+0000\" , \"weekday\" : \"Tuesday\" , \"weekday_number\" : \"2\" , \"weeknumber\" : \"50\" , \"year\" : \"2019\" }, \"ansible_default_ipv4\" : { \"address\" : \"172.31.94.190\" , \"alias\" : \"eth0\" , \"broadcast\" : \"172.31.95.255\" , \"gateway\" : \"172.31.80.1\" , \"interface\" : \"eth0\" , \"macaddress\" : \"12:f3:50:a3:91:51\" , \"mtu\" : 9001 , \"netmask\" : \"255.255.240.0\" , \"network\" : \"172.31.80.0\" , \"type\" : \"ether\" }, \"ansible_default_ipv6\" : {}, \"ansible_device_links\" : { \"ids\" : {}, \"labels\" : { \"xvda1\" : [ \"\\\\x2f\" ] }, \"masters\" : {}, \"uuids\" : { \"xvda1\" : [ \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" ] } }, \"ansible_devices\" : { \"xvda\" : { \"holders\" : [], \"host\" : \"\" , \"links\" : { \"ids\" : [], \"labels\" : [], \"masters\" : [], \"uuids\" : [] }, \"model\" : null , \"partitions\" : { \"xvda1\" : { \"holders\" : [], \"links\" : { \"ids\" : [], \"labels\" : [ \"\\\\x2f\" ], \"masters\" : [], \"uuids\" : [ \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" ] }, \"sectors\" : \"16773087\" , \"sectorsize\" : 512 , \"size\" : \"8.00 GB\" , \"start\" : \"4096\" , \"uuid\" : \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" } }, \"removable\" : \"0\" , \"rotational\" : \"0\" , \"sas_address\" : null , \"sas_device_handle\" : null , \"scheduler_mode\" : \"noop\" , \"sectors\" : \"16777216\" , \"sectorsize\" : \"512\" , \"size\" : \"8.00 GB\" , \"support_discard\" : \"0\" , \"vendor\" : null , \"virtual\" : 1 } }, \"ansible_distribution\" : \"Amazon\" , \"ansible_distribution_file_parsed\" : true , \"ansible_distribution_file_path\" : \"/etc/system-release\" , \"ansible_distribution_file_variety\" : \"Amazon\" , \"ansible_distribution_major_version\" : \"2018\" , \"ansible_distribution_release\" : \"NA\" , \"ansible_distribution_version\" : \"NA\" , \"ansible_dns\" : { \"nameservers\" : [ \"172.31.0.2\" ], \"options\" : { \"attempts\" : \"5\" , \"timeout\" : \"2\" }, \"search\" : [ \"ec2.internal\" ] }, \"ansible_domain\" : \"ec2.internal\" , \"ansible_effective_group_id\" : 500 , \"ansible_effective_user_id\" : 500 , \"ansible_env\" : { \"AWS_AUTO_SCALING_HOME\" : \"/opt/aws/apitools/as\" , \"AWS_CLOUDWATCH_HOME\" : \"/opt/aws/apitools/mon\" , \"AWS_ELB_HOME\" : \"/opt/aws/apitools/elb\" , \"AWS_PATH\" : \"/opt/aws\" , \"EC2_AMITOOL_HOME\" : \"/opt/aws/amitools/ec2\" , \"EC2_HOME\" : \"/opt/aws/apitools/ec2\" , \"HOME\" : \"/home/ec2-user\" , \"JAVA_HOME\" : \"/usr/lib/jvm/jre\" , \"LANG\" : \"en_US.UTF-8\" , \"LESSOPEN\" : \"||/usr/bin/lesspipe.sh %s\" , \"LESS_TERMCAP_mb\" : \"\\u001b[01;31m\" , \"LESS_TERMCAP_md\" : \"\\u001b[01;38;5;208m\" , \"LESS_TERMCAP_me\" : \"\\u001b[0m\" , \"LESS_TERMCAP_se\" : \"\\u001b[0m\" , \"LESS_TERMCAP_ue\" : \"\\u001b[0m\" , \"LESS_TERMCAP_us\" : \"\\u001b[04;38;5;111m\" , \"LOGNAME\" : \"ec2-user\" , \"MAIL\" : \"/var/mail/ec2-user\" , \"PATH\" : \"/usr/local/bin:/bin:/usr/bin:/opt/aws/bin\" , \"PWD\" : \"/home/ec2-user\" , \"SHELL\" : \"/bin/bash\" , \"SHLVL\" : \"2\" , \"SSH_CLIENT\" : \"24.187.17.118 60904 22\" , \"SSH_CONNECTION\" : \"24.187.17.118 60904 172.31.94.190 22\" , \"SSH_TTY\" : \"/dev/pts/0\" , \"TERM\" : \"xterm-256color\" , \"USER\" : \"ec2-user\" , \"_\" : \"/usr/bin/python\" }, \"ansible_eth0\" : { \"active\" : true , \"device\" : \"eth0\" , \"features\" : { \"esp_hw_offload\" : \"off [fixed]\" , \"esp_tx_csum_hw_offload\" : \"off [fixed]\" , \"fcoe_mtu\" : \"off [fixed]\" , \"generic_receive_offload\" : \"on\" , \"generic_segmentation_offload\" : \"on\" , \"highdma\" : \"off [fixed]\" , \"hw_tc_offload\" : \"off [fixed]\" , \"l2_fwd_offload\" : \"off [fixed]\" , \"large_receive_offload\" : \"off [fixed]\" , \"loopback\" : \"off [fixed]\" , \"netns_local\" : \"off [fixed]\" , \"ntuple_filters\" : \"off [fixed]\" , \"receive_hashing\" : \"off [fixed]\" , \"rx_all\" : \"off [fixed]\" , \"rx_checksumming\" : \"on [fixed]\" , \"rx_fcs\" : \"off [fixed]\" , \"rx_udp_tunnel_port_offload\" : \"off [fixed]\" , \"rx_vlan_filter\" : \"off [fixed]\" , \"rx_vlan_offload\" : \"off [fixed]\" , \"rx_vlan_stag_filter\" : \"off [fixed]\" , \"rx_vlan_stag_hw_parse\" : \"off [fixed]\" , \"scatter_gather\" : \"on\" , \"tcp_segmentation_offload\" : \"on\" , \"tx_checksum_fcoe_crc\" : \"off [fixed]\" , \"tx_checksum_ip_generic\" : \"off [fixed]\" , \"tx_checksum_ipv4\" : \"on [fixed]\" , \"tx_checksum_ipv6\" : \"off [requested on]\" , \"tx_checksum_sctp\" : \"off [fixed]\" , \"tx_checksumming\" : \"on\" , \"tx_esp_segmentation\" : \"off [fixed]\" , \"tx_fcoe_segmentation\" : \"off [fixed]\" , \"tx_gre_csum_segmentation\" : \"off [fixed]\" , \"tx_gre_segmentation\" : \"off [fixed]\" , \"tx_gso_partial\" : \"off [fixed]\" , \"tx_gso_robust\" : \"on [fixed]\" , \"tx_ipxip4_segmentation\" : \"off [fixed]\" , \"tx_ipxip6_segmentation\" : \"off [fixed]\" , \"tx_lockless\" : \"off [fixed]\" , \"tx_nocache_copy\" : \"off\" , \"tx_scatter_gather\" : \"on\" , \"tx_scatter_gather_fraglist\" : \"off [fixed]\" , \"tx_sctp_segmentation\" : \"off [fixed]\" , \"tx_tcp6_segmentation\" : \"off [requested on]\" , \"tx_tcp_ecn_segmentation\" : \"off [fixed]\" , \"tx_tcp_mangleid_segmentation\" : \"off\" , \"tx_tcp_segmentation\" : \"on\" , \"tx_udp_tnl_csum_segmentation\" : \"off [fixed]\" , \"tx_udp_tnl_segmentation\" : \"off [fixed]\" , \"tx_vlan_offload\" : \"off [fixed]\" , \"tx_vlan_stag_hw_insert\" : \"off [fixed]\" , \"udp_fragmentation_offload\" : \"off\" , \"vlan_challenged\" : \"off [fixed]\" }, \"hw_timestamp_filters\" : [], \"ipv4\" : { \"address\" : \"172.31.94.190\" , \"broadcast\" : \"172.31.95.255\" , \"netmask\" : \"255.255.240.0\" , \"network\" : \"172.31.80.0\" }, \"ipv6\" : [ { \"address\" : \"fe80::10f3:50ff:fea3:9151\" , \"prefix\" : \"64\" , \"scope\" : \"link\" } ], \"macaddress\" : \"12:f3:50:a3:91:51\" , \"module\" : \"xen_netfront\" , \"mtu\" : 9001 , \"pciid\" : \"vif-0\" , \"promisc\" : false , \"timestamping\" : [ \"rx_software\" , \"software\" ], \"type\" : \"ether\" }, \"ansible_fibre_channel_wwn\" : [], \"ansible_fips\" : false , \"ansible_form_factor\" : \"Other\" , \"ansible_fqdn\" : \"ip-172-31-94-190.ec2.internal\" , \"ansible_hostname\" : \"ip-172-31-94-190\" , \"ansible_hostnqn\" : \"\" , \"ansible_interfaces\" : [ \"lo\" , \"eth0\" ], \"ansible_is_chroot\" : false , \"ansible_iscsi_iqn\" : \"\" , \"ansible_kernel\" : \"4.14.62-65.117.amzn1.x86_64\" , \"ansible_kernel_version\" : \"#1 SMP Fri Aug 10 20:03:52 UTC 2018\" , \"ansible_lo\" : { \"active\" : true , \"device\" : \"lo\" , \"features\" : { \"esp_hw_offload\" : \"off [fixed]\" , \"esp_tx_csum_hw_offload\" : \"off [fixed]\" , \"fcoe_mtu\" : \"off [fixed]\" , \"generic_receive_offload\" : \"on\" , \"generic_segmentation_offload\" : \"on\" , \"highdma\" : \"on [fixed]\" , \"hw_tc_offload\" : \"off [fixed]\" , \"l2_fwd_offload\" : \"off [fixed]\" , \"large_receive_offload\" : \"off [fixed]\" , \"loopback\" : \"on [fixed]\" , \"netns_local\" : \"on [fixed]\" , \"ntuple_filters\" : \"off [fixed]\" , \"receive_hashing\" : \"off [fixed]\" , \"rx_all\" : \"off [fixed]\" , \"rx_checksumming\" : \"on [fixed]\" , \"rx_fcs\" : \"off [fixed]\" , \"rx_udp_tunnel_port_offload\" : \"off [fixed]\" , \"rx_vlan_filter\" : \"off [fixed]\" , \"rx_vlan_offload\" : \"off [fixed]\" , \"rx_vlan_stag_filter\" : \"off [fixed]\" , \"rx_vlan_stag_hw_parse\" : \"off [fixed]\" , \"scatter_gather\" : \"on\" , \"tcp_segmentation_offload\" : \"on\" , \"tx_checksum_fcoe_crc\" : \"off [fixed]\" , \"tx_checksum_ip_generic\" : \"on [fixed]\" , \"tx_checksum_ipv4\" : \"off [fixed]\" , \"tx_checksum_ipv6\" : \"off [fixed]\" , \"tx_checksum_sctp\" : \"on [fixed]\" , \"tx_checksumming\" : \"on\" , \"tx_esp_segmentation\" : \"off [fixed]\" , \"tx_fcoe_segmentation\" : \"off [fixed]\" , \"tx_gre_csum_segmentation\" : \"off [fixed]\" , \"tx_gre_segmentation\" : \"off [fixed]\" , \"tx_gso_partial\" : \"off [fixed]\" , \"tx_gso_robust\" : \"off [fixed]\" , \"tx_ipxip4_segmentation\" : \"off [fixed]\" , \"tx_ipxip6_segmentation\" : \"off [fixed]\" , \"tx_lockless\" : \"on [fixed]\" , \"tx_nocache_copy\" : \"off [fixed]\" , \"tx_scatter_gather\" : \"on [fixed]\" , \"tx_scatter_gather_fraglist\" : \"on [fixed]\" , \"tx_sctp_segmentation\" : \"on\" , \"tx_tcp6_segmentation\" : \"on\" , \"tx_tcp_ecn_segmentation\" : \"on\" , \"tx_tcp_mangleid_segmentation\" : \"on\" , \"tx_tcp_segmentation\" : \"on\" , \"tx_udp_tnl_csum_segmentation\" : \"off [fixed]\" , \"tx_udp_tnl_segmentation\" : \"off [fixed]\" , \"tx_vlan_offload\" : \"off [fixed]\" , \"tx_vlan_stag_hw_insert\" : \"off [fixed]\" , \"udp_fragmentation_offload\" : \"off\" , \"vlan_challenged\" : \"on [fixed]\" }, \"hw_timestamp_filters\" : [], \"ipv4\" : { \"address\" : \"127.0.0.1\" , \"broadcast\" : \"host\" , \"netmask\" : \"255.0.0.0\" , \"network\" : \"127.0.0.0\" }, \"ipv6\" : [ { \"address\" : \"::1\" , \"prefix\" : \"128\" , \"scope\" : \"host\" } ], \"mtu\" : 65536 , \"promisc\" : false , \"timestamping\" : [ \"tx_software\" , \"rx_software\" , \"software\" ], \"type\" : \"loopback\" }, \"ansible_local\" : {}, \"ansible_lsb\" : {}, \"ansible_machine\" : \"x86_64\" , \"ansible_machine_id\" : \"fef67c45adfb1d65225893255df2b04b\" , \"ansible_memfree_mb\" : 224 , \"ansible_memory_mb\" : { \"nocache\" : { \"free\" : 868 , \"used\" : 117 }, \"real\" : { \"free\" : 224 , \"total\" : 985 , \"used\" : 761 }, \"swap\" : { \"cached\" : 0 , \"free\" : 0 , \"total\" : 0 , \"used\" : 0 } }, \"ansible_memtotal_mb\" : 985 , \"ansible_mounts\" : [ { \"block_available\" : 1632826 , \"block_size\" : 4096 , \"block_total\" : 2030953 , \"block_used\" : 398127 , \"device\" : \"/dev/xvda1\" , \"fstype\" : \"ext4\" , \"inode_available\" : 483722 , \"inode_total\" : 524288 , \"inode_used\" : 40566 , \"mount\" : \"/\" , \"options\" : \"rw,noatime,data=ordered\" , \"size_available\" : 6688055296 , \"size_total\" : 8318783488 , \"uuid\" : \"1ade993f-5854-45a8-ae15-3d49e5fcbe67\" } ], \"ansible_nodename\" : \"ip-172-31-94-190\" , \"ansible_os_family\" : \"RedHat\" , \"ansible_pkg_mgr\" : \"yum\" , \"ansible_proc_cmdline\" : { \"console\" : [ \"tty1\" , \"ttyS0\" ], \"nvme_core.io_timeout\" : \"4294967295\" , \"root\" : \"LABEL=/\" , \"selinux\" : \"0\" }, \"ansible_processor\" : [ \"0\" , \"GenuineIntel\" , \"Intel(R) Xeon(R) CPU E5-2676 v3 @ 2.40GHz\" ], \"ansible_processor_cores\" : 1 , \"ansible_processor_count\" : 1 , \"ansible_processor_threads_per_core\" : 1 , \"ansible_processor_vcpus\" : 1 , \"ansible_product_name\" : \"HVM domU\" , \"ansible_product_serial\" : \"NA\" , \"ansible_product_uuid\" : \"NA\" , \"ansible_product_version\" : \"4.2.amazon\" , \"ansible_python\" : { \"executable\" : \"/usr/bin/python\" , \"has_sslcontext\" : true , \"type\" : \"CPython\" , \"version\" : { \"major\" : 2 , \"micro\" : 16 , \"minor\" : 7 , \"releaselevel\" : \"final\" , \"serial\" : 0 }, \"version_info\" : [ 2 , 7 , 16 , \"final\" , 0 ] }, \"ansible_python_version\" : \"2.7.16\" , \"ansible_real_group_id\" : 500 , \"ansible_real_user_id\" : 500 , \"ansible_selinux\" : { \"status\" : \"Missing selinux Python library\" }, \"ansible_selinux_python_present\" : false , \"ansible_service_mgr\" : \"upstart\" , \"ansible_ssh_host_key_dsa_public\" : \"AAAAB3NzaC1kc3MAAACBAKmcc4NBxWTiOqX/jdg01Ywl1xYmHKFES13l3gPDstl1jOkFkCjJTZhY1hnCywOXC/yYWd1dlBAf3U1rnrGsEWG98/gGHamBsR6ykuPe2pxI9znh46V1xYpRpY1RfjZZ/8D7aEPHXIBDpCsUxHlF41gvX2EhVpIIioJeOwF5uz8zAAAAFQCRBwQz9E5URC/umtsp+si9VNsGjwAAAIBpxNzHbhGIab4+YpLc1Mt1EucWH202OcaYPHN8Rzyxx0BMp4Pqh0yvlAo8tfwOzDUxVYCPimSzXKRHwobeXy/hmv9DMekUT+DYFFSpDYTQn23rurihuHxTM3StGourHhq3iTk00B+X1qKF69V9O1HwPKR8VYbQnSNgnJ81NdJN6AAAAIAc+TnOyu3TXLHw0IXJEPmUqi2sOBgHNEWbtqEybhFIJKmXGrfYF6CPJLlwdtd0rIR3iPz0du1CKvBXAKY50+y29v8gDHTQdYbnDLeYxqpMf+xFEjfuSU3rmbRA/ealoG3P+dpR5DhhOuIN1beRMZFarArqtMBmTRiCqAioRIEz5A==\" , \"ansible_ssh_host_key_ecdsa_public\" : \"AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBI95vsHxkXdL4JjueQ/F8E7tfr5dUhH8kdvnnXiu0QvEKo8wJVV6cBGnBKuvckEesAoOc+55QWwn8RviHp/LfyU=\" , \"ansible_ssh_host_key_ed25519_public\" : \"AAAAC3NzaC1lZDI1NTE5AAAAIBCcSbXTmYudwPkmhmShysni/n0CRfdcPvY9l6NN095B\" , \"ansible_ssh_host_key_rsa_public\" : \"AAAAB3NzaC1yc2EAAAADAQABAAABAQDn03D+cJVjB7XOMu62K7xTTwoYNeMWa2lSmgoFJSlxvCYw9eCqlt1XPFAAoHmlYvBhPmCXATu2v6f1XWQRaD4VYR1uopDh6Ku9WhBOyYk5FGkJWgyM5Vm+TTrd6Btx5d/+w8tWUIT1BQ4V0pQYSeYRwZilfs+AA8mYRFrlf6+Bqvzn1hGGlRnC5PD4JTbjCT6ys22Il1RSIwWYHtx+zHfCJTrcP8qUcYB9mftJspWrwyOSw1zTCQBs8yQazlxLlzy3XsGK8zgJEtA5vQvosibPGYckfp/TM7SIYzY/pLz2IhQiaeHwdmInlvvSssUlJ6taTBiLga7LDQDmDH7G9sLn\" , \"ansible_swapfree_mb\" : 0 , \"ansible_swaptotal_mb\" : 0 , \"ansible_system\" : \"Linux\" , \"ansible_system_capabilities\" : [ \"\" ], \"ansible_system_capabilities_enforced\" : \"True\" , \"ansible_system_vendor\" : \"Xen\" , \"ansible_uptime_seconds\" : 11917 , \"ansible_user_dir\" : \"/home/ec2-user\" , \"ansible_user_gecos\" : \"EC2 Default User\" , \"ansible_user_gid\" : 500 , \"ansible_user_id\" : \"ec2-user\" , \"ansible_user_shell\" : \"/bin/bash\" , \"ansible_user_uid\" : 500 , \"ansible_userspace_architecture\" : \"x86_64\" , \"ansible_userspace_bits\" : \"64\" , \"ansible_virtualization_role\" : \"guest\" , \"ansible_virtualization_type\" : \"xen\" , \"discovered_interpreter_python\" : \"/usr/bin/python\" , \"gather_subset\" : [ \"all\" ], \"module_setup\" : true }, \"changed\" : false } Example playbook: - name : Add webserver info copy : dest : /var/www/html/info.php content : \"{{ ansible_hostname }}\" We can create local variables within the playbooks. Create playbook variables using vars to create key/value pairs and dictionary/map of variables. Can reference variables directly in playbooks. Can create variable files and import them into the playbooks. Example playbook: vars : html_path : \"/var/www/html\" new_var : \"repeated information\" tasks : - name : Add webserver information copy : dest : \"{{ html_path }}/info.php\" content : \"{{ new_var }}\" Ansible also gives us the ability to register variables from tasks that run to get information about its execution. Create variables from info returned from tasks ran using register . Call registered variables for later use. Use debug module anytime to see variables and debug our playbooks. Example playbook: vars : html_path : \"/var/www/html\" tasks : - name : See directory contents command : ls -la {{ html_path }} register : dir_contents # Output/information of execution of the above command is stored in this variable - name : Debug dir contents debug : msg : \"{{ dir_contents }}\" Variables can also be stored and used from command line, variables in external files or in inventory files.","title":"Playbooks"},{"location":"DevOps/ansible/#roles","text":"Ansible provides roles framework that makes each part of variables, tasks, templates and modules fully independent. Helps group tasks together in a way that is self containing. Clean and predefined directory structure. Break up the configurations into files. Re-use code by others who need similar configurations. Easy to modify and reduces syntax errors. Ansible helps in creating the directory structure using ansible-galaxy init <dir_path> ansible-galaxy init |dir_path| $ ansible-galaxy init roles/webservers roles/ \u2514\u2500\u2500 webservers \u251c\u2500\u2500 README.md \u251c\u2500\u2500 defaults \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 files \u251c\u2500\u2500 handlers \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 meta \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 tasks \u2502 \u2514\u2500\u2500 main.yml \u251c\u2500\u2500 templates \u251c\u2500\u2500 tests \u2502 \u251c\u2500\u2500 inventory \u2502 \u2514\u2500\u2500 test.yml \u2514\u2500\u2500 vars \u2514\u2500\u2500 main.yml . \u251c\u2500\u2500 ansible.cfg \u251c\u2500\u2500 hosts-dev \u251c\u2500\u2500 index.php \u251c\u2500\u2500 roles \u2502 \u2514\u2500\u2500 webservers \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 defaults \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 files \u2502 \u2502 \u2514\u2500\u2500 index.php \u2502 \u251c\u2500\u2500 handlers \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 meta \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 tasks \u2502 \u2502 \u2514\u2500\u2500 main.yml \u2502 \u251c\u2500\u2500 templates \u2502 \u251c\u2500\u2500 tests \u2502 \u2502 \u251c\u2500\u2500 inventory \u2502 \u2502 \u2514\u2500\u2500 test.yml \u2502 \u2514\u2500\u2500 vars \u2502 \u2514\u2500\u2500 main.yml \u2514\u2500\u2500 setup-app.yml # ansible.cfg [defaults] inventory = ./hosts-dev remote_user = ec2-user private_key_file = ~/.ssh/ansible-2020.pem host_key_checking = False # hosts-dev [webservers] app1 ansible_host=54.152.194.112 app2 ansible_host=3.211.181.182 [loadbalancers] lb ansible_host=54.89.101.67 [local] control ansible_connection=local # above 2 lines code is the way Ansible communicates back to the control host and so this is important # index.php <?php echo \"<h1>Hello, World! This is my Ansible page.</h1>\"; ?> # setup-app.yml --- - hosts: webservers become: true roles: - webservers","title":"Roles"},{"location":"DevOps/ansible/#check-mode-dry-run","text":"Reports changes that Ansible would have to make on the end hosts rather than applying the changes. Commands are run without affecting the remote system and reports changes. Great for one node at a time and used for basic configuration management use cases. $ ansible-playbook setup-app.yml --check","title":"Check Mode (Dry run)"},{"location":"DevOps/ansible/#error-handling-in-playbooks","text":"Change the default behavior of Ansible when certain events happen that may or may not need to report as a failure or changed status. Sometimes non-zero exit code is fine. Sometimes commands might not need to report a changed status. Explicitly force Ansible to ignore errors or changes that occur. Shell Module and Command Module always return a changed status when run and even when no changes are made to the node server # check-status.yml --- - hosts : webservers:loadbalancers become : true tasks : - name : Check apache status command : service httpd status changed_when : false # Ignores if changes occur - name : this will not fail command : /bin/false # always returns non-zero exit code (fail status) ignore_errors : yes # Ignore errors","title":"Error Handling in playbooks"},{"location":"DevOps/ansible/#tags","text":"Assigning tags to specific tasks in playbooks allows you to only call certain tasks in a very long playbook. It only runs specific parts of a playbook rather than all of the plays. Add tags to any tasks and re-use if needed. Specify the tags you want to run (or not to) on the command line. After adding tags: <tag_name> to any of the tasks, it can be called (or not) using: $ ansible-playbook setup-app.yml --tags <tag_name> $ ansible-playbook setup-app.yml --skip-tags <tag_name>","title":"Tags"},{"location":"DevOps/ansible/#ansible-vault","text":"Ansible vault is a way to keep sensitive information in encrypted files and not plain text in playbooks. Keeps passwords, keys and other sensitive variables in encrypted vault files. Vault files can be shared through source control. Password protected and default cipher is AES. Encrypted data file can be created using: $ ansible-vault create secret-variables.yml # create encrypted file $ ansible-vault edit secret-variables.yml # edit encrypted file $ ansible-playbook setup-app.yml --ask-vault-pass # prompt for password to use the encrypted secret variables file","title":"Ansible Vault"},{"location":"DevOps/deployment-processes/","text":"Deployment Processes \u00b6 Informative link Deploy vs Release \u00b6 Four phases of code deployment, Build -> Test -> Deploy -> Release Deploy might not be the same as release. There are different types of deployment processes, Canary and Blue/Green. Canary Deployment (Deploy == Release) \u00b6 In a clustered environment, you might first release-in-place to just one of your instances. This practice, most commonly referred to as canary , can mitigate some risk, the percentage of your traffic exposed to deploy and release risk is equal to the number of instances with the new version of your service divided by the total number of instances in your service\u2019s cluster. Blue/Green (Deploy != Release) \u00b6 A blue/green deploy involves deploying your service\u2019s new version alongside the released version in production. You may use dedicated hardware or VMs for each color and alternate subsequent deploys between them, or you may use containers and an orchestration framework like Kubernetes to manage ephemeral processes. Regardless, the key here is that once your new (green) version is deployed, it is not released \u2014 it does not start responding to production requests. Those are still being handled by the existing, known-good (blue) version of the service. Release in a blue/green setup usually involves making changes at the load balancer that add hosts running the new version and remove hosts running the known-good version. While this approach is much better than release-in-place, it has some limitations, particularly as it relates to release risk. If your deployment is hung up in a crash loop backoff or if the database secret is wrong and the newly-deployed service can\u2019t connect, you\u2019re not under any pressure to do anything. Your team can diagnose the problem. When deployment is separate from release, you can run automated health checks and integration tests against the newly-deployed version before exposing any production traffic to it. If we agree that every release is a test in production (and whether we agree or not, they are), then what we really want is to segment our production requests using pattern-matching rules and dynamically route an arbitrary percentage of that traffic to any version of our service. This is a powerful concept that forms the foundation of sophisticated release workflows like dogfooding , incremental release , rollbacks , and dark traffic . Dogfooding is a popular technique of releasing a new version of a service to employees only. With a powerful release service in place, you can write rules like \u201csend 50% of internal employee traffic to instances where version=x.x\u201d In my career, dogfooding in production has caught more embarrassing bugs than I care to admit. Incremental Release is the process of starting with some small percentage of production requests routed to a new version of a service while monitoring the performance of those requests \u2014 errors, latency, success rate, and so on, against the previous production release. When you\u2019re confident the new version doesn\u2019t exhibit any unexpected behavior relative to the known-good version, you can increase the percentage and repeat the process until you\u2019ve reached 100%. Rollback with a release-as-a-continuum system is simply a function of routing production requests back to instances that are still running the last known-good version. It\u2019s fast, low-risk, and like release itself, can be done in a fine-grained, targeted fashion. Dark Traffic is a powerful technique where your release system duplicates production requests and sends one copy to the known-good, \u201clight\u201d version of your service and another to a new, \u201cdark\u201d version. The \u201clight\u201d version is responsible for actually responding to the user\u2019s request. The \u201cdark\u201d version handles the request, but its response is ignored. This is particularly effective when you need to test new software under production load. A sophisticated release system does more than mitigate deploy risk\u2014it directly improves your product velocity and user experience.","title":"Deployment Processes"},{"location":"DevOps/deployment-processes/#deployment-processes","text":"Informative link","title":"Deployment Processes"},{"location":"DevOps/deployment-processes/#deploy-vs-release","text":"Four phases of code deployment, Build -> Test -> Deploy -> Release Deploy might not be the same as release. There are different types of deployment processes, Canary and Blue/Green.","title":"Deploy vs Release"},{"location":"DevOps/deployment-processes/#canary-deployment-deploy-release","text":"In a clustered environment, you might first release-in-place to just one of your instances. This practice, most commonly referred to as canary , can mitigate some risk, the percentage of your traffic exposed to deploy and release risk is equal to the number of instances with the new version of your service divided by the total number of instances in your service\u2019s cluster.","title":"Canary Deployment (Deploy == Release)"},{"location":"DevOps/deployment-processes/#bluegreen-deploy-release","text":"A blue/green deploy involves deploying your service\u2019s new version alongside the released version in production. You may use dedicated hardware or VMs for each color and alternate subsequent deploys between them, or you may use containers and an orchestration framework like Kubernetes to manage ephemeral processes. Regardless, the key here is that once your new (green) version is deployed, it is not released \u2014 it does not start responding to production requests. Those are still being handled by the existing, known-good (blue) version of the service. Release in a blue/green setup usually involves making changes at the load balancer that add hosts running the new version and remove hosts running the known-good version. While this approach is much better than release-in-place, it has some limitations, particularly as it relates to release risk. If your deployment is hung up in a crash loop backoff or if the database secret is wrong and the newly-deployed service can\u2019t connect, you\u2019re not under any pressure to do anything. Your team can diagnose the problem. When deployment is separate from release, you can run automated health checks and integration tests against the newly-deployed version before exposing any production traffic to it. If we agree that every release is a test in production (and whether we agree or not, they are), then what we really want is to segment our production requests using pattern-matching rules and dynamically route an arbitrary percentage of that traffic to any version of our service. This is a powerful concept that forms the foundation of sophisticated release workflows like dogfooding , incremental release , rollbacks , and dark traffic . Dogfooding is a popular technique of releasing a new version of a service to employees only. With a powerful release service in place, you can write rules like \u201csend 50% of internal employee traffic to instances where version=x.x\u201d In my career, dogfooding in production has caught more embarrassing bugs than I care to admit. Incremental Release is the process of starting with some small percentage of production requests routed to a new version of a service while monitoring the performance of those requests \u2014 errors, latency, success rate, and so on, against the previous production release. When you\u2019re confident the new version doesn\u2019t exhibit any unexpected behavior relative to the known-good version, you can increase the percentage and repeat the process until you\u2019ve reached 100%. Rollback with a release-as-a-continuum system is simply a function of routing production requests back to instances that are still running the last known-good version. It\u2019s fast, low-risk, and like release itself, can be done in a fine-grained, targeted fashion. Dark Traffic is a powerful technique where your release system duplicates production requests and sends one copy to the known-good, \u201clight\u201d version of your service and another to a new, \u201cdark\u201d version. The \u201clight\u201d version is responsible for actually responding to the user\u2019s request. The \u201cdark\u201d version handles the request, but its response is ignored. This is particularly effective when you need to test new software under production load. A sophisticated release system does more than mitigate deploy risk\u2014it directly improves your product velocity and user experience.","title":"Blue/Green (Deploy != Release)"},{"location":"DevOps/devops-handbook-summary/","text":"DevOps Handbook Summary \u00b6 DevOps Handbook Summary","title":"DevOps Handbook Summary"},{"location":"DevOps/devops-handbook-summary/#devops-handbook-summary","text":"DevOps Handbook Summary","title":"DevOps Handbook Summary"},{"location":"DevOps/docker-basics/","text":"Docker Basics \u00b6 Install with script \u00b6 Link to script # Install steps can be skipped by using this script $ wget https://get.docker.com | sh Install \u00b6 # Install Dependencies $ sudo yum install -y yum-utils device-mapper-persistent-data lvm2 # Add docker repo $ sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # Install Docker community edition $ sudo yum install docker-ce # Enable Docker service and start Docker $ sudo systemctl enable docker $ sudo systemctl start docker $ sudo docker run hello-world # Run docker container 'hello-world' $ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:4fe721ccc2e8dc7362278a29dc660d833570ec2682f4e4194f4ee23e415e1064 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ # Giving cloud_user permissions to run docker by adding it to docker group $ sudo usermod -a -G docker cloud_user $ exit $ docker container run hello-world DockerHub and Docker repository \u00b6 DockerHub is analogous to GitHub where our code/docker-image is stored either privately or publicly. Docker repo is analogous to RedHat repo where it can either be created privately in an organization or use a public repo when needed. Show docker images can be done either by images or image ls docker commands: # Deprecated $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB #New command $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB Pull docker image: $ docker pull ubuntu:16.04 16 .04: Pulling from library/ubuntu 976a760c94fc: Extracting [=============================================== > ] 41 .75MB/44.15MB c58992f3c37b: Download complete 0ca0e5e7f12e: Download complete f2a274cc00ca: Download complete Ubuntu 16.04 dockerfile FROM scratch ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz / # delete all the apt list files since they're big and get stale quickly RUN rm -rf /var/lib/apt/lists/* # this forces \"apt-get update\" in dependent images, which is also good # (see also https://bugs.launchpad.net/cloud-images/+bug/1699913) # a few minor docker-specific tweaks # see https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap RUN set -xe \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L40-L48 && echo '#!/bin/sh' > /usr/sbin/policy-rc.d \\ && echo 'exit 101' >> /usr/sbin/policy-rc.d \\ && chmod +x /usr/sbin/policy-rc.d \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L54-L56 && dpkg-divert --local --rename --add /sbin/initctl \\ && cp -a /usr/sbin/policy-rc.d /sbin/initctl \\ && sed -i 's/^exit.*/exit 0/' /sbin/initctl \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L71-L78 && echo 'force-unsafe-io' > /etc/dpkg/dpkg.cfg.d/docker-apt-speedup \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L85-L105 && echo 'DPkg::Post-Invoke { \"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\"; };' > /etc/apt/apt.conf.d/docker-clean \\ && echo 'APT::Update::Post-Invoke { \"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\"; };' >> /etc/apt/apt.conf.d/docker-clean \\ && echo 'Dir::Cache::pkgcache \"\"; Dir::Cache::srcpkgcache \"\";' >> /etc/apt/apt.conf.d/docker-clean \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L109-L115 && echo 'Acquire::Languages \"none\";' > /etc/apt/apt.conf.d/docker-no-languages \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L118-L130 && echo 'Acquire::GzipIndexes \"true\"; Acquire::CompressionTypes::Order:: \"gz\";' > /etc/apt/apt.conf.d/docker-gzip-indexes \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L134-L151 && echo 'Apt::AutoRemove::SuggestsImportant \"false\";' > /etc/apt/apt.conf.d/docker-autoremove-suggests # make systemd-detect-virt return \"docker\" # See: https://github.com/systemd/systemd/blob/aa0c34279ee40bce2f9681b496922dedbadfca19/src/basic/virt.c#L434 RUN mkdir -p /run/systemd && echo 'docker' > /run/systemd/container CMD [ \"/bin/bash\" ] To run the docker image, # deprecated $ docker run ubuntu:16.04 $ docker run <ImageID> # New command $ docker container run ubuntu:16.04 Dockerfile \u00b6 As an example, building a docker image with Ubuntu 16.04 as the base OS image, update the image to latest patches and install python3 on top of it. Sample Dockerfile FROM ubuntu:16.04 LABEL maintainer = \"deepgorthi@gmail.com\" RUN apt-get update RUN apt-get install -y python3 Building Docker image \u00b6 To build the docker image, we need to run # '.' specifies to look for the dockerfile in the current directory $ docker image build . If the name of the dockerfile is different, we can build the image by doing $ docker image build <docker file name> Build output $ docker image build . Sending build context to Docker daemon 2 .048kB Step 1 /4 : FROM ubuntu:16.04 ---> 56bab49eef2e Step 2 /4 : LABEL maintainer = \"deepgorthi@gmail.com\" ---> Running in d76d48fb3f92 Removing intermediate container d76d48fb3f92 ---> 03b3755c4237 Step 3 /4 : RUN apt-get update ---> Running in 6b1995d3f8d0 Get:1 http://archive.ubuntu.com/ubuntu xenial InRelease [ 247 kB ] Get:2 http://security.ubuntu.com/ubuntu xenial-security InRelease [ 109 kB ] Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [ 109 kB ] Get:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [ 1019 kB ] Get:5 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [ 107 kB ] Get:6 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [ 1558 kB ] Get:7 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [ 12 .7 kB ] Get:8 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [ 593 kB ] Get:9 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [ 6280 B ] Get:10 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [ 14 .1 kB ] Get:11 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [ 9827 kB ] Get:12 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [ 176 kB ] Get:13 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [ 1396 kB ] Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [ 13 .1 kB ] Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [ 996 kB ] Get:16 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [ 19 .3 kB ] Get:17 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [ 7942 B ] Get:18 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [ 8807 B ] Fetched 16 .2 MB in 3s ( 4134 kB/s ) Reading package lists... Removing intermediate container 6b1995d3f8d0 ---> 33e4e45672d8 Step 4 /4 : RUN apt-get install -y python3 ---> Running in ac8388b43a00 Reading package lists... Building dependency tree... Reading state information... The following additional packages will be installed: dh-python file libexpat1 libmagic1 libmpdec2 libpython3-stdlib libpython3.5-minimal libpython3.5-stdlib libsqlite3-0 libssl1.0.0 mime-support python3-minimal python3.5 python3.5-minimal Suggested packages: libdpkg-perl python3-doc python3-tk python3-venv python3.5-venv python3.5-doc binutils binfmt-support The following NEW packages will be installed: dh-python file libexpat1 libmagic1 libmpdec2 libpython3-stdlib libpython3.5-minimal libpython3.5-stdlib libsqlite3-0 libssl1.0.0 mime-support python3 python3-minimal python3.5 python3.5-minimal 0 upgraded, 15 newly installed, 0 to remove and 4 not upgraded. Need to get 6436 kB of archives. After this operation, 33 .2 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libssl1.0.0 amd64 1 .0.2g-1ubuntu4.15 [ 1084 kB ] Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython3.5-minimal amd64 3 .5.2-2ubuntu0~16.04.9 [ 524 kB ] Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libexpat1 amd64 2 .1.0-7ubuntu0.16.04.5 [ 71 .5 kB ] Get:4 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python3.5-minimal amd64 3 .5.2-2ubuntu0~16.04.9 [ 1593 kB ] Get:5 http://archive.ubuntu.com/ubuntu xenial/main amd64 python3-minimal amd64 3 .5.1-3 [ 23 .3 kB ] Get:6 http://archive.ubuntu.com/ubuntu xenial/main amd64 mime-support all 3 .59ubuntu1 [ 31 .0 kB ] Get:7 http://archive.ubuntu.com/ubuntu xenial/main amd64 libmpdec2 amd64 2 .4.2-1 [ 82 .6 kB ] Get:8 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libsqlite3-0 amd64 3 .11.0-1ubuntu1.3 [ 397 kB ] Get:9 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython3.5-stdlib amd64 3 .5.2-2ubuntu0~16.04.9 [ 2137 kB ] Get:10 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python3.5 amd64 3 .5.2-2ubuntu0~16.04.9 [ 165 kB ] Get:11 http://archive.ubuntu.com/ubuntu xenial/main amd64 libpython3-stdlib amd64 3 .5.1-3 [ 6818 B ] Get:12 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 dh-python all 2 .20151103ubuntu1.2 [ 73 .9 kB ] Get:13 http://archive.ubuntu.com/ubuntu xenial/main amd64 python3 amd64 3 .5.1-3 [ 8710 B ] Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libmagic1 amd64 1 :5.25-2ubuntu1.3 [ 216 kB ] Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 file amd64 1 :5.25-2ubuntu1.3 [ 21 .3 kB ] debconf: delaying package configuration, since apt-utils is not installed Fetched 6436 kB in 2s ( 3013 kB/s ) Selecting previously unselected package libssl1.0.0:amd64. ( Reading database ... 4781 files and directories currently installed. ) Preparing to unpack .../libssl1.0.0_1.0.2g-1ubuntu4.15_amd64.deb ... Unpacking libssl1.0.0:amd64 ( 1 .0.2g-1ubuntu4.15 ) ... Selecting previously unselected package libpython3.5-minimal:amd64. Preparing to unpack .../libpython3.5-minimal_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking libpython3.5-minimal:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package libexpat1:amd64. Preparing to unpack .../libexpat1_2.1.0-7ubuntu0.16.04.5_amd64.deb ... Unpacking libexpat1:amd64 ( 2 .1.0-7ubuntu0.16.04.5 ) ... Selecting previously unselected package python3.5-minimal. Preparing to unpack .../python3.5-minimal_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking python3.5-minimal ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package python3-minimal. Preparing to unpack .../python3-minimal_3.5.1-3_amd64.deb ... Unpacking python3-minimal ( 3 .5.1-3 ) ... Selecting previously unselected package mime-support. Preparing to unpack .../mime-support_3.59ubuntu1_all.deb ... Unpacking mime-support ( 3 .59ubuntu1 ) ... Selecting previously unselected package libmpdec2:amd64. Preparing to unpack .../libmpdec2_2.4.2-1_amd64.deb ... Unpacking libmpdec2:amd64 ( 2 .4.2-1 ) ... Selecting previously unselected package libsqlite3-0:amd64. Preparing to unpack .../libsqlite3-0_3.11.0-1ubuntu1.3_amd64.deb ... Unpacking libsqlite3-0:amd64 ( 3 .11.0-1ubuntu1.3 ) ... Selecting previously unselected package libpython3.5-stdlib:amd64. Preparing to unpack .../libpython3.5-stdlib_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking libpython3.5-stdlib:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package python3.5. Preparing to unpack .../python3.5_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking python3.5 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package libpython3-stdlib:amd64. Preparing to unpack .../libpython3-stdlib_3.5.1-3_amd64.deb ... Unpacking libpython3-stdlib:amd64 ( 3 .5.1-3 ) ... Selecting previously unselected package dh-python. Preparing to unpack .../dh-python_2.20151103ubuntu1.2_all.deb ... Unpacking dh-python ( 2 .20151103ubuntu1.2 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Setting up libssl1.0.0:amd64 ( 1 .0.2g-1ubuntu4.15 ) ... debconf: unable to initialize frontend: Dialog debconf: ( TERM is not set, so the dialog frontend is not usable. ) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: ( Can ' t locate Term/ReadLine.pm in @INC ( you may need to install the Term::ReadLine module ) ( @INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.22.1 /usr/local/share/perl/5.22.1 /usr/lib/x86_64-linux-gnu/perl5/5.22 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl/5.22 /usr/share/perl/5.22 /usr/local/lib/site_perl /usr/lib/x86_64-linux-gnu/perl-base . ) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7 . ) debconf: falling back to frontend: Teletype Setting up libpython3.5-minimal:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up libexpat1:amd64 ( 2 .1.0-7ubuntu0.16.04.5 ) ... Setting up python3.5-minimal ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up python3-minimal ( 3 .5.1-3 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Selecting previously unselected package python3. ( Reading database ... 5757 files and directories currently installed. ) Preparing to unpack .../python3_3.5.1-3_amd64.deb ... Unpacking python3 ( 3 .5.1-3 ) ... Selecting previously unselected package libmagic1:amd64. Preparing to unpack .../libmagic1_1%3a5.25-2ubuntu1.3_amd64.deb ... Unpacking libmagic1:amd64 ( 1 :5.25-2ubuntu1.3 ) ... Selecting previously unselected package file. Preparing to unpack .../file_1%3a5.25-2ubuntu1.3_amd64.deb ... Unpacking file ( 1 :5.25-2ubuntu1.3 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Setting up mime-support ( 3 .59ubuntu1 ) ... Setting up libmpdec2:amd64 ( 2 .4.2-1 ) ... Setting up libsqlite3-0:amd64 ( 3 .11.0-1ubuntu1.3 ) ... Setting up libpython3.5-stdlib:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up python3.5 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up libpython3-stdlib:amd64 ( 3 .5.1-3 ) ... Setting up libmagic1:amd64 ( 1 :5.25-2ubuntu1.3 ) ... Setting up file ( 1 :5.25-2ubuntu1.3 ) ... Setting up python3 ( 3 .5.1-3 ) ... running python rtupdate hooks for python3.5... running python post-rtupdate hooks for python3.5... Setting up dh-python ( 2 .20151103ubuntu1.2 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Removing intermediate container ac8388b43a00 ---> a41c41ef3ce9 Successfully built a41c41ef3ce9 Running the Container \u00b6 To run the image in a docker container, $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> a41c41ef3ce9 11 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB $ docker container run -i -t --name python-container a41 # just enough chars are enough to distinguish between docker images # -i is short for --interactive. Keep STDIN open even if unattached. # -t is short for--tty. Allocates a pseudo terminal that connects your terminal with the container\u2019s STDIN and STDOUT. # You need to specify both -i and -t to then interact with the container through your terminal shell. A container stops after exiting from the command line of that container. List of running containers: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e365eb0e4568 a41 \"/bin/bash\" 42 seconds ago Up 40 seconds python-container_v2 List of all stopped and running containers: $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e365eb0e4568 a41 \"/bin/bash\" About a minute ago Up About a minute python-container_v2 11087b710633 a41 \"/bin/bash\" 11 minutes ago Exited ( 0 ) 4 minutes ago python-container db3e7658a34b ubuntu:16.04 \"/bin/bash\" 35 minutes ago Exited ( 0 ) 34 minutes ago cool_beaver b6fd3fb58e5a hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago hardcore_pascal 167b6f3834e0 hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago boring_hellman Starting and stopping a container without running the shell or logging in: $ docker container start python-container python-container $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 11087b710633 a41 \"/bin/bash\" 14 minutes ago Up 24 seconds python-container $ docker container stop python-container python-container $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES After starting up a container using start , to attach to that container, we can run: $ docker container attach <container ID> root@11087b710633:/# Removing Containers (rm) \u00b6 $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e365eb0e4568 a41 \"/bin/bash\" 10 minutes ago Exited ( 0 ) 6 minutes ago python-container_v2 11087b710633 a41 \"/bin/bash\" 20 minutes ago Exited ( 0 ) 7 seconds ago python-container db3e7658a34b ubuntu:16.04 \"/bin/bash\" 43 minutes ago Exited ( 0 ) 43 minutes ago cool_beaver b6fd3fb58e5a hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago hardcore_pascal 167b6f3834e0 hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago boring_hellman $ docker container rm 167b6f3834e0 167b6f3834e0 $ docker container rm hardcore_pascal hardcore_pascal $ docker container rm db3e7658a34b python-container python-container_v2 db3e7658a34b python-container python-container_v2 $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Removing Images \u00b6 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> a41c41ef3ce9 34 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB $ docker image rm fce289e99eb9 Untagged: hello-world:latest Untagged: hello-world@sha256:4fe721ccc2e8dc7362278a29dc660d833570ec2682f4e4194f4ee23e415e1064 Deleted: sha256:fce289e99eb9bca977dae136fbe2a82b6b7d4c372474c9235adc1741675f587e Deleted: sha256:af0b15c8625bb1938f1d7b17081031f649fd14e6b233688eea3c5483994a66a3 Pushing image to Docker repo \u00b6 To login to docker: $ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don ' t have a Docker ID, head over to https://hub.docker.com to create one. Username: deepgorthi Password: WARNING! Your password will be stored unencrypted in /home/cloud_user/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Tag the image that will be pushed to docker repo and push it: $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> a41c41ef3ce9 37 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB $ docker image tag a41c41ef3ce9 deepgorthi/ubu-python3 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE deepgorthi/ubu-python3 latest a41c41ef3ce9 38 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB $ docker image push deepgorthi/ubu-python3 The push refers to repository [ docker.io/deepgorthi/ubu-python3 ] fc529f7b8eec: Pushed c992de3f99a0: Pushed aa7f8c8d5f39: Mounted from library/ubuntu 48817fbd6c92: Mounted from library/ubuntu 1b039d138968: Mounted from library/ubuntu 7082d7d696f8: Mounted from library/ubuntu latest: digest: sha256:f348584be25aa65a0d266d77eadfeaad955385736462cea5efd68d1a4473760d size: 1574 Links List of Docker commands VMs, Containers and Docker Learn enough useful Docker series Part 1 Part 2 Part 3 Part 4 Part 5 Part 6","title":"Docker Basics"},{"location":"DevOps/docker-basics/#docker-basics","text":"","title":"Docker Basics"},{"location":"DevOps/docker-basics/#install-with-script","text":"Link to script # Install steps can be skipped by using this script $ wget https://get.docker.com | sh","title":"Install with script"},{"location":"DevOps/docker-basics/#install","text":"# Install Dependencies $ sudo yum install -y yum-utils device-mapper-persistent-data lvm2 # Add docker repo $ sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo # Install Docker community edition $ sudo yum install docker-ce # Enable Docker service and start Docker $ sudo systemctl enable docker $ sudo systemctl start docker $ sudo docker run hello-world # Run docker container 'hello-world' $ sudo docker run hello-world Unable to find image 'hello-world:latest' locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:4fe721ccc2e8dc7362278a29dc660d833570ec2682f4e4194f4ee23e415e1064 Status: Downloaded newer image for hello-world:latest Hello from Docker! This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1 . The Docker client contacted the Docker daemon. 2 . The Docker daemon pulled the \"hello-world\" image from the Docker Hub. ( amd64 ) 3 . The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4 . The Docker daemon streamed that output to the Docker client, which sent it to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bash Share images, automate workflows, and more with a free Docker ID: https://hub.docker.com/ For more examples and ideas, visit: https://docs.docker.com/get-started/ # Giving cloud_user permissions to run docker by adding it to docker group $ sudo usermod -a -G docker cloud_user $ exit $ docker container run hello-world","title":"Install"},{"location":"DevOps/docker-basics/#dockerhub-and-docker-repository","text":"DockerHub is analogous to GitHub where our code/docker-image is stored either privately or publicly. Docker repo is analogous to RedHat repo where it can either be created privately in an organization or use a public repo when needed. Show docker images can be done either by images or image ls docker commands: # Deprecated $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB #New command $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB Pull docker image: $ docker pull ubuntu:16.04 16 .04: Pulling from library/ubuntu 976a760c94fc: Extracting [=============================================== > ] 41 .75MB/44.15MB c58992f3c37b: Download complete 0ca0e5e7f12e: Download complete f2a274cc00ca: Download complete Ubuntu 16.04 dockerfile FROM scratch ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz / # delete all the apt list files since they're big and get stale quickly RUN rm -rf /var/lib/apt/lists/* # this forces \"apt-get update\" in dependent images, which is also good # (see also https://bugs.launchpad.net/cloud-images/+bug/1699913) # a few minor docker-specific tweaks # see https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap RUN set -xe \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L40-L48 && echo '#!/bin/sh' > /usr/sbin/policy-rc.d \\ && echo 'exit 101' >> /usr/sbin/policy-rc.d \\ && chmod +x /usr/sbin/policy-rc.d \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L54-L56 && dpkg-divert --local --rename --add /sbin/initctl \\ && cp -a /usr/sbin/policy-rc.d /sbin/initctl \\ && sed -i 's/^exit.*/exit 0/' /sbin/initctl \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L71-L78 && echo 'force-unsafe-io' > /etc/dpkg/dpkg.cfg.d/docker-apt-speedup \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L85-L105 && echo 'DPkg::Post-Invoke { \"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\"; };' > /etc/apt/apt.conf.d/docker-clean \\ && echo 'APT::Update::Post-Invoke { \"rm -f /var/cache/apt/archives/*.deb /var/cache/apt/archives/partial/*.deb /var/cache/apt/*.bin || true\"; };' >> /etc/apt/apt.conf.d/docker-clean \\ && echo 'Dir::Cache::pkgcache \"\"; Dir::Cache::srcpkgcache \"\";' >> /etc/apt/apt.conf.d/docker-clean \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L109-L115 && echo 'Acquire::Languages \"none\";' > /etc/apt/apt.conf.d/docker-no-languages \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L118-L130 && echo 'Acquire::GzipIndexes \"true\"; Acquire::CompressionTypes::Order:: \"gz\";' > /etc/apt/apt.conf.d/docker-gzip-indexes \\ \\ # https://github.com/docker/docker/blob/9a9fc01af8fb5d98b8eec0740716226fadb3735c/contrib/mkimage/debootstrap#L134-L151 && echo 'Apt::AutoRemove::SuggestsImportant \"false\";' > /etc/apt/apt.conf.d/docker-autoremove-suggests # make systemd-detect-virt return \"docker\" # See: https://github.com/systemd/systemd/blob/aa0c34279ee40bce2f9681b496922dedbadfca19/src/basic/virt.c#L434 RUN mkdir -p /run/systemd && echo 'docker' > /run/systemd/container CMD [ \"/bin/bash\" ] To run the docker image, # deprecated $ docker run ubuntu:16.04 $ docker run <ImageID> # New command $ docker container run ubuntu:16.04","title":"DockerHub and Docker repository"},{"location":"DevOps/docker-basics/#dockerfile","text":"As an example, building a docker image with Ubuntu 16.04 as the base OS image, update the image to latest patches and install python3 on top of it. Sample Dockerfile FROM ubuntu:16.04 LABEL maintainer = \"deepgorthi@gmail.com\" RUN apt-get update RUN apt-get install -y python3","title":"Dockerfile"},{"location":"DevOps/docker-basics/#building-docker-image","text":"To build the docker image, we need to run # '.' specifies to look for the dockerfile in the current directory $ docker image build . If the name of the dockerfile is different, we can build the image by doing $ docker image build <docker file name> Build output $ docker image build . Sending build context to Docker daemon 2 .048kB Step 1 /4 : FROM ubuntu:16.04 ---> 56bab49eef2e Step 2 /4 : LABEL maintainer = \"deepgorthi@gmail.com\" ---> Running in d76d48fb3f92 Removing intermediate container d76d48fb3f92 ---> 03b3755c4237 Step 3 /4 : RUN apt-get update ---> Running in 6b1995d3f8d0 Get:1 http://archive.ubuntu.com/ubuntu xenial InRelease [ 247 kB ] Get:2 http://security.ubuntu.com/ubuntu xenial-security InRelease [ 109 kB ] Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [ 109 kB ] Get:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [ 1019 kB ] Get:5 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [ 107 kB ] Get:6 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [ 1558 kB ] Get:7 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [ 12 .7 kB ] Get:8 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [ 593 kB ] Get:9 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [ 6280 B ] Get:10 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [ 14 .1 kB ] Get:11 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [ 9827 kB ] Get:12 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [ 176 kB ] Get:13 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [ 1396 kB ] Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [ 13 .1 kB ] Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [ 996 kB ] Get:16 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [ 19 .3 kB ] Get:17 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [ 7942 B ] Get:18 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [ 8807 B ] Fetched 16 .2 MB in 3s ( 4134 kB/s ) Reading package lists... Removing intermediate container 6b1995d3f8d0 ---> 33e4e45672d8 Step 4 /4 : RUN apt-get install -y python3 ---> Running in ac8388b43a00 Reading package lists... Building dependency tree... Reading state information... The following additional packages will be installed: dh-python file libexpat1 libmagic1 libmpdec2 libpython3-stdlib libpython3.5-minimal libpython3.5-stdlib libsqlite3-0 libssl1.0.0 mime-support python3-minimal python3.5 python3.5-minimal Suggested packages: libdpkg-perl python3-doc python3-tk python3-venv python3.5-venv python3.5-doc binutils binfmt-support The following NEW packages will be installed: dh-python file libexpat1 libmagic1 libmpdec2 libpython3-stdlib libpython3.5-minimal libpython3.5-stdlib libsqlite3-0 libssl1.0.0 mime-support python3 python3-minimal python3.5 python3.5-minimal 0 upgraded, 15 newly installed, 0 to remove and 4 not upgraded. Need to get 6436 kB of archives. After this operation, 33 .2 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libssl1.0.0 amd64 1 .0.2g-1ubuntu4.15 [ 1084 kB ] Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython3.5-minimal amd64 3 .5.2-2ubuntu0~16.04.9 [ 524 kB ] Get:3 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libexpat1 amd64 2 .1.0-7ubuntu0.16.04.5 [ 71 .5 kB ] Get:4 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python3.5-minimal amd64 3 .5.2-2ubuntu0~16.04.9 [ 1593 kB ] Get:5 http://archive.ubuntu.com/ubuntu xenial/main amd64 python3-minimal amd64 3 .5.1-3 [ 23 .3 kB ] Get:6 http://archive.ubuntu.com/ubuntu xenial/main amd64 mime-support all 3 .59ubuntu1 [ 31 .0 kB ] Get:7 http://archive.ubuntu.com/ubuntu xenial/main amd64 libmpdec2 amd64 2 .4.2-1 [ 82 .6 kB ] Get:8 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libsqlite3-0 amd64 3 .11.0-1ubuntu1.3 [ 397 kB ] Get:9 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libpython3.5-stdlib amd64 3 .5.2-2ubuntu0~16.04.9 [ 2137 kB ] Get:10 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 python3.5 amd64 3 .5.2-2ubuntu0~16.04.9 [ 165 kB ] Get:11 http://archive.ubuntu.com/ubuntu xenial/main amd64 libpython3-stdlib amd64 3 .5.1-3 [ 6818 B ] Get:12 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 dh-python all 2 .20151103ubuntu1.2 [ 73 .9 kB ] Get:13 http://archive.ubuntu.com/ubuntu xenial/main amd64 python3 amd64 3 .5.1-3 [ 8710 B ] Get:14 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 libmagic1 amd64 1 :5.25-2ubuntu1.3 [ 216 kB ] Get:15 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 file amd64 1 :5.25-2ubuntu1.3 [ 21 .3 kB ] debconf: delaying package configuration, since apt-utils is not installed Fetched 6436 kB in 2s ( 3013 kB/s ) Selecting previously unselected package libssl1.0.0:amd64. ( Reading database ... 4781 files and directories currently installed. ) Preparing to unpack .../libssl1.0.0_1.0.2g-1ubuntu4.15_amd64.deb ... Unpacking libssl1.0.0:amd64 ( 1 .0.2g-1ubuntu4.15 ) ... Selecting previously unselected package libpython3.5-minimal:amd64. Preparing to unpack .../libpython3.5-minimal_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking libpython3.5-minimal:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package libexpat1:amd64. Preparing to unpack .../libexpat1_2.1.0-7ubuntu0.16.04.5_amd64.deb ... Unpacking libexpat1:amd64 ( 2 .1.0-7ubuntu0.16.04.5 ) ... Selecting previously unselected package python3.5-minimal. Preparing to unpack .../python3.5-minimal_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking python3.5-minimal ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package python3-minimal. Preparing to unpack .../python3-minimal_3.5.1-3_amd64.deb ... Unpacking python3-minimal ( 3 .5.1-3 ) ... Selecting previously unselected package mime-support. Preparing to unpack .../mime-support_3.59ubuntu1_all.deb ... Unpacking mime-support ( 3 .59ubuntu1 ) ... Selecting previously unselected package libmpdec2:amd64. Preparing to unpack .../libmpdec2_2.4.2-1_amd64.deb ... Unpacking libmpdec2:amd64 ( 2 .4.2-1 ) ... Selecting previously unselected package libsqlite3-0:amd64. Preparing to unpack .../libsqlite3-0_3.11.0-1ubuntu1.3_amd64.deb ... Unpacking libsqlite3-0:amd64 ( 3 .11.0-1ubuntu1.3 ) ... Selecting previously unselected package libpython3.5-stdlib:amd64. Preparing to unpack .../libpython3.5-stdlib_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking libpython3.5-stdlib:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package python3.5. Preparing to unpack .../python3.5_3.5.2-2ubuntu0~16.04.9_amd64.deb ... Unpacking python3.5 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Selecting previously unselected package libpython3-stdlib:amd64. Preparing to unpack .../libpython3-stdlib_3.5.1-3_amd64.deb ... Unpacking libpython3-stdlib:amd64 ( 3 .5.1-3 ) ... Selecting previously unselected package dh-python. Preparing to unpack .../dh-python_2.20151103ubuntu1.2_all.deb ... Unpacking dh-python ( 2 .20151103ubuntu1.2 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Setting up libssl1.0.0:amd64 ( 1 .0.2g-1ubuntu4.15 ) ... debconf: unable to initialize frontend: Dialog debconf: ( TERM is not set, so the dialog frontend is not usable. ) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: ( Can ' t locate Term/ReadLine.pm in @INC ( you may need to install the Term::ReadLine module ) ( @INC contains: /etc/perl /usr/local/lib/x86_64-linux-gnu/perl/5.22.1 /usr/local/share/perl/5.22.1 /usr/lib/x86_64-linux-gnu/perl5/5.22 /usr/share/perl5 /usr/lib/x86_64-linux-gnu/perl/5.22 /usr/share/perl/5.22 /usr/local/lib/site_perl /usr/lib/x86_64-linux-gnu/perl-base . ) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7 . ) debconf: falling back to frontend: Teletype Setting up libpython3.5-minimal:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up libexpat1:amd64 ( 2 .1.0-7ubuntu0.16.04.5 ) ... Setting up python3.5-minimal ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up python3-minimal ( 3 .5.1-3 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Selecting previously unselected package python3. ( Reading database ... 5757 files and directories currently installed. ) Preparing to unpack .../python3_3.5.1-3_amd64.deb ... Unpacking python3 ( 3 .5.1-3 ) ... Selecting previously unselected package libmagic1:amd64. Preparing to unpack .../libmagic1_1%3a5.25-2ubuntu1.3_amd64.deb ... Unpacking libmagic1:amd64 ( 1 :5.25-2ubuntu1.3 ) ... Selecting previously unselected package file. Preparing to unpack .../file_1%3a5.25-2ubuntu1.3_amd64.deb ... Unpacking file ( 1 :5.25-2ubuntu1.3 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Setting up mime-support ( 3 .59ubuntu1 ) ... Setting up libmpdec2:amd64 ( 2 .4.2-1 ) ... Setting up libsqlite3-0:amd64 ( 3 .11.0-1ubuntu1.3 ) ... Setting up libpython3.5-stdlib:amd64 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up python3.5 ( 3 .5.2-2ubuntu0~16.04.9 ) ... Setting up libpython3-stdlib:amd64 ( 3 .5.1-3 ) ... Setting up libmagic1:amd64 ( 1 :5.25-2ubuntu1.3 ) ... Setting up file ( 1 :5.25-2ubuntu1.3 ) ... Setting up python3 ( 3 .5.1-3 ) ... running python rtupdate hooks for python3.5... running python post-rtupdate hooks for python3.5... Setting up dh-python ( 2 .20151103ubuntu1.2 ) ... Processing triggers for libc-bin ( 2 .23-0ubuntu11 ) ... Removing intermediate container ac8388b43a00 ---> a41c41ef3ce9 Successfully built a41c41ef3ce9","title":"Building Docker image"},{"location":"DevOps/docker-basics/#running-the-container","text":"To run the image in a docker container, $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> a41c41ef3ce9 11 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB $ docker container run -i -t --name python-container a41 # just enough chars are enough to distinguish between docker images # -i is short for --interactive. Keep STDIN open even if unattached. # -t is short for--tty. Allocates a pseudo terminal that connects your terminal with the container\u2019s STDIN and STDOUT. # You need to specify both -i and -t to then interact with the container through your terminal shell. A container stops after exiting from the command line of that container. List of running containers: $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e365eb0e4568 a41 \"/bin/bash\" 42 seconds ago Up 40 seconds python-container_v2 List of all stopped and running containers: $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e365eb0e4568 a41 \"/bin/bash\" About a minute ago Up About a minute python-container_v2 11087b710633 a41 \"/bin/bash\" 11 minutes ago Exited ( 0 ) 4 minutes ago python-container db3e7658a34b ubuntu:16.04 \"/bin/bash\" 35 minutes ago Exited ( 0 ) 34 minutes ago cool_beaver b6fd3fb58e5a hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago hardcore_pascal 167b6f3834e0 hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago boring_hellman Starting and stopping a container without running the shell or logging in: $ docker container start python-container python-container $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 11087b710633 a41 \"/bin/bash\" 14 minutes ago Up 24 seconds python-container $ docker container stop python-container python-container $ docker container ls CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES After starting up a container using start , to attach to that container, we can run: $ docker container attach <container ID> root@11087b710633:/#","title":"Running the Container"},{"location":"DevOps/docker-basics/#removing-containers-rm","text":"$ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES e365eb0e4568 a41 \"/bin/bash\" 10 minutes ago Exited ( 0 ) 6 minutes ago python-container_v2 11087b710633 a41 \"/bin/bash\" 20 minutes ago Exited ( 0 ) 7 seconds ago python-container db3e7658a34b ubuntu:16.04 \"/bin/bash\" 43 minutes ago Exited ( 0 ) 43 minutes ago cool_beaver b6fd3fb58e5a hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago hardcore_pascal 167b6f3834e0 hello-world \"/hello\" 6 hours ago Exited ( 0 ) 6 hours ago boring_hellman $ docker container rm 167b6f3834e0 167b6f3834e0 $ docker container rm hardcore_pascal hardcore_pascal $ docker container rm db3e7658a34b python-container python-container_v2 db3e7658a34b python-container python-container_v2 $ docker container ls -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES","title":"Removing Containers (rm)"},{"location":"DevOps/docker-basics/#removing-images","text":"$ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> a41c41ef3ce9 34 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB hello-world latest fce289e99eb9 11 months ago 1 .84kB $ docker image rm fce289e99eb9 Untagged: hello-world:latest Untagged: hello-world@sha256:4fe721ccc2e8dc7362278a29dc660d833570ec2682f4e4194f4ee23e415e1064 Deleted: sha256:fce289e99eb9bca977dae136fbe2a82b6b7d4c372474c9235adc1741675f587e Deleted: sha256:af0b15c8625bb1938f1d7b17081031f649fd14e6b233688eea3c5483994a66a3","title":"Removing Images"},{"location":"DevOps/docker-basics/#pushing-image-to-docker-repo","text":"To login to docker: $ docker login Login with your Docker ID to push and pull images from Docker Hub. If you don ' t have a Docker ID, head over to https://hub.docker.com to create one. Username: deepgorthi Password: WARNING! Your password will be stored unencrypted in /home/cloud_user/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store Login Succeeded Tag the image that will be pushed to docker repo and push it: $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE <none> <none> a41c41ef3ce9 37 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB $ docker image tag a41c41ef3ce9 deepgorthi/ubu-python3 $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE deepgorthi/ubu-python3 latest a41c41ef3ce9 38 minutes ago 186MB ubuntu 16 .04 56bab49eef2e 2 weeks ago 123MB $ docker image push deepgorthi/ubu-python3 The push refers to repository [ docker.io/deepgorthi/ubu-python3 ] fc529f7b8eec: Pushed c992de3f99a0: Pushed aa7f8c8d5f39: Mounted from library/ubuntu 48817fbd6c92: Mounted from library/ubuntu 1b039d138968: Mounted from library/ubuntu 7082d7d696f8: Mounted from library/ubuntu latest: digest: sha256:f348584be25aa65a0d266d77eadfeaad955385736462cea5efd68d1a4473760d size: 1574 Links List of Docker commands VMs, Containers and Docker Learn enough useful Docker series Part 1 Part 2 Part 3 Part 4 Part 5 Part 6","title":"Pushing image to Docker repo"},{"location":"DevOps/docker-deepdive/","text":"Docker Deepdive \u00b6 VMs vs Containers \u00b6 Two types of hypervisors: Type 1: Direct link to infrastructure (Hyperkit on Mac/Hyper-V on Windows). Generally, more efficient than Type 2 due to their direct link to hardware. Type 2: Runs as an app on the Host OS (VMWare/VirtualBox) While working with VMs, each VM will have its own OS that needs CPU, memory and other resources. While working with containers, the need for separate OS is eliminated by the use of Docker daemon that communicates directly with the Host OS and knows how to assign resources to running docker containers. It also makes sure each container is isolated from Host OS and other containers. VMs take minutes to start while Docker containers take milliseconds. VMs wastes resources by allocating separate OS for each app while Docker containers save resources by using Docker daemon. While VMs isolate systems , Docker containers isolate applications . When to use VMs and Docker \u00b6 The setup might be different in development and production environments. If we want to test the Firewall config for Ubuntu OS that is running Docker containers, this cannot be done if we solely rely on containers when the host OS on dev setup might be Windows or Mac OS and not Ubuntu. To solve this issue, we can run a VM in the dev setup which has the containers in it. Development Setup Production Setup Docker's Architecture \u00b6 Docker Arch source Docker is written in Go and takes advantage of several features of the Linux kernel to deliver its functionality. Because the Docker Engine daemon uses Linux-specific kernel features, you can't run Docker Engine natively on Windows. Instead, you must use the Docker Machine command, docker-machine, to create and attach to a small Linux VM on your machine. This VM hosts Docker Engine for you on your Windows system. Docker Engine is a client-server application with these major components: A server which is a type of long-running program called a daemon process (the dockerd command). A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do. A command line interface (CLI) client (the docker command). Namespaces: Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container.These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace. Docker Engine uses namespaces such as the following on Linux: The pid namespace: Process isolation (PID: Process ID). The net namespace: Managing network interfaces (NET: Networking). The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication). The mnt namespace: Managing filesystem mount points (MNT: Mount). The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System). Control groups: Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints. For example, you can limit the memory available to a specific container. Union file systems: Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast. Docker Engine uses UnionFS to provide the building blocks for containers. Docker Engine can use multiple UnionFS variants, including AUFS, btrfs, vfs, and DeviceMapper. Container format: Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer. In the future, Docker may support other container formats by integrating with technologies such as BSD Jails or Solaris Zones. Images and Containers \u00b6 A docker image cannot be changed. A new version of the image can be created using the base image. A container and image are not interchangeable. A container is run using the image. The act of running an image is called a container Image can be viewed as class Container can be seen as instance of that class Many containers can be run using a single image. Containers are immutable. Changes are not maintained after stopping a docker container. Automated Builds can be triggered from DockerHub by connecting the account to Github. We can create automated workflows like: - pushing code changes to github to trigger a CI task - kick off CI tasks like testing using the CI server - build a new docker image after testing is completed - send a custom webhook to a server - receive the webhook from the server - pull new docker image after receiving the webhook - restart containers with the new docker image Two ways to build a Docker image: Option 1: Run a docker container, make the changes and commit them with docker commit Option 2: To use a Dockerfile Commit is rarely used to build a docker image. Dockerfile is superior because of its version control support. Dockerfile is a Blueprint of what the docker image will be. The reason why docker images are so efficient is that they use Layers. While pulling the latest image, it compares what has changed between the layers and downloads only the change by using Layers. Building docker images goes with the same logic by using Layers. Building Docker image \u00b6 Docker CLI commands $ docker --help Usage: docker [ OPTIONS ] COMMAND A self-sufficient runtime for containers Options: --config string Location of client config files ( default \"/Users/pradeepgorthi/.docker\" ) -c, --context string Name of the context to use to connect to the daemon ( overrides DOCKER_HOST env var and default context set with \"docker context use\" ) -D, --debug Enable debug mode -H, --host list Daemon socket ( s ) to connect to -l, --log-level string Set the logging level ( \"debug\" | \"info\" | \"warn\" | \"error\" | \"fatal\" ) ( default \"info\" ) --tls Use TLS ; implied by --tlsverify --tlscacert string Trust certs signed only by this CA ( default \"/Users/pradeepgorthi/.docker/ca.pem\" ) --tlscert string Path to TLS certificate file ( default \"/Users/pradeepgorthi/.docker/cert.pem\" ) --tlskey string Path to TLS key file ( default \"/Users/pradeepgorthi/.docker/key.pem\" ) --tlsverify Use TLS and verify the remote -v, --version Print version information and quit Management Commands: builder Manage builds config Manage Docker configs container Manage containers context Manage contexts image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes Commands: attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container 's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container' s filesystem events Get real time events from the server exec Run a command in a running container export Export a container 's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes Run ' docker COMMAND --help ' for more information on a command. Sample Docker file: FROM python:3.8-alpine # Runs a command in the image RUN mkdir /app # Changes Working directory WORKDIR /app COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt # Copy all the files in the current/below dir to the destination directory in the image COPY . . LABEL maintainer = \"deepgorthi\" \\ version = \"1.0\" # Default command that will be run when the docker image is ran as opposed to `RUN` which is executed when docker image is in build stage. CMD flask run --host = 0 .0.0.0 --port = 5000 Docker image can be built using: docker image build -t |image_name| |project_dir| $ docker image build -t web1 . Sending build context to Docker daemon 6 .144kB Step 1 /8 : FROM python:3.8-alpine 3 .8-alpine: Pulling from library/python 89d9c30c1d48: Already exists 910c49c00810: Pull complete 66564da92047: Pull complete bb0265c0d9a9: Pull complete b86dfbfbb035: Pull complete Digest: sha256:7f465d82a49e092b609fa97cea8ea761c9aa4fa6cab05f4876150e28bf16bcc3 Status: Downloaded newer image for python:3.8-alpine ---> 59acf2b3028c Step 2 /8 : RUN mkdir /app ---> Running in 35c9bfe4094f Removing intermediate container 35c9bfe4094f ---> 189a38c581b5 Step 3 /8 : WORKDIR /app ---> Running in 548c3c0d8c92 Removing intermediate container 548c3c0d8c92 ---> 86fed1cc4245 Step 4 /8 : COPY requirements.txt requirements.txt ---> 09f541a26042 Step 5 /8 : RUN pip3 install -r requirements.txt ---> Running in 9dd700237426 Collecting Flask == 1 .1.1 Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl ( 94kB ) Collecting click> = 5 .1 Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl ( 81kB ) Collecting Werkzeug> = 0 .15 Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl ( 327kB ) Collecting itsdangerous> = 0 .24 Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl Collecting Jinja2> = 2 .10.1 Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl ( 125kB ) Collecting MarkupSafe> = 0 .23 Downloading https://files.pythonhosted.org/packages/b9/2e/64db92e53b86efccfaea71321f597fa2e1b2bd3853d8ce658568f7a13094/MarkupSafe-1.1.1.tar.gz Building wheels for collected packages: MarkupSafe Building wheel for MarkupSafe ( setup.py ) : started Building wheel for MarkupSafe ( setup.py ) : finished with status 'done' Created wheel for MarkupSafe: filename = MarkupSafe-1.1.1-cp38-none-any.whl size = 12629 sha256 = 2ef358e3f930fc77951c40c287a217e2947523c8bed82a87b8b339905517a6bf Stored in directory: /root/.cache/pip/wheels/f2/aa/04/0edf07a1b8a5f5f1aed7580fffb69ce8972edc16a505916a77 Successfully built MarkupSafe Installing collected packages: click, Werkzeug, itsdangerous, MarkupSafe, Jinja2, Flask Successfully installed Flask-1.1.1 Jinja2-2.10.3 MarkupSafe-1.1.1 Werkzeug-0.16.0 click-7.0 itsdangerous-1.1.0 Removing intermediate container 9dd700237426 ---> 698e47e5a545 Step 6 /8 : COPY . . ---> bc46e74d6e0d Step 7 /8 : LABEL maintainer = \"deepgorthi\" version = \"1.0\" ---> Running in 381673955c74 Removing intermediate container 381673955c74 ---> b38993872302 Step 8 /8 : CMD flask run --host = 0 .0.0.0 --port = 5000 ---> Running in deb8e859481f Removing intermediate container deb8e859481f ---> 6b827d1a1fa1 Successfully built 6b827d1a1fa1 Successfully tagged web1:latest We can inspect the newly built docker image: $ docker image inspect web1 Deleting docker image: $ docker image rm web1 To run a container using the web1 docker image: $ docker container run -it -p 5000 :5000 --rm -e FLASK_APP = app.py web1 # '-p' -> Port mapping from localhost to docker image # '--rm' -> Deletes the container once it is stopped # '-e' -> ENV variable # and lastly the docker image (web1) # '-d' -> Run the container in detach mode (running in background) # '--restart on-failure' -> restart docker container if it stops unexpectedly, cannot be used with '--rm' # we can use '-p 5000' instead of '-p 5000:5000' to use a random port on localhost * Serving Flask app \"app.py\" * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ ( Press CTRL+C to quit ) # Checking logs $ docker container logs <container_name> # Checking stats $ docker container stats # Using help for container run commands $ docker container run --help # To run commands in a detached container # $ docker container exec -it <container name or ID> <command> $ docker container exec -it web1 sh $ docker container exec -it --user \" $( id -u ) : $( id -g ) \" web1 touch test.txt # create a file with user and not root Volumes \u00b6 When changes are made with the source code, the initial docker image does not reflect the code change. The container needs to be re-run with a newly built image that has the recent code change. To make this easier, we can make use of Docker Volumes where we can mount local machine dir to the container. $ docker container run -it -p 5000 :5000 -e FLASK_APP = app.py --rm --name web1 -e FLASK_DEBUG = 1 -v \" $PWD :/app\" web1 # '-v source:dest' -> This will overwrite the image with the new source code through Dockerfile. # This is best for development # In prod, the dockerfile will copy the source code at the time of build and bake it into the image without a need for volume mounts. Named Volumes for Persistent Data: Instead of providing a local path to mount to docker container, we can provide a named volume so that docker can use that to manage the volume. This is perfect for databases. This is for persisting data after a container stops. $ docker volume create <vol_name> $ docker volume ls $ docker volume inspect <vol_name> # For Redis image, the db will be at /data $ docker container run -it -p 5000 :5000 -e FLASK_APP = app.py --rm --name web1 -e FLASK_DEBUG = 1 -v vol_name:/data web1 Sharing Data between Containers: To share data between containers, in Dockerfile, give an instruction with the path to the dir that will be shared with other containers: VOLUME [ \"/app/public\" ] To share data from one container to another, after modifying the Dockerfile for an image, build the image and run the container from that image. Once that is done, run the second container and add --volumes-from <source_container_name> to the docker container command while starting up the second container. This will result in container2 accessing data from container1. Changes on running container1 can be seen immediately in running container2. Networks \u00b6 Docker creates a few networks for us. $ docker network ls NETWORK ID NAME DRIVER SCOPE 8f87c8b5b9ba bridge bridge local 8674645af790 host host local fba1734c012f none null local $ docker network inspect bridge Bridge network is the default network and all containers are automatically added to this network when created. To refer the containers with IP addresses might be a problem when the IP address changes on a reboot. Instead, it is better to use DNS names to refer to Docker contaienrs on a network. If we create our own bridge network with docker, then Docker will configure DNS name for the containers automatically for us. To create a bridge network for automatic DNS: $ docker network create --driver bridge <network_name> # Once the network is created, the containers can be assigned into that network by adding a '--net' tag with the network name $ docker container run --rm -itd -p 5000 -e FLASK_APP = app.py --name web1 -v \" $PWD :/app\" --net <network_name> web2 Bridge network driver can only connect docker containers on the same docker host. If we want to connect over multiple docker hosts, then we need to use overlay network driver. Optimizing Docker Images \u00b6 Similar to .gitignore, there is a .dockerignore file to ignore unnecessary files in the image. The starting point of .dockerignore paths is the WORKDIR path set in the Dockerfile. .dockerignore .git/ .foo/* # Allows folder to be included but the contents are ignored **/*.swp # Recursively ignore all files of specific type **/*.txt !special.txt # All text files will be ignored because of the previous line but special.txt will be included in the image Running scripts when a container starts: We can do this by using ENTRYPOINT instruction in Dockerfile. ENTRYPOINT lets you run custom scripts. They do not add layers to the docker image as the commands are run after the image is ready from Dockerfile. $ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 17 11 3 .678GB 2 .196GB ( 59 % ) Containers 35 0 163MB 163MB ( 100 % ) Local Volumes 0 0 0B 0B Build Cache 0 0 0B 0B $ docker system prune -y WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache docker-compose \u00b6 There is no need to keep docker-compose.yml file in the same dir as the Dockerfiles. We can refer the folder where dockerfiles are placed using build: and specifying the path to directory where Dockerfile is placed. version : '3' services : redis : image : 'redis:3.2-alpine' ports : - '6379:6379' volumes : - 'redis:/data' web : build : . depends_on : - 'redis' env_file : - '.env' ports : - '5000:5000' volumes : - '.:/app' volumes : redis : {} # .env file COMPOSE_PROJECT_NAME=web2 PYTHONBUFFERED=true FLASK_APP=app.py FLASK_DEBUG=1 docker-compose --help $ docker-compose --help Define and run multi-container applications with Docker. Usage: docker-compose [ -f <arg>... ] [ options ] [ COMMAND ] [ ARGS... ] docker-compose -h | --help Options: -f, --file FILE Specify an alternate compose file ( default: docker-compose.yml ) -p, --project-name NAME Specify an alternate project name ( default: directory name ) --verbose Show more output --log-level LEVEL Set log level ( DEBUG, INFO, WARNING, ERROR, CRITICAL ) --no-ansi Do not print ANSI control characters -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS ; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don 't check the daemon' s hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory ( default: the path of the Compose file ) --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalent Commands: build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information Running docker-compose file: # Building image $ docker-compose build # Pulling any required images from Docker repo $ docker-compose pull # Setting up the project $ docker-compose up # All the 3 above commands can be run using $ docker-compose up --build # Logs output $ docker-compose logs -f # Executing a command on a container $ docker-compose exec <service_name> ls -la # Stopping containers $ docker-compose stop # Removing stopped containers $ docker-compose rm","title":"Docker Deepdive"},{"location":"DevOps/docker-deepdive/#docker-deepdive","text":"","title":"Docker Deepdive"},{"location":"DevOps/docker-deepdive/#vms-vs-containers","text":"Two types of hypervisors: Type 1: Direct link to infrastructure (Hyperkit on Mac/Hyper-V on Windows). Generally, more efficient than Type 2 due to their direct link to hardware. Type 2: Runs as an app on the Host OS (VMWare/VirtualBox) While working with VMs, each VM will have its own OS that needs CPU, memory and other resources. While working with containers, the need for separate OS is eliminated by the use of Docker daemon that communicates directly with the Host OS and knows how to assign resources to running docker containers. It also makes sure each container is isolated from Host OS and other containers. VMs take minutes to start while Docker containers take milliseconds. VMs wastes resources by allocating separate OS for each app while Docker containers save resources by using Docker daemon. While VMs isolate systems , Docker containers isolate applications .","title":"VMs vs Containers"},{"location":"DevOps/docker-deepdive/#when-to-use-vms-and-docker","text":"The setup might be different in development and production environments. If we want to test the Firewall config for Ubuntu OS that is running Docker containers, this cannot be done if we solely rely on containers when the host OS on dev setup might be Windows or Mac OS and not Ubuntu. To solve this issue, we can run a VM in the dev setup which has the containers in it. Development Setup Production Setup","title":"When to use VMs and Docker"},{"location":"DevOps/docker-deepdive/#dockers-architecture","text":"Docker Arch source Docker is written in Go and takes advantage of several features of the Linux kernel to deliver its functionality. Because the Docker Engine daemon uses Linux-specific kernel features, you can't run Docker Engine natively on Windows. Instead, you must use the Docker Machine command, docker-machine, to create and attach to a small Linux VM on your machine. This VM hosts Docker Engine for you on your Windows system. Docker Engine is a client-server application with these major components: A server which is a type of long-running program called a daemon process (the dockerd command). A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do. A command line interface (CLI) client (the docker command). Namespaces: Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container.These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace. Docker Engine uses namespaces such as the following on Linux: The pid namespace: Process isolation (PID: Process ID). The net namespace: Managing network interfaces (NET: Networking). The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication). The mnt namespace: Managing filesystem mount points (MNT: Mount). The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System). Control groups: Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints. For example, you can limit the memory available to a specific container. Union file systems: Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast. Docker Engine uses UnionFS to provide the building blocks for containers. Docker Engine can use multiple UnionFS variants, including AUFS, btrfs, vfs, and DeviceMapper. Container format: Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer. In the future, Docker may support other container formats by integrating with technologies such as BSD Jails or Solaris Zones.","title":"Docker's Architecture"},{"location":"DevOps/docker-deepdive/#images-and-containers","text":"A docker image cannot be changed. A new version of the image can be created using the base image. A container and image are not interchangeable. A container is run using the image. The act of running an image is called a container Image can be viewed as class Container can be seen as instance of that class Many containers can be run using a single image. Containers are immutable. Changes are not maintained after stopping a docker container. Automated Builds can be triggered from DockerHub by connecting the account to Github. We can create automated workflows like: - pushing code changes to github to trigger a CI task - kick off CI tasks like testing using the CI server - build a new docker image after testing is completed - send a custom webhook to a server - receive the webhook from the server - pull new docker image after receiving the webhook - restart containers with the new docker image Two ways to build a Docker image: Option 1: Run a docker container, make the changes and commit them with docker commit Option 2: To use a Dockerfile Commit is rarely used to build a docker image. Dockerfile is superior because of its version control support. Dockerfile is a Blueprint of what the docker image will be. The reason why docker images are so efficient is that they use Layers. While pulling the latest image, it compares what has changed between the layers and downloads only the change by using Layers. Building docker images goes with the same logic by using Layers.","title":"Images and Containers"},{"location":"DevOps/docker-deepdive/#building-docker-image","text":"Docker CLI commands $ docker --help Usage: docker [ OPTIONS ] COMMAND A self-sufficient runtime for containers Options: --config string Location of client config files ( default \"/Users/pradeepgorthi/.docker\" ) -c, --context string Name of the context to use to connect to the daemon ( overrides DOCKER_HOST env var and default context set with \"docker context use\" ) -D, --debug Enable debug mode -H, --host list Daemon socket ( s ) to connect to -l, --log-level string Set the logging level ( \"debug\" | \"info\" | \"warn\" | \"error\" | \"fatal\" ) ( default \"info\" ) --tls Use TLS ; implied by --tlsverify --tlscacert string Trust certs signed only by this CA ( default \"/Users/pradeepgorthi/.docker/ca.pem\" ) --tlscert string Path to TLS certificate file ( default \"/Users/pradeepgorthi/.docker/cert.pem\" ) --tlskey string Path to TLS key file ( default \"/Users/pradeepgorthi/.docker/key.pem\" ) --tlsverify Use TLS and verify the remote -v, --version Print version information and quit Management Commands: builder Manage builds config Manage Docker configs container Manage containers context Manage contexts image Manage images network Manage networks node Manage Swarm nodes plugin Manage plugins secret Manage Docker secrets service Manage services stack Manage Docker stacks swarm Manage Swarm system Manage Docker trust Manage trust on Docker images volume Manage volumes Commands: attach Attach local standard input, output, and error streams to a running container build Build an image from a Dockerfile commit Create a new image from a container 's changes cp Copy files/folders between a container and the local filesystem create Create a new container diff Inspect changes to files or directories on a container' s filesystem events Get real time events from the server exec Run a command in a running container export Export a container 's filesystem as a tar archive history Show the history of an image images List images import Import the contents from a tarball to create a filesystem image info Display system-wide information inspect Return low-level information on Docker objects kill Kill one or more running containers load Load an image from a tar archive or STDIN login Log in to a Docker registry logout Log out from a Docker registry logs Fetch the logs of a container pause Pause all processes within one or more containers port List port mappings or a specific mapping for the container ps List containers pull Pull an image or a repository from a registry push Push an image or a repository to a registry rename Rename a container restart Restart one or more containers rm Remove one or more containers rmi Remove one or more images run Run a command in a new container save Save one or more images to a tar archive (streamed to STDOUT by default) search Search the Docker Hub for images start Start one or more stopped containers stats Display a live stream of container(s) resource usage statistics stop Stop one or more running containers tag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top Display the running processes of a container unpause Unpause all processes within one or more containers update Update configuration of one or more containers version Show the Docker version information wait Block until one or more containers stop, then print their exit codes Run ' docker COMMAND --help ' for more information on a command. Sample Docker file: FROM python:3.8-alpine # Runs a command in the image RUN mkdir /app # Changes Working directory WORKDIR /app COPY requirements.txt requirements.txt RUN pip3 install -r requirements.txt # Copy all the files in the current/below dir to the destination directory in the image COPY . . LABEL maintainer = \"deepgorthi\" \\ version = \"1.0\" # Default command that will be run when the docker image is ran as opposed to `RUN` which is executed when docker image is in build stage. CMD flask run --host = 0 .0.0.0 --port = 5000 Docker image can be built using: docker image build -t |image_name| |project_dir| $ docker image build -t web1 . Sending build context to Docker daemon 6 .144kB Step 1 /8 : FROM python:3.8-alpine 3 .8-alpine: Pulling from library/python 89d9c30c1d48: Already exists 910c49c00810: Pull complete 66564da92047: Pull complete bb0265c0d9a9: Pull complete b86dfbfbb035: Pull complete Digest: sha256:7f465d82a49e092b609fa97cea8ea761c9aa4fa6cab05f4876150e28bf16bcc3 Status: Downloaded newer image for python:3.8-alpine ---> 59acf2b3028c Step 2 /8 : RUN mkdir /app ---> Running in 35c9bfe4094f Removing intermediate container 35c9bfe4094f ---> 189a38c581b5 Step 3 /8 : WORKDIR /app ---> Running in 548c3c0d8c92 Removing intermediate container 548c3c0d8c92 ---> 86fed1cc4245 Step 4 /8 : COPY requirements.txt requirements.txt ---> 09f541a26042 Step 5 /8 : RUN pip3 install -r requirements.txt ---> Running in 9dd700237426 Collecting Flask == 1 .1.1 Downloading https://files.pythonhosted.org/packages/9b/93/628509b8d5dc749656a9641f4caf13540e2cdec85276964ff8f43bbb1d3b/Flask-1.1.1-py2.py3-none-any.whl ( 94kB ) Collecting click> = 5 .1 Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl ( 81kB ) Collecting Werkzeug> = 0 .15 Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl ( 327kB ) Collecting itsdangerous> = 0 .24 Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl Collecting Jinja2> = 2 .10.1 Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl ( 125kB ) Collecting MarkupSafe> = 0 .23 Downloading https://files.pythonhosted.org/packages/b9/2e/64db92e53b86efccfaea71321f597fa2e1b2bd3853d8ce658568f7a13094/MarkupSafe-1.1.1.tar.gz Building wheels for collected packages: MarkupSafe Building wheel for MarkupSafe ( setup.py ) : started Building wheel for MarkupSafe ( setup.py ) : finished with status 'done' Created wheel for MarkupSafe: filename = MarkupSafe-1.1.1-cp38-none-any.whl size = 12629 sha256 = 2ef358e3f930fc77951c40c287a217e2947523c8bed82a87b8b339905517a6bf Stored in directory: /root/.cache/pip/wheels/f2/aa/04/0edf07a1b8a5f5f1aed7580fffb69ce8972edc16a505916a77 Successfully built MarkupSafe Installing collected packages: click, Werkzeug, itsdangerous, MarkupSafe, Jinja2, Flask Successfully installed Flask-1.1.1 Jinja2-2.10.3 MarkupSafe-1.1.1 Werkzeug-0.16.0 click-7.0 itsdangerous-1.1.0 Removing intermediate container 9dd700237426 ---> 698e47e5a545 Step 6 /8 : COPY . . ---> bc46e74d6e0d Step 7 /8 : LABEL maintainer = \"deepgorthi\" version = \"1.0\" ---> Running in 381673955c74 Removing intermediate container 381673955c74 ---> b38993872302 Step 8 /8 : CMD flask run --host = 0 .0.0.0 --port = 5000 ---> Running in deb8e859481f Removing intermediate container deb8e859481f ---> 6b827d1a1fa1 Successfully built 6b827d1a1fa1 Successfully tagged web1:latest We can inspect the newly built docker image: $ docker image inspect web1 Deleting docker image: $ docker image rm web1 To run a container using the web1 docker image: $ docker container run -it -p 5000 :5000 --rm -e FLASK_APP = app.py web1 # '-p' -> Port mapping from localhost to docker image # '--rm' -> Deletes the container once it is stopped # '-e' -> ENV variable # and lastly the docker image (web1) # '-d' -> Run the container in detach mode (running in background) # '--restart on-failure' -> restart docker container if it stops unexpectedly, cannot be used with '--rm' # we can use '-p 5000' instead of '-p 5000:5000' to use a random port on localhost * Serving Flask app \"app.py\" * Environment: production WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ ( Press CTRL+C to quit ) # Checking logs $ docker container logs <container_name> # Checking stats $ docker container stats # Using help for container run commands $ docker container run --help # To run commands in a detached container # $ docker container exec -it <container name or ID> <command> $ docker container exec -it web1 sh $ docker container exec -it --user \" $( id -u ) : $( id -g ) \" web1 touch test.txt # create a file with user and not root","title":"Building Docker image"},{"location":"DevOps/docker-deepdive/#volumes","text":"When changes are made with the source code, the initial docker image does not reflect the code change. The container needs to be re-run with a newly built image that has the recent code change. To make this easier, we can make use of Docker Volumes where we can mount local machine dir to the container. $ docker container run -it -p 5000 :5000 -e FLASK_APP = app.py --rm --name web1 -e FLASK_DEBUG = 1 -v \" $PWD :/app\" web1 # '-v source:dest' -> This will overwrite the image with the new source code through Dockerfile. # This is best for development # In prod, the dockerfile will copy the source code at the time of build and bake it into the image without a need for volume mounts. Named Volumes for Persistent Data: Instead of providing a local path to mount to docker container, we can provide a named volume so that docker can use that to manage the volume. This is perfect for databases. This is for persisting data after a container stops. $ docker volume create <vol_name> $ docker volume ls $ docker volume inspect <vol_name> # For Redis image, the db will be at /data $ docker container run -it -p 5000 :5000 -e FLASK_APP = app.py --rm --name web1 -e FLASK_DEBUG = 1 -v vol_name:/data web1 Sharing Data between Containers: To share data between containers, in Dockerfile, give an instruction with the path to the dir that will be shared with other containers: VOLUME [ \"/app/public\" ] To share data from one container to another, after modifying the Dockerfile for an image, build the image and run the container from that image. Once that is done, run the second container and add --volumes-from <source_container_name> to the docker container command while starting up the second container. This will result in container2 accessing data from container1. Changes on running container1 can be seen immediately in running container2.","title":"Volumes"},{"location":"DevOps/docker-deepdive/#networks","text":"Docker creates a few networks for us. $ docker network ls NETWORK ID NAME DRIVER SCOPE 8f87c8b5b9ba bridge bridge local 8674645af790 host host local fba1734c012f none null local $ docker network inspect bridge Bridge network is the default network and all containers are automatically added to this network when created. To refer the containers with IP addresses might be a problem when the IP address changes on a reboot. Instead, it is better to use DNS names to refer to Docker contaienrs on a network. If we create our own bridge network with docker, then Docker will configure DNS name for the containers automatically for us. To create a bridge network for automatic DNS: $ docker network create --driver bridge <network_name> # Once the network is created, the containers can be assigned into that network by adding a '--net' tag with the network name $ docker container run --rm -itd -p 5000 -e FLASK_APP = app.py --name web1 -v \" $PWD :/app\" --net <network_name> web2 Bridge network driver can only connect docker containers on the same docker host. If we want to connect over multiple docker hosts, then we need to use overlay network driver.","title":"Networks"},{"location":"DevOps/docker-deepdive/#optimizing-docker-images","text":"Similar to .gitignore, there is a .dockerignore file to ignore unnecessary files in the image. The starting point of .dockerignore paths is the WORKDIR path set in the Dockerfile. .dockerignore .git/ .foo/* # Allows folder to be included but the contents are ignored **/*.swp # Recursively ignore all files of specific type **/*.txt !special.txt # All text files will be ignored because of the previous line but special.txt will be included in the image Running scripts when a container starts: We can do this by using ENTRYPOINT instruction in Dockerfile. ENTRYPOINT lets you run custom scripts. They do not add layers to the docker image as the commands are run after the image is ready from Dockerfile. $ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 17 11 3 .678GB 2 .196GB ( 59 % ) Containers 35 0 163MB 163MB ( 100 % ) Local Volumes 0 0 0B 0B Build Cache 0 0 0B 0B $ docker system prune -y WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache","title":"Optimizing Docker Images"},{"location":"DevOps/docker-deepdive/#docker-compose","text":"There is no need to keep docker-compose.yml file in the same dir as the Dockerfiles. We can refer the folder where dockerfiles are placed using build: and specifying the path to directory where Dockerfile is placed. version : '3' services : redis : image : 'redis:3.2-alpine' ports : - '6379:6379' volumes : - 'redis:/data' web : build : . depends_on : - 'redis' env_file : - '.env' ports : - '5000:5000' volumes : - '.:/app' volumes : redis : {} # .env file COMPOSE_PROJECT_NAME=web2 PYTHONBUFFERED=true FLASK_APP=app.py FLASK_DEBUG=1 docker-compose --help $ docker-compose --help Define and run multi-container applications with Docker. Usage: docker-compose [ -f <arg>... ] [ options ] [ COMMAND ] [ ARGS... ] docker-compose -h | --help Options: -f, --file FILE Specify an alternate compose file ( default: docker-compose.yml ) -p, --project-name NAME Specify an alternate project name ( default: directory name ) --verbose Show more output --log-level LEVEL Set log level ( DEBUG, INFO, WARNING, ERROR, CRITICAL ) --no-ansi Do not print ANSI control characters -v, --version Print version and exit -H, --host HOST Daemon socket to connect to --tls Use TLS ; implied by --tlsverify --tlscacert CA_PATH Trust certs signed only by this CA --tlscert CLIENT_CERT_PATH Path to TLS certificate file --tlskey TLS_KEY_PATH Path to TLS key file --tlsverify Use TLS and verify the remote --skip-hostname-check Don 't check the daemon' s hostname against the name specified in the client certificate --project-directory PATH Specify an alternate working directory ( default: the path of the Compose file ) --compatibility If set, Compose will attempt to convert keys in v3 files to their non-Swarm equivalent Commands: build Build or rebuild services bundle Generate a Docker bundle from the Compose file config Validate and view the Compose file create Create services down Stop and remove containers, networks, images, and volumes events Receive real time events from containers exec Execute a command in a running container help Get help on a command images List images kill Kill containers logs View output from containers pause Pause services port Print the public port for a port binding ps List containers pull Pull service images push Push service images restart Restart services rm Remove stopped containers run Run a one-off command scale Set number of containers for a service start Start services stop Stop services top Display the running processes unpause Unpause services up Create and start containers version Show the Docker-Compose version information Running docker-compose file: # Building image $ docker-compose build # Pulling any required images from Docker repo $ docker-compose pull # Setting up the project $ docker-compose up # All the 3 above commands can be run using $ docker-compose up --build # Logs output $ docker-compose logs -f # Executing a command on a container $ docker-compose exec <service_name> ls -la # Stopping containers $ docker-compose stop # Removing stopped containers $ docker-compose rm","title":"docker-compose"},{"location":"DevOps/docker-tidbits/","text":"Docker Tidbits \u00b6 Source: Learn enough Docker Part 1 Part 2 ** Part 3 ** Part 4 Part 5 Part 6 Containers \u00b6 Containers are hugely helpful for improving security, reproducibility, and scalability in software development. Docker is a platform to develop, deploy, and run applications inside containers. A Docker container: Holds things Is portable \u2014 It can be used on your local machine or a cloud provider\u2019s servers. Has clear interfaces for access \u2014 It has ports that can be opened for interacting through the browser. You can configure it to interact with data through the command line. Can be obtained from a remote location \u2014 An offsite registry keeps an image for your container. Containers vs VMs \u00b6 The main difference between containers and VMs is in their architectural approach. The one big difference between containers and VMs is that containers share the host system\u2019s kernel with other containers. VM VMs run on top of a physical machine using a \u201chypervisor\u201d. A hypervisor, in turn, runs on either a host machine or on \u201cbare-metal\u201d. The host machine provides the VMs with resources, including RAM and CPU. A guest machine can run on either a hosted hypervisor or a bare-metal hypervisor. Container Containers package up just the user space, and not the kernel or virtual hardware like a VM does. All the operating system level architecture is being shared across containers. The only parts that are created from scratch are the bins and libs. Images \u00b6 Images are the immutable master template that is used to pump out containers that are all exactly alike. An image contains the Dockerfile, libraries, and code your application needs to run, all bundled together. Dockerfile \u00b6 A Dockerfile is a file with instructions for how Docker should build your image. Docker Platform \u00b6 It is Docker\u2019s software that provides the ability to package and run an application in a container on any Linux server. Docker Platform bundles code files and dependencies. It promotes easy scaling by enabling portability and reproducibility. Docker Engine \u00b6 It is the client-server application. Two parts: Community Edition (CE) which is open source. Enterprise Edition (EE which is enterprise oriented and comes with additional features and support. Docker Client \u00b6 It is the primary way to interact with Docker. Docker Client uses the Docker API to send the command from CLI to the Docker Daemon. Docker Daemon \u00b6 It is the Docker server that listens for Docker API requests. The Docker Daemon manages images, containers, networks, and volumes. Docker Volumes \u00b6 These are the best way to store persistent data that your apps consume and create. Docker Registry \u00b6 It is the remote location where Docker Images are stored. You push images to a registry and pull images from a registry. Docker Hub \u00b6 It is the largest and default registry of Docker images. Docker Repository \u00b6 It is a collection of Docker images with the same name and different tags. The tag is the identifier for the image. Docker Networking \u00b6 It allows you to connect Docker containers together that are either on the same host or multiple hosts. A Docker container needs a host to run on. This can either be a physical machine or a VM, either on-prem or in the cloud. The host has the Docker daemon and client running which enables you to interact with a Docker registry on the one hand (to pull/push Docker images), and on the other hand, allows you to start, stop and inspect containers. For single host deployments, data can be exchanged via shared volumes . This introduces tight coupling and it will be harded to translate a single host deployment into a multihost deployment. The upside, though, is speed for shared volumes. In multihost deployments, we need to consider how are containers communicating within the host and how does communication paths look between different hosts. We need to take care of both performance and security aspects. Docker networking is the native container SDN solution: Bridge Mode (default) : Docker daemon creates docker0, a virtual Ethernet bridge that automatically forwards packets between any other network interfaces that are attached to it. Connects all containers on a host to this internal network through creating a pair of peer interfaces. Host Mode : Using host mode networking, the container effectively inherits the IP address from its host. This mode is faster than the bridge mode as there is no routing overhead, but it exposes the container directly to the public network and its security implications. Container mode : Docker will re-use the networking namespace of another container. The second container that started with --net=container:<first_container_name> has the same IP address as the first container. Kubernetes networking uses this mode. No Networking : This mode puts the container inside of its own network stack but doesn\u2019t configure it. For containers that don\u2019t need a network like batch jobs writing to a disk volume For containers that will be configured with custom networking. Note : By default, Docker has inter-container communication enabled [ --icc=true ]. Containers on a host can communicate without any restrictions which may lead to Denial of Service attacks. Docker Compose \u00b6 It is a tool to run apps that require multiple Docker containers and allows us to move commands into a docker-compose.yml file for reuse. Docker Services \u00b6 Services are really just \u201ccontainers in production.\u201d A service only runs one image, but it codifies the way that image runs \u2014 what ports it should use, how many replicas of the container should run so the service has the capacity it needs, and so on. Scaling a service changes the number of container instances running that piece of software, assigning more computing resources to the service in the process. Dockerfile \u00b6 A container is built from a series of layers. Each layer is read only, except the final container layer that sits on top of the others. The Dockerfile tells Docker which layers to add and in which order to add them. When an image is pulled from a remote repository to a local machine only layers that are not already on the local machine are downloaded. Layers are created in the final image only with these commands. FROM RUN COPY ADD All other commands ( Dockerfile INSTRUCTIONS ) configure, or do other tasks that does effect the image directly. Here are the Dockerfile INSTRUCTIONS: FROM \u2014 specifies the base (parent) image. LABEL \u2014 provides metadata. Good place to include maintainer info. ENV \u2014 sets a persistent environment variable. RUN \u2014 runs a command and creates an image layer. Used to install packages into containers. COPY (recommended) \u2014 copies files and directories to the container. ADD \u2014 copies files and directories to the container. Can upack local .tar files. (ADD does the same thing as COPY, but has two more use cases. ADD can be used to move files from a remote URL to a container and ADD can extract local TAR files.) ADD instruction contains the line continuation character. CMD \u2014 provides a command and arguments for an executing container. Parameters can be overridden. There can be only one CMD. WORKDIR \u2014 sets the working directory for the instructions that follow. ARG \u2014 defines a variable to pass to Docker at build-time. ENTRYPOINT \u2014 provides command and arguments for an executing container. Arguments persist. EXPOSE \u2014 exposes a port. EXPOSE does not actually publish the port. It acts as a information relay betweenthe one building the image and the one who is running the container. Use docker run with the -p flag to publish and map one or more ports at run time. The uppercase -P flag will publish all exposed ports. VOLUME \u2014 creates a directory mount point to access and store persistent data. Cheatsheet Source cheatsheet FROM Usage: FROM FROM : FROM @ Information: FROM must be the first non-comment instruction in the Dockerfile. FROM can appear multiple times within a single Dockerfile in order to create multiple images. Simply make a note of the last image ID output by the commit before each new FROM command. The tag or digest values are optional. If you omit either of them, the builder assumes a latest by default. The builder returns an error if it cannot match the tag value. Reference - Best Practices MAINTAINER Usage: MAINTAINER The MAINTAINER instruction allows you to set the Author field of the generated images. Reference RUN Usage: RUN (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows) RUN [\" \", \" \", \" \"] (exec form) Information: The exec form makes it possible to avoid shell string munging, and to RUN commands using a base image that does not contain the specified shell executable. The default shell for the shell form can be changed using the SHELL command. Normal shell processing does not occur when using the exec form. For example, RUN [\"echo\", \"$HOME\"] will not do variable substitution on $HOME. Reference - Best Practices CMD Usage: CMD [\" \",\" \",\" \"] (exec form, this is the preferred form) CMD [\" \",\" \"] (as default parameters to ENTRYPOINT) CMD (shell form) Information: The main purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well. There can only be one CMD instruction in a Dockerfile. If you list more than one CMD then only the last CMD will take effect. If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified with the JSON array format. If the user specifies arguments to docker run then they will override the default specified in CMD. Normal shell processing does not occur when using the exec form. For example, CMD [\"echo\", \"$HOME\"] will not do variable substitution on $HOME. Reference - Best Practices LABEL Usage: LABEL = [ = ...] Information: The LABEL instruction adds metadata to an image. To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing. Labels are additive including LABELs in FROM images. If Docker encounters a label/key that already exists, the new value overrides any previous labels with identical keys. To view an image\u2019s labels, use the docker inspect command. They will be under the \"Labels\" JSON attribute. Reference - Best Practices EXPOSE Usage: EXPOSE [ ...] Information: Informs Docker that the container listens on the specified network port(s) at runtime. EXPOSE does not make the ports of the container accessible to the host. Reference - Best Practices ENV Usage: ENV ENV = [ = ...] Information: The ENV instruction sets the environment variable to the value . The value will be in the environment of all \u201cdescendant\u201d Dockerfile commands and can be replaced inline as well. The environment variables set using ENV will persist when a container is run from the resulting image. The first form will set a single variable to a value with the entire string after the first space being treated as the - including characters such as spaces and quotes. Reference - Best Practices ADD Usage: ADD [ ...] ADD [\" \", ... \" \"] (this form is required for paths containing whitespace) Information: Copies new files, directories, or remote file URLs from and adds them to the filesystem of the image at the path . may contain wildcards and matching will be done using Go\u2019s filepath.Match rules. If is a file or directory, then they must be relative to the source directory that is being built (the context of the build). is an absolute path, or a path relative to WORKDIR. If doesn\u2019t exist, it is created along with all missing directories in its path. Reference - Best Practices COPY Usage: COPY [ ...] COPY [\" \", ... \" \"] (this form is required for paths containing whitespace) Information: Copies new files or directories from and adds them to the filesystem of the image at the path . may contain wildcards and matching will be done using Go\u2019s filepath.Match rules. must be relative to the source directory that is being built (the context of the build). is an absolute path, or a path relative to WORKDIR. If doesn\u2019t exist, it is created along with all missing directories in its path. Reference - Best Practices ENTRYPOINT Usage: ENTRYPOINT [\" \", \" \", \" \"] (exec form, preferred) ENTRYPOINT (shell form) Information: Allows you to configure a container that will run as an executable. Command line arguments to docker run will be appended after all elements in an exec form ENTRYPOINT and will override all elements specified using CMD. The shell form prevents any CMD or run command line arguments from being used, but the ENTRYPOINT will start via the shell. This means the executable will not be PID 1 nor will it receive UNIX signals. Prepend exec to get around this drawback. Only the last ENTRYPOINT instruction in the Dockerfile will have an effect. Reference - Best Practices VOLUME Usage: VOLUME [\" \", ...] VOLUME [ ...] Creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. Reference - Best Practices USER Usage: USER The USER instruction sets the user name or UID to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile. Reference - Best Practices WORKDIR Usage: WORKDIR Information: Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow it. It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. Reference - Best Practices ARG Usage: ARG [= ] Information: Defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg = flag. Multiple variables may be defined by specifying ARG multiple times. It is not recommended to use build-time variables for passing secrets like github keys, user credentials, etc. Build-time variable values are visible to any user of the image with the docker history command. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Docker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile. HTTP_PROXY and http_proxy HTTPS_PROXY and https_proxy FTP_PROXY and ftp_proxy NO_PROXY and no_proxy Reference ONBUILD Usage: ONBUILD Information: Adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile. Any build instruction can be registered as a trigger. Triggers are inherited by the \"child\" build only. In other words, they are not inherited by \"grand-children\" builds. The ONBUILD instruction may not trigger FROM, MAINTAINER, or ONBUILD instructions. Reference - Best Practices STOPSIGNAL Usage: STOPSIGNAL The STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit. This signal can be a valid unsigned number that matches a position in the kernel\u2019s syscall table, for instance 9, or a signal name in the format SIGNAME, for instance SIGKILL. Reference HEALTHCHECK Usage: HEALTHCHECK [ ] CMD (check container health by running a command inside the container) HEALTHCHECK NONE (disable any healthcheck inherited from the base image) Information: Tells Docker how to test a container to check that it is still working Whenever a health check passes, it becomes healthy. After a certain number of consecutive failures, it becomes unhealthy. The that can appear are... --interval= (default: 30s) --timeout= (default: 30s) --retries= (default: 3) The health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes. If a single run of the check takes longer than timeout seconds then the check is considered to have failed. It takes retries consecutive failures of the health check for the container to be considered unhealthy. There can only be one HEALTHCHECK instruction in a Dockerfile. If you list more than one then only the last HEALTHCHECK will take effect. can be either a shell command or an exec JSON array. The command's exit status indicates the health status of the container. 0: success - the container is healthy and ready for use 1: unhealthy - the container is not working correctly 2: reserved - do not use this exit code The first 4096 bytes of stdout and stderr from the are stored and can be queried with docker inspect. When the health status of a container changes, a health_status event is generated with the new status. Reference SHELL Usage: SHELL [\" \", \" \", \" \"] Information: Allows the default shell used for the shell form of commands to be overridden. Each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent instructions. Allows an alternate shell be used such as zsh, csh, tcsh, powershell, and others. Smaller Docker Images \u00b6 To help make the images as small as possible, we can use .dockerignore to exclude files that we don't need. It helps in: Excluding secrets. Reduce image size with fewer not-needed files. Reduce build cache invalidation due to log files changing. Best Practices to Reduce Image Sizes & Build Times: Use an official base image whenever possible Use variations of Alpine images when possible to keep your images lightweight. If using apt, combine RUN apt-get update with apt-get install in the same instruction to reduce number of layers to be built. Then chain multiple packages in that instruction. List the packages in alphabetical order over multiple lines with the character. For example: RUN apt-get update && apt-get install -y \\ package-one \\ package-two && rm -rf /var/lib/apt/lists/* Include && rm -rf /var/lib/apt/lists/* at the end of the RUN instruction to clean up the apt cache so it isn\u2019t stored in the layer. Use caching wisely by putting instructions likely to change lower in your Dockerfile. Use a .dockerignore file to keep unwanted and unnecessary files out of your image. CLI \u00b6 docker container my_command create \u2014 Create a container from an image. start \u2014 Start an existing container. run \u2014 Create a new container and start it. ls \u2014 List running containers. inspect \u2014 See lots of info about a container. logs \u2014 Print logs. stop \u2014 Gracefully stop running container. kill \u2014Stop main process in container abruptly. rm\u2014 Delete a stopped container. docker image my_command build \u2014 Build an image. push \u2014 Push an image to a remote registry. ls \u2014 List images. history \u2014 See intermediate image info. inspect \u2014 See lots of info about an image, including the layers. rm \u2014 Delete an image. docker version \u2014 List info about your Docker Client and Server versions. docker login \u2014 Log in to a Docker registry. docker system prune \u2014 Delete all unused containers, unused networks, and dangling images. docker container create my_repo/my_image:my_tag - Create a container from an image. docker container create -a STDIN my_image -a is short for --attach. Attach the container to STDIN, STDOUT or STDERR. docker container start my_container \u2014 Start an existing container. docker container run my_image \u2014 Create a new container and start it. docker container run -i -t -p 1000:8000 --rm my_image -i is short for --interactive. Keep STDIN open even if unattached. -t is short for--tty. Allocates a pseudo terminal that connects your terminal with the container\u2019s STDIN and STDOUT. --rm Automatically delete the container when it stops running. docker container inspect my_container \u2014 See lots of info about a container. docker container logs my_container \u2014 Print a container\u2019s logs. docker container kill $(docker ps -q) \u2014 Kill all running containers. docker image build -t my_repo/my_image:my_tag . - Build a Docker image named my_image from the Dockerfile located at the specified path or URL. docker image push my_repo/my_image:my_tag \u2014 Push an image to a registry. docker volume create docker volume ls docker volume inspect docker volume rm docker volume prune Common options for the --mount flag in docker run --mount my_options my_image : type=volume source=volume_name destination=/path/in/container readonly","title":"Docker Tidbits"},{"location":"DevOps/docker-tidbits/#docker-tidbits","text":"Source: Learn enough Docker Part 1 Part 2 ** Part 3 ** Part 4 Part 5 Part 6","title":"Docker Tidbits"},{"location":"DevOps/docker-tidbits/#containers","text":"Containers are hugely helpful for improving security, reproducibility, and scalability in software development. Docker is a platform to develop, deploy, and run applications inside containers. A Docker container: Holds things Is portable \u2014 It can be used on your local machine or a cloud provider\u2019s servers. Has clear interfaces for access \u2014 It has ports that can be opened for interacting through the browser. You can configure it to interact with data through the command line. Can be obtained from a remote location \u2014 An offsite registry keeps an image for your container.","title":"Containers"},{"location":"DevOps/docker-tidbits/#containers-vs-vms","text":"The main difference between containers and VMs is in their architectural approach. The one big difference between containers and VMs is that containers share the host system\u2019s kernel with other containers. VM VMs run on top of a physical machine using a \u201chypervisor\u201d. A hypervisor, in turn, runs on either a host machine or on \u201cbare-metal\u201d. The host machine provides the VMs with resources, including RAM and CPU. A guest machine can run on either a hosted hypervisor or a bare-metal hypervisor. Container Containers package up just the user space, and not the kernel or virtual hardware like a VM does. All the operating system level architecture is being shared across containers. The only parts that are created from scratch are the bins and libs.","title":"Containers vs VMs"},{"location":"DevOps/docker-tidbits/#images","text":"Images are the immutable master template that is used to pump out containers that are all exactly alike. An image contains the Dockerfile, libraries, and code your application needs to run, all bundled together.","title":"Images"},{"location":"DevOps/docker-tidbits/#dockerfile","text":"A Dockerfile is a file with instructions for how Docker should build your image.","title":"Dockerfile"},{"location":"DevOps/docker-tidbits/#docker-platform","text":"It is Docker\u2019s software that provides the ability to package and run an application in a container on any Linux server. Docker Platform bundles code files and dependencies. It promotes easy scaling by enabling portability and reproducibility.","title":"Docker Platform"},{"location":"DevOps/docker-tidbits/#docker-engine","text":"It is the client-server application. Two parts: Community Edition (CE) which is open source. Enterprise Edition (EE which is enterprise oriented and comes with additional features and support.","title":"Docker Engine"},{"location":"DevOps/docker-tidbits/#docker-client","text":"It is the primary way to interact with Docker. Docker Client uses the Docker API to send the command from CLI to the Docker Daemon.","title":"Docker Client"},{"location":"DevOps/docker-tidbits/#docker-daemon","text":"It is the Docker server that listens for Docker API requests. The Docker Daemon manages images, containers, networks, and volumes.","title":"Docker Daemon"},{"location":"DevOps/docker-tidbits/#docker-volumes","text":"These are the best way to store persistent data that your apps consume and create.","title":"Docker Volumes"},{"location":"DevOps/docker-tidbits/#docker-registry","text":"It is the remote location where Docker Images are stored. You push images to a registry and pull images from a registry.","title":"Docker Registry"},{"location":"DevOps/docker-tidbits/#docker-hub","text":"It is the largest and default registry of Docker images.","title":"Docker Hub"},{"location":"DevOps/docker-tidbits/#docker-repository","text":"It is a collection of Docker images with the same name and different tags. The tag is the identifier for the image.","title":"Docker Repository"},{"location":"DevOps/docker-tidbits/#docker-networking","text":"It allows you to connect Docker containers together that are either on the same host or multiple hosts. A Docker container needs a host to run on. This can either be a physical machine or a VM, either on-prem or in the cloud. The host has the Docker daemon and client running which enables you to interact with a Docker registry on the one hand (to pull/push Docker images), and on the other hand, allows you to start, stop and inspect containers. For single host deployments, data can be exchanged via shared volumes . This introduces tight coupling and it will be harded to translate a single host deployment into a multihost deployment. The upside, though, is speed for shared volumes. In multihost deployments, we need to consider how are containers communicating within the host and how does communication paths look between different hosts. We need to take care of both performance and security aspects. Docker networking is the native container SDN solution: Bridge Mode (default) : Docker daemon creates docker0, a virtual Ethernet bridge that automatically forwards packets between any other network interfaces that are attached to it. Connects all containers on a host to this internal network through creating a pair of peer interfaces. Host Mode : Using host mode networking, the container effectively inherits the IP address from its host. This mode is faster than the bridge mode as there is no routing overhead, but it exposes the container directly to the public network and its security implications. Container mode : Docker will re-use the networking namespace of another container. The second container that started with --net=container:<first_container_name> has the same IP address as the first container. Kubernetes networking uses this mode. No Networking : This mode puts the container inside of its own network stack but doesn\u2019t configure it. For containers that don\u2019t need a network like batch jobs writing to a disk volume For containers that will be configured with custom networking. Note : By default, Docker has inter-container communication enabled [ --icc=true ]. Containers on a host can communicate without any restrictions which may lead to Denial of Service attacks.","title":"Docker Networking"},{"location":"DevOps/docker-tidbits/#docker-compose","text":"It is a tool to run apps that require multiple Docker containers and allows us to move commands into a docker-compose.yml file for reuse.","title":"Docker Compose"},{"location":"DevOps/docker-tidbits/#docker-services","text":"Services are really just \u201ccontainers in production.\u201d A service only runs one image, but it codifies the way that image runs \u2014 what ports it should use, how many replicas of the container should run so the service has the capacity it needs, and so on. Scaling a service changes the number of container instances running that piece of software, assigning more computing resources to the service in the process.","title":"Docker Services"},{"location":"DevOps/docker-tidbits/#dockerfile_1","text":"A container is built from a series of layers. Each layer is read only, except the final container layer that sits on top of the others. The Dockerfile tells Docker which layers to add and in which order to add them. When an image is pulled from a remote repository to a local machine only layers that are not already on the local machine are downloaded. Layers are created in the final image only with these commands. FROM RUN COPY ADD All other commands ( Dockerfile INSTRUCTIONS ) configure, or do other tasks that does effect the image directly. Here are the Dockerfile INSTRUCTIONS: FROM \u2014 specifies the base (parent) image. LABEL \u2014 provides metadata. Good place to include maintainer info. ENV \u2014 sets a persistent environment variable. RUN \u2014 runs a command and creates an image layer. Used to install packages into containers. COPY (recommended) \u2014 copies files and directories to the container. ADD \u2014 copies files and directories to the container. Can upack local .tar files. (ADD does the same thing as COPY, but has two more use cases. ADD can be used to move files from a remote URL to a container and ADD can extract local TAR files.) ADD instruction contains the line continuation character. CMD \u2014 provides a command and arguments for an executing container. Parameters can be overridden. There can be only one CMD. WORKDIR \u2014 sets the working directory for the instructions that follow. ARG \u2014 defines a variable to pass to Docker at build-time. ENTRYPOINT \u2014 provides command and arguments for an executing container. Arguments persist. EXPOSE \u2014 exposes a port. EXPOSE does not actually publish the port. It acts as a information relay betweenthe one building the image and the one who is running the container. Use docker run with the -p flag to publish and map one or more ports at run time. The uppercase -P flag will publish all exposed ports. VOLUME \u2014 creates a directory mount point to access and store persistent data. Cheatsheet Source cheatsheet FROM Usage: FROM FROM : FROM @ Information: FROM must be the first non-comment instruction in the Dockerfile. FROM can appear multiple times within a single Dockerfile in order to create multiple images. Simply make a note of the last image ID output by the commit before each new FROM command. The tag or digest values are optional. If you omit either of them, the builder assumes a latest by default. The builder returns an error if it cannot match the tag value. Reference - Best Practices MAINTAINER Usage: MAINTAINER The MAINTAINER instruction allows you to set the Author field of the generated images. Reference RUN Usage: RUN (shell form, the command is run in a shell, which by default is /bin/sh -c on Linux or cmd /S /C on Windows) RUN [\" \", \" \", \" \"] (exec form) Information: The exec form makes it possible to avoid shell string munging, and to RUN commands using a base image that does not contain the specified shell executable. The default shell for the shell form can be changed using the SHELL command. Normal shell processing does not occur when using the exec form. For example, RUN [\"echo\", \"$HOME\"] will not do variable substitution on $HOME. Reference - Best Practices CMD Usage: CMD [\" \",\" \",\" \"] (exec form, this is the preferred form) CMD [\" \",\" \"] (as default parameters to ENTRYPOINT) CMD (shell form) Information: The main purpose of a CMD is to provide defaults for an executing container. These defaults can include an executable, or they can omit the executable, in which case you must specify an ENTRYPOINT instruction as well. There can only be one CMD instruction in a Dockerfile. If you list more than one CMD then only the last CMD will take effect. If CMD is used to provide default arguments for the ENTRYPOINT instruction, both the CMD and ENTRYPOINT instructions should be specified with the JSON array format. If the user specifies arguments to docker run then they will override the default specified in CMD. Normal shell processing does not occur when using the exec form. For example, CMD [\"echo\", \"$HOME\"] will not do variable substitution on $HOME. Reference - Best Practices LABEL Usage: LABEL = [ = ...] Information: The LABEL instruction adds metadata to an image. To include spaces within a LABEL value, use quotes and backslashes as you would in command-line parsing. Labels are additive including LABELs in FROM images. If Docker encounters a label/key that already exists, the new value overrides any previous labels with identical keys. To view an image\u2019s labels, use the docker inspect command. They will be under the \"Labels\" JSON attribute. Reference - Best Practices EXPOSE Usage: EXPOSE [ ...] Information: Informs Docker that the container listens on the specified network port(s) at runtime. EXPOSE does not make the ports of the container accessible to the host. Reference - Best Practices ENV Usage: ENV ENV = [ = ...] Information: The ENV instruction sets the environment variable to the value . The value will be in the environment of all \u201cdescendant\u201d Dockerfile commands and can be replaced inline as well. The environment variables set using ENV will persist when a container is run from the resulting image. The first form will set a single variable to a value with the entire string after the first space being treated as the - including characters such as spaces and quotes. Reference - Best Practices ADD Usage: ADD [ ...] ADD [\" \", ... \" \"] (this form is required for paths containing whitespace) Information: Copies new files, directories, or remote file URLs from and adds them to the filesystem of the image at the path . may contain wildcards and matching will be done using Go\u2019s filepath.Match rules. If is a file or directory, then they must be relative to the source directory that is being built (the context of the build). is an absolute path, or a path relative to WORKDIR. If doesn\u2019t exist, it is created along with all missing directories in its path. Reference - Best Practices COPY Usage: COPY [ ...] COPY [\" \", ... \" \"] (this form is required for paths containing whitespace) Information: Copies new files or directories from and adds them to the filesystem of the image at the path . may contain wildcards and matching will be done using Go\u2019s filepath.Match rules. must be relative to the source directory that is being built (the context of the build). is an absolute path, or a path relative to WORKDIR. If doesn\u2019t exist, it is created along with all missing directories in its path. Reference - Best Practices ENTRYPOINT Usage: ENTRYPOINT [\" \", \" \", \" \"] (exec form, preferred) ENTRYPOINT (shell form) Information: Allows you to configure a container that will run as an executable. Command line arguments to docker run will be appended after all elements in an exec form ENTRYPOINT and will override all elements specified using CMD. The shell form prevents any CMD or run command line arguments from being used, but the ENTRYPOINT will start via the shell. This means the executable will not be PID 1 nor will it receive UNIX signals. Prepend exec to get around this drawback. Only the last ENTRYPOINT instruction in the Dockerfile will have an effect. Reference - Best Practices VOLUME Usage: VOLUME [\" \", ...] VOLUME [ ...] Creates a mount point with the specified name and marks it as holding externally mounted volumes from native host or other containers. Reference - Best Practices USER Usage: USER The USER instruction sets the user name or UID to use when running the image and for any RUN, CMD and ENTRYPOINT instructions that follow it in the Dockerfile. Reference - Best Practices WORKDIR Usage: WORKDIR Information: Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow it. It can be used multiple times in the one Dockerfile. If a relative path is provided, it will be relative to the path of the previous WORKDIR instruction. Reference - Best Practices ARG Usage: ARG [= ] Information: Defines a variable that users can pass at build-time to the builder with the docker build command using the --build-arg = flag. Multiple variables may be defined by specifying ARG multiple times. It is not recommended to use build-time variables for passing secrets like github keys, user credentials, etc. Build-time variable values are visible to any user of the image with the docker history command. Environment variables defined using the ENV instruction always override an ARG instruction of the same name. Docker has a set of predefined ARG variables that you can use without a corresponding ARG instruction in the Dockerfile. HTTP_PROXY and http_proxy HTTPS_PROXY and https_proxy FTP_PROXY and ftp_proxy NO_PROXY and no_proxy Reference ONBUILD Usage: ONBUILD Information: Adds to the image a trigger instruction to be executed at a later time, when the image is used as the base for another build. The trigger will be executed in the context of the downstream build, as if it had been inserted immediately after the FROM instruction in the downstream Dockerfile. Any build instruction can be registered as a trigger. Triggers are inherited by the \"child\" build only. In other words, they are not inherited by \"grand-children\" builds. The ONBUILD instruction may not trigger FROM, MAINTAINER, or ONBUILD instructions. Reference - Best Practices STOPSIGNAL Usage: STOPSIGNAL The STOPSIGNAL instruction sets the system call signal that will be sent to the container to exit. This signal can be a valid unsigned number that matches a position in the kernel\u2019s syscall table, for instance 9, or a signal name in the format SIGNAME, for instance SIGKILL. Reference HEALTHCHECK Usage: HEALTHCHECK [ ] CMD (check container health by running a command inside the container) HEALTHCHECK NONE (disable any healthcheck inherited from the base image) Information: Tells Docker how to test a container to check that it is still working Whenever a health check passes, it becomes healthy. After a certain number of consecutive failures, it becomes unhealthy. The that can appear are... --interval= (default: 30s) --timeout= (default: 30s) --retries= (default: 3) The health check will first run interval seconds after the container is started, and then again interval seconds after each previous check completes. If a single run of the check takes longer than timeout seconds then the check is considered to have failed. It takes retries consecutive failures of the health check for the container to be considered unhealthy. There can only be one HEALTHCHECK instruction in a Dockerfile. If you list more than one then only the last HEALTHCHECK will take effect. can be either a shell command or an exec JSON array. The command's exit status indicates the health status of the container. 0: success - the container is healthy and ready for use 1: unhealthy - the container is not working correctly 2: reserved - do not use this exit code The first 4096 bytes of stdout and stderr from the are stored and can be queried with docker inspect. When the health status of a container changes, a health_status event is generated with the new status. Reference SHELL Usage: SHELL [\" \", \" \", \" \"] Information: Allows the default shell used for the shell form of commands to be overridden. Each SHELL instruction overrides all previous SHELL instructions, and affects all subsequent instructions. Allows an alternate shell be used such as zsh, csh, tcsh, powershell, and others.","title":"Dockerfile"},{"location":"DevOps/docker-tidbits/#smaller-docker-images","text":"To help make the images as small as possible, we can use .dockerignore to exclude files that we don't need. It helps in: Excluding secrets. Reduce image size with fewer not-needed files. Reduce build cache invalidation due to log files changing. Best Practices to Reduce Image Sizes & Build Times: Use an official base image whenever possible Use variations of Alpine images when possible to keep your images lightweight. If using apt, combine RUN apt-get update with apt-get install in the same instruction to reduce number of layers to be built. Then chain multiple packages in that instruction. List the packages in alphabetical order over multiple lines with the character. For example: RUN apt-get update && apt-get install -y \\ package-one \\ package-two && rm -rf /var/lib/apt/lists/* Include && rm -rf /var/lib/apt/lists/* at the end of the RUN instruction to clean up the apt cache so it isn\u2019t stored in the layer. Use caching wisely by putting instructions likely to change lower in your Dockerfile. Use a .dockerignore file to keep unwanted and unnecessary files out of your image.","title":"Smaller Docker Images"},{"location":"DevOps/docker-tidbits/#cli","text":"docker container my_command create \u2014 Create a container from an image. start \u2014 Start an existing container. run \u2014 Create a new container and start it. ls \u2014 List running containers. inspect \u2014 See lots of info about a container. logs \u2014 Print logs. stop \u2014 Gracefully stop running container. kill \u2014Stop main process in container abruptly. rm\u2014 Delete a stopped container. docker image my_command build \u2014 Build an image. push \u2014 Push an image to a remote registry. ls \u2014 List images. history \u2014 See intermediate image info. inspect \u2014 See lots of info about an image, including the layers. rm \u2014 Delete an image. docker version \u2014 List info about your Docker Client and Server versions. docker login \u2014 Log in to a Docker registry. docker system prune \u2014 Delete all unused containers, unused networks, and dangling images. docker container create my_repo/my_image:my_tag - Create a container from an image. docker container create -a STDIN my_image -a is short for --attach. Attach the container to STDIN, STDOUT or STDERR. docker container start my_container \u2014 Start an existing container. docker container run my_image \u2014 Create a new container and start it. docker container run -i -t -p 1000:8000 --rm my_image -i is short for --interactive. Keep STDIN open even if unattached. -t is short for--tty. Allocates a pseudo terminal that connects your terminal with the container\u2019s STDIN and STDOUT. --rm Automatically delete the container when it stops running. docker container inspect my_container \u2014 See lots of info about a container. docker container logs my_container \u2014 Print a container\u2019s logs. docker container kill $(docker ps -q) \u2014 Kill all running containers. docker image build -t my_repo/my_image:my_tag . - Build a Docker image named my_image from the Dockerfile located at the specified path or URL. docker image push my_repo/my_image:my_tag \u2014 Push an image to a registry. docker volume create docker volume ls docker volume inspect docker volume rm docker volume prune Common options for the --mount flag in docker run --mount my_options my_image : type=volume source=volume_name destination=/path/in/container readonly","title":"CLI"},{"location":"DevOps/git-commands-to-know/","text":"Git commands to know \u00b6 Inspection \u00b6 git diff See all file changes locally. A file name can be appended to show changes for only one file. git log See all commit history. git log -p my_file This can be used for a file. git blame my_file See who changed what and when in my_file. git reflog Show a log of changes to the local repository\u2019s HEAD. Good for finding lost work. Undoing \u00b6 git reset It can be used on both commits and individual files . It is used when working locally and commits have not been merged into collaborative remote work. git reset --hard HEAD -- Discard staged and unstaged changes since the most recent commit. Specify a different commit instead of HEAD to discard changes since that commit. --hard specifies that both the staged and unstaged changes are discarded. git checkout It can be used on both commits and individual files . It is used when working locally and commits have not been merged into collaborative remote work. git checkout my_commit -- Discard unstaged changes since my_commit . HEAD is often used for my_commit to discard changes to your local working directory since the most recent commit. git checkout is best used for local-only undo. It doesn\u2019t mess up the commit history from a remote branch that your collaborators are depending upon. If you use git checkout with a branch instead of a commit, HEAD is switched to the specified branch and the working directory is updated to match. This is the more common use of the checkout command. git revert It is used only at the commit level. It is used when working collaboratively and need to neutralize a commit in the remote branch. git revert my_commit -- Undo the effects of changes in my_commit . It makes a new commit when it undo the changes. This is safe for collaborative projects because it doesn\u2019t overwrite history that other users\u2019 branches might depend upon. git clean Delete untracked files in the local working directory. -n flag is for a dry run where nothing is deleted. -f flag is to actually remove the files. -d flag is to remove untracked directories. Tidying things \u00b6 git commit --amend Add your staged changes to the most recent commit. If nothing is staged, this command just allows you to edit the most recent commit message. Only use this command if the commit has not been integrated into the remote master branch. git push my_remote --tags Send all local tags to the remote repo. Good for versioning changes.","title":"Git commands to know"},{"location":"DevOps/git-commands-to-know/#git-commands-to-know","text":"","title":"Git commands to know"},{"location":"DevOps/git-commands-to-know/#inspection","text":"git diff See all file changes locally. A file name can be appended to show changes for only one file. git log See all commit history. git log -p my_file This can be used for a file. git blame my_file See who changed what and when in my_file. git reflog Show a log of changes to the local repository\u2019s HEAD. Good for finding lost work.","title":"Inspection"},{"location":"DevOps/git-commands-to-know/#undoing","text":"git reset It can be used on both commits and individual files . It is used when working locally and commits have not been merged into collaborative remote work. git reset --hard HEAD -- Discard staged and unstaged changes since the most recent commit. Specify a different commit instead of HEAD to discard changes since that commit. --hard specifies that both the staged and unstaged changes are discarded. git checkout It can be used on both commits and individual files . It is used when working locally and commits have not been merged into collaborative remote work. git checkout my_commit -- Discard unstaged changes since my_commit . HEAD is often used for my_commit to discard changes to your local working directory since the most recent commit. git checkout is best used for local-only undo. It doesn\u2019t mess up the commit history from a remote branch that your collaborators are depending upon. If you use git checkout with a branch instead of a commit, HEAD is switched to the specified branch and the working directory is updated to match. This is the more common use of the checkout command. git revert It is used only at the commit level. It is used when working collaboratively and need to neutralize a commit in the remote branch. git revert my_commit -- Undo the effects of changes in my_commit . It makes a new commit when it undo the changes. This is safe for collaborative projects because it doesn\u2019t overwrite history that other users\u2019 branches might depend upon. git clean Delete untracked files in the local working directory. -n flag is for a dry run where nothing is deleted. -f flag is to actually remove the files. -d flag is to remove untracked directories.","title":"Undoing"},{"location":"DevOps/git-commands-to-know/#tidying-things","text":"git commit --amend Add your staged changes to the most recent commit. If nothing is staged, this command just allows you to edit the most recent commit message. Only use this command if the commit has not been integrated into the remote master branch. git push my_remote --tags Send all local tags to the remote repo. Good for versioning changes.","title":"Tidying things"},{"location":"DevOps/git-process/","text":"Git Process \u00b6 Initializing a repo and committing files \u00b6 To store a directory under version control you need to create a repository. With Git you initialise a repository in the top-level directory for a project. As this is a new project, a new repository needs to be created. Use the git init command to create a repository. When a directory is part of a repository it is called a Working Directory. A working directory contains the latest downloaded version from the repository together with any changes that have yet to be committed. You can view which files have changed between your working directory and what's been previously committed to the repository using the command git status . To save, or commit, files into your Git repository you first need to add them to the staging area. Git has three areas, - a working directory - a staging area - the repository itself. Use the command git add <file|directory> to add a file or folder to the staging area. Once a file has been added to the staging area it needs to be committed to the repository. Use git commit -m \"<commit message>\" to commit the staged file. Each commit is assigned a SHA-1 hash which enables you to refer back to the commit in other commands. The .gitignore should be committed to the repository to ensure the rules apply across different machines. Pradeeps-MBP:learning-git pradeepgorthi$ git init Initialized empty Git repository in /Users/pradeepgorthi/Documents/github-repos/learning-git/.git/ Pradeeps-MBP:learning-git pradeepgorthi$ git status On branch master No commits yet Untracked files: ( use \"git add <file>...\" to include in what will be committed ) Notes.md nothing added to commit but untracked files present ( use \"git add\" to track ) Pradeeps-MBP:learning-git pradeepgorthi$ git add Notes.md Pradeeps-MBP:learning-git pradeepgorthi$ git status On branch master No commits yet Changes to be committed: ( use \"git rm --cached <file>...\" to unstage ) new file: Notes.md Pradeeps-MBP:learning-git pradeepgorthi$ git commit -m \"learning git initial commit with commit notes\" [ master ( root-commit ) 5e7bc28 ] learning git initial commit with commit notes 1 file changed, 28 insertions ( + ) create mode 100644 Notes.md Pradeeps-MBP:learning-git pradeepgorthi$ git status On branch master nothing to commit, working tree clean Committing Changes \u00b6 As discussed in the previous scenario git status allows us to view the changes in the working directory and staging area compared to the repository. The command git diff enables you to compare changes in the working directory against a previously committed version. By default the command compares the working directory and the HEAD commit. If you wish to compare against an older version then provide the commit hash as a parameter, for example git diff <commit> . Comparing against commits will output the changes for all of the files modified. If you want to compare the changes to a single file then provide the name as an argument such as git diff committed.js > git diff diff --git a/committed.js b/committed.js index 12e7e7c..fc77969 100644 --- a/committed.js +++ b/committed.js @@ -1 +1 @@ -console.log ( \"Committed File\" ) +console.log ( \"Demostrating changing a committed file\" ) As in the previous scenario in order to commit a change it must first be staged using git add command. If you rename or delete files then you need to specify these files in the add command for them to be tracked. Alternatives you can use git mv and git rm for git to perform the action and include update the staging area. Once the changes are in the staging area they will not show in the output from git diff . By default, it will only compare the working directory and not the staging area. To compare the changes in the staging area against the previous commit, provide the staged parameter git diff --staged to ensure that you have correctly staged all your changes. > git diff --staged diff --git a/committed.js b/committed.js index 12e7e7c..fc77969 100644 --- a/committed.js +++ b/committed.js @@ -1 +1 @@ -console.log ( \"Committed File\" ) +console.log ( \"Demostrating changing a committed file\" ) diff --git a/untracked.js b/untracked.js new file mode 100644 index 0000000 ..264a5be --- /dev/null +++ b/untracked.js @@ -0,0 +1 @@ +console.log ( \"Untracked File\" ) The command git log allows you to view the history of the repository and the commit log. > git log commit 8e406ea6214f7c89cd732ed8ee43b4e4ee121e24 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Wed Sep 18 15 :44:53 2019 +0000 Changed the output message in committed.js commit 44a3b80bfcdf8a18688cad2149ca8494f7da91a1 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Wed Sep 18 15 :19:35 2019 +0000 Initial Commit The format of the log output is very flexible. > git log --pretty = format: \"%h %an %ar - %s\" 8e406ea Scrapbook Git Tutorial 2 minutes ago - Changed the output message in committed.js 44a3b80 Scrapbook Git Tutorial 27 minutes ago - Initial Commit While git log tells you the commit author and message, to view the changes made in the commit you need to use the the command git show . Use git show <commit-hash> to view older changes. > git show commit 8e406ea6214f7c89cd732ed8ee43b4e4ee121e24 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Wed Sep 18 15 :44:53 2019 +0000 Changed the output message in committed.js diff --git a/committed.js b/committed.js index 12e7e7c..fc77969 100644 --- a/committed.js +++ b/committed.js @@ -1 +1 @@ -console.log ( \"Committed File\" ) +console.log ( \"Demostrating changing a committed file\" ) diff --git a/untracked.js b/untracked.js new file mode 100644 index 0000000 ..264a5be --- /dev/null +++ b/untracked.js @@ -0,0 +1 @@ +console.log ( \"Untracked File\" ) Here is an interesting image to refer to when using git. Working Remotely \u00b6 Remote repositories allow you to share changes from or to your repository. Remote locations are generally a build server, a team members machine or a centralised store such as Github.com. Remotes are added using the git remote command with a friendly name and the remote location, typically a HTTPS URL or a SSH connection. > git remote add test-name /s/remote-project/1 usage: git remote [ -v | --verbose ] or: git remote add [ -t <branch> ] [ -m <master> ] [ -f ] [ --tags | --no-tags ] [ --mirror = <fetch | push> ] <name> <url> or: git remote rename <old> <new> or: git remote remove <name> or: git remote set-head <name> ( -a | --auto | -d | --delete | <branch> ) or: git remote [ -v | --verbose ] show [ -n ] <name> or: git remote prune [ -n | --dry-run ] <name> or: git remote [ -v | --verbose ] update [ -p | --prune ] [( <group> | <remote> ) ... ] or: git remote set-branches [ --add ] <name> <branch>... or: git remote set-url [ --push ] <name> <newurl> [ <oldurl> ] or: git remote set-url --add <name> <newurl> or: git remote set-url --delete <name> <url> -v, --verbose be verbose ; must be placed before a subcommand If you use git clone , then the location being cloned will be automatically added as a remote with the name origin. Pradeeps-MBP:learning-git pradeepgorthi$ git remote add origin https://github.com/deepgorthi/learning-git.git When ready to share the commits, you need to push them to a remote repository via git push and it is followed by two parameters. The first parameter is the friendly name of the remote repository we defined in the first step. The second parameter is the name of the branch. By default all git repositories have a master branch where the code is worked on. Pradeeps-MBP:learning-git pradeepgorthi$ git push -u origin master Username for 'https://github.com' : deepgorthi Password for 'https://deepgorthi@github.com' : Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 6 /6 ) , 1 .62 KiB | 1 .62 MiB/s, done . Total 6 ( delta 1 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 1 /1 ) , done . To https://github.com/deepgorthi/learning-git.git * [ new branch ] master -> master Branch 'master' set up to track remote branch 'master' from 'origin' . A typical Git workflow would be to perform multiple small commits as you complete a task and push to a remote at relevant points, such as when the task is complete, to ensure synchronisation of the code within the team. git pull allows you to sync changes from a remote repository into your local version. Use the command git log --grep=\"#1234\" to find all the commits containing #1234 The command git pull is a combination of two different commands, git pull => git fetch + git merge Fetch downloads the changes from the remote repository into a separate branch named remotes/<remote-name>/<remote-branch-name> . The branch can be accessed using git checkout . Using git fetch is a great way to review the changes without affecting your current branch. The naming format of branches is flexible enough that you can have multiple remotes and branches with the same name and easily switch between them. > git merge remotes/<remote-name>/<remote-branch-name> master You can view a list of all the remote branches using the command > git branch -r Undoing Changes \u00b6 The command git checkout will replace everything in the working directory to the last committed version. If you want to replace all files then use a dot (.) to indicate the current directory, otherwise list the directories/files separated by spaces. If you're in the middle of a commit and have added files to the staging area but then changed your mind then you'll need to use the git reset command. It will move files back from the staging area to the working directory. If you want to reset all files then use a \".\" to indicate current directory, otherwise list the files separated by spaces. $ git add Notes.md $ git add gitimage.jpg $ git reset Notes.md Unstaged changes after reset: M Notes.md $ git commit -m \"Adding git image\" [ master 8eba1eb ] Adding git image 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) create mode 100644 gitimage.jpg $ git push origin master Enumerating objects: 4 , done . Counting objects: 100 % ( 4 /4 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 3 /3 ) , done . Writing objects: 100 % ( 3 /3 ) , 25 .23 KiB | 8 .41 MiB/s, done . Total 3 ( delta 0 ) , reused 0 ( delta 0 ) To https://github.com/deepgorthi/learning-git.git a0d745f..8eba1eb master -> master git reset --hard => git reset + git checkout The result of git reset --hard will be files removed from staging area and the working directory is taken back to the state of the last commit. Using HEAD will clear the state back to the last commit and using git reset --hard <commit-hash> allows you to go back to any commit state. HEAD is an alias for the last commit-hash of the branch. If you have already committed files but realized you made a mistake then the command git revert allows you to undo the commits. The command will create a new commit which has the inverse affect of the commit being reverted. If you haven't pushed your changes then git reset HEAD~1 has the same affect and will remove the last commit. To revert multiple commits at once we use the character ~ to mean minus. For example, HEAD~2 is two commits from the head. This can be combined with the characters ... to say between two commits. Use the command git revert HEAD...HEAD~2 to revert the commits between HEAD and HEAD~2. The command git log --oneline for a quick overview of the commit history. Fixing Merge Conflicts \u00b6 The git fetch command downloads changes into a separate branch which can be checked out and merge. During a merge, Git will attempt to automatically combine the commits. When no conflicts exist then the merge will be 'fast-forwarded' and you won't have to do anything. If a conflict does exist, then you will retrieve an error and the repository will be in a merging state. > git fetch > git merge origin/master Auto-merging staging.txt CONFLICT ( add/add ) : Merge conflict in staging.txt Automatic merge failed ; fix conflicts and then commit the result. When a conflict occurs the changes from both the local and remote will appear in the same file in the unix diff format. To read the format, the local changes will appear at the top between <<<<<<< HEAD and ======= with the remote changes being underneath between ======= and >>>>>>> remotes/origin/master . <<<<<<< HEAD Fixing Error, Let's Hope No-One Else Does ======= Fixing Previous Error >>>>>>> origin/master The simplest way to fix a conflict is to pick either the local or remote version using git checkout --ours staging.txt or git checkout --theirs staging.txt . If you need to have more control then you can manually edit the file(s) like normal. > git checkout --theirs staging.txt > git add staging.txt > git commit [ master 51ae805 ] Merge remote-tracking branch 'origin/master' If you want to revert in the middle of a merge and try again then use the command git reset --hard HEAD; to go back to your previous state. Use git commit --no-edit when you wish to use the default commit message. To simulate a non-fast forward merge the following has occurred. 1) Developer A pulls the latest changes from Developer B. 2) Developer B commits changes to their local repository. 3) Developer A commits non-conflicting changes to their local repository. 4) Developer A pulls the latest changes from Developer B. In this scenario Git is unable to fast-forward the changes from Developer B because Developer A has made a number of changes. When this happens, Git will attempt to auto-merge the changes. If no conflicts exist then the merge will be completed and a new commit will be created to indicate the merge happening at that point in time. The default commit message for merges is \"Merge branch '' of \". These commits can be useful to indicate synchronization points between repositories but also produce a noisy commit log. > git pull --no-edit origin master remote: Counting objects: 5 , done . remote: Compressing objects: 100 % ( 4 /4 ) , done . remote: Total 4 ( delta 1 ) , reused 0 ( delta 0 ) Unpacking objects: 100 % ( 4 /4 ) , done . From /s/remote-project/1 * branch master -> FETCH_HEAD 418fe6e..095b88a master -> origin/master Merge made by the 'recursive' strategy. new-file-6.txt | 1 + new-file-6a.txt | 1 + 2 files changed, 2 insertions ( + ) create mode 100644 new-file-6.txt create mode 100644 new-file-6a.txt Git Rebase The merge commit messages can be useful to indicate synchronization points but they can also produce a lot of noise. For example, if you're working against local branches and haven't pushed then this additional information is meaningless, and confusing, to other developers looking at the repository. To solve this you can use git rebase instead of git merge. A rebase will unwind the changes you've made and replay the changes in the branch, applying your changes as if they happened all on the same branch. The result is a clean history and graph for the merge. From git-scm : - The easiest way to integrate the branches, as we\u2019ve already covered, is the merge command. It performs a three-way merge between the two latest branch snapshots (C3 and C4) and the most recent common ancestor of the two (C2), creating a new snapshot (and commit). - However, there is another way: you can take the patch of the change that was introduced in C4 and reapply it on top of C3. In Git, this is called rebasing. With the rebase command, you can take all the changes that were committed on one branch and replay them on a different branch. Important: As rebase will replay the changes instead of merging, each commit will have a new hash id. If you, or other developers, have pushed/pulled the repository then changing the history can git to lose commits. As such you shouldn't rebase commits that have been made public, for example pushing commits then rebasing in older commits from a different branch. The result will be previously public commits having different hash ids. More details can be found at The Perils of Rebasing . This approach also applies when working with remote branches and can be applied when issuing a pull request using: git pull --rebase > git pull --rebase There is no tracking information for the current branch. Please specify which branch you want to rebase against. See git-pull ( 1 ) for details git pull <remote> <branch> If you wish to set tracking information for this branch you can do so with: git branch --set-upstream-to = origin/<branch> feature/10 This will act as if you had done a pull request before each of your commits. In general, the way to get the best of both worlds is to rebase local changes you\u2019ve made but haven\u2019t shared yet before you push them in order to clean up your story, but never rebase anything you\u2019ve pushed somewhere. Git Branches \u00b6 A branch allows you to work in a brand new working directory efficiently. The result is that a single Git repository can have multiple different versions of the code-base, each of which can be swapped between without changing directories. The default branch in Git is called master. When you switch a branch, Git changes the contents of the working directory. This means you don't need to change any configurations or settings to reflect different branches or locations. Branches are created based on another branch, generally master. The command git branch <new branch name> <starting branch> takes an existing branch and creates a separate branch to work in. At this point, both branches are identical. To switch to a branch you use the git checkout <new branch name> command. The above two commands can be combined into a single command by doing git checkout -b <new branch name> which will create and checkout the newly created branch and the starting branch defaults to HEAD . > git branch new_branch master > git checkout new_branch Switched to branch 'new_branch' To list all the branches, use the command git branch . The additional argument -a will include remote branches while including -v will include the HEAD commit message for the branch. > git branch -va master 75fa002 First Commit on master new_branch 75fa002 First Commit on master * new_branch_b 1296799 Commit on branch A commit has been made to the new branch. To merge this into master, you would first need to checkout the target branch, in this case master, git checkout master and then use git merge <branch-name> command to merge in the commits from a branch. > git status On branch new_branch nothing to commit, working directory clean > touch test_file > git add test_file > git status On branch new_branch Changes to be committed: ( use \"git reset HEAD <file>...\" to unstage ) new file: test_file > git commit -m \"Adding test file to new_branch\" [ new_branch 0fceb73 ] Adding test file to new_branch 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) create mode 100644 test_file > git checkout master Switched to branch 'master' > git merge new_branch Updating 75fa002..0fceb73 Fast-forward test_file | 0 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) create mode 100644 test_file To push a branch to a remote, use git push <remote_name> <branch_name> > git push origin new_branch Counting objects: 6 , done . Delta compression using up to 12 threads. Compressing objects: 100 % ( 3 /3 ) , done . Writing objects: 100 % ( 6 /6 ) , 524 bytes | 0 bytes/s, done . Total 6 ( delta 0 ) , reused 0 ( delta 0 ) To /s/remote-project/1 * [ new branch ] new_branch -> new_branch Cleaning up branches is important to remove the amount of noise and confusion. To delete a branch, you need to provide the argument -d like git branch -d <branch_name> > git branch -d new_branch Deleted branch new_branch ( was 0fceb73 ) . Finding Bugs \u00b6 git diff This command is the simplest to compare what's changed between commits. It will output the differences between the two commits. > git diff HEAD~2 HEAD diff --git a/list.html b/list.html index 96e99d0..9f53aec 100644 --- a/list.html +++ b/list.html @@ -2,4 +2,6 @@ <li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> +<li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> +<li>Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat.</li> </ul> git log While git log helps you see the commit messages but by default it does not output what actually changed. git log --oneline To see the overview of the commits in a short view, use this command. > git log --oneline 43a9071 Final Item 49f29f0 New Item 3c2ef9c Initial commit of the list git log -p To output the commit information with the differences of what changed you need to include the -p prompt > git log -p commit 43a9071cd9f4e4e17dbd1f7ee50020373a04dd6f Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Final Item diff --git a/list.html b/list.html index def310d..9f53aec 100644 --- a/list.html +++ b/list.html @@ -3,4 +3,5 @@ <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> <li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> +<li>Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat.</li> </ul> commit 49f29f018078ec090b466edea6d57ba980e06bd3 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 New Item diff --git a/list.html b/list.html index 96e99d0..def310d 100644 --- a/list.html +++ b/list.html @@ -2,4 +2,5 @@ <li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> +<li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> </ul> commit 3c2ef9c7048a9e98af1679e2b7349cbd53ec17c9 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Initial commit of the list diff --git a/list.html b/list.html new file mode 100644 index 0000000 ..96e99d0 --- /dev/null +++ b/list.html @@ -0,0 +1,5 @@ +<ul> +<li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> +<li>Aliquam tincidunt mauris eu risus.</li> +<li>Vestibulum auctor dapibus neque.</li> +</ul> git log -p -n 2 This will output the entire history. You can filter it with a number of different options. The -n specifies a limit of commits to display from the HEAD. Using '2' displays HEAD and HEAD~1. > git log -p -n 2 commit 43a9071cd9f4e4e17dbd1f7ee50020373a04dd6f Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Final Item diff --git a/list.html b/list.html index def310d..9f53aec 100644 --- a/list.html +++ b/list.html @@ -3,4 +3,5 @@ <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> <li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> +<li>Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat.</li> </ul> commit 49f29f018078ec090b466edea6d57ba980e06bd3 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 New Item diff --git a/list.html b/list.html index 96e99d0..def310d 100644 --- a/list.html +++ b/list.html @@ -2,4 +2,5 @@ <li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> +<li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> </ul> git log --grep=\"Initial\" This will output all the commits which include the word \"Initial\" in their commit message. This is useful if you tag commits with bug-tracking numbers. > git log --grep = \"Initial\" commit 3c2ef9c7048a9e98af1679e2b7349cbd53ec17c9 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Initial commit of the list git bisect This command allows you to do a binary search of the repository looking for which commit introduced the problem and the regression. Git bisect takes a number of steps, execute the steps in order to see the results. Steps To enter into bisect mode you use the command git bisect start . Once in bisect mode, define your current checkout as bad using git bisect bad . This indicates that it contains the problem you are searching to see when it was introduced. We have defined where a bad commit happened. Now we need to define when the last known good commit was using git bisect good HEAD~5 . In this case it was five commits ago. The above step will checkout the commit in-between bad and good commits. You can then check the commit, run tests etc to see if the bug exists. In this example, you can check the contents using `cat list.html This commit looks good as everything has correct HTML tags. We tell Git we're happy using git bisect good . This will automatically check out the commit in the middle of the last known good commit, as defined in step 5 and our bad commit. As we did before, we need to check to see if the commit is good or bad using cat list.html This commit has missing HTML tags. Using git bisect bad will end the search and output the the related commit id. The result is that instead of searching five commits, we only searched two. On a much larger timescale, bisect can save you signifant time. git blame It can be useful to know who worked on a certain section of the file to help with improvements in future. This command shows the revision and author who last modified each line of a file. If we know the lines which we're concerned with then we can use the -L parameter to provide a range of lines to output. Being picky with Git \u00b6 Cherry Picking In this scenario, we only care about changes to one of the files but if we merged the branch, then we'd merge all five commits and the unwanted changes. To merge individual commits, we use git cherry-pick <hash-id|ref> command. This behaves in a similar way to merge , if no conflicts exist then the commit will be automatically merged. new_branch~3 refers to the second-to-last commit in the branch. > git log --pretty = oneline --reverse new_branch f3a03a060cd76c4ebd68ba2a35ad9685f041a9b6 Readme File 2be62bb15c92ad4dfc57b62592df09907afc2c5a Initial commit, no items ac4d5aec7ee23de2b6b6756a598dbd5f59f65ac5 Initial list 0e2dc8c0428a3fd5e42af547a97c5eb9828b3c4b Creating Second List 4b4bcb18289c3adf476a97c328caaacfc0f4a9a8 Adding final items to the list > git cherry-pick 2be62bb15c92ad4dfc57b62592df09907afc2c5a [ master 902442e ] Initial commit, no items 1 file changed, 2 insertions ( + ) create mode 100644 list.html > git cherry-pick ac4d5aec7ee23de2b6b6756a598dbd5f59f65ac5 [ master 903c134 ] Initial list 1 file changed, 3 insertions ( + ) > git cherry-pick 4b4bcb18289c3adf476a97c328caaacfc0f4a9a8 [ master e91edc3 ] Adding final items to the list 1 file changed, 1 insertion ( + ) Conflicts can be solved in the same way as with merging a branch - either manually fixing the files or selecting theirs or ours via git checkout . You can stop and revert the pick using git cherry-pick --abort Once the conflicts have been resolved, you can continue with the cherry pick using the command git cherry-pick --continue Re-writing History in Git \u00b6 Re-writing the repositories history is done using git rebase -interactive . This has a total of 6 commands that can be used. > git rebase --interactive --root # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message # x, exec = run command (the rest of the line) using shell This will open up a file that can be edited in vim. If we want to correct a type in the commit message, we can change pick to reword and that will result in a new vim file opening. Now, we can change the commit message typo and then save it. > git rebase --interactive --root [ detached HEAD 9eebea9 ] Initial commit of the list 1 file changed, 5 insertions ( + ) create mode 100644 list.html Successfully rebased and updated refs/heads/master git commit --amend can be used to reword the last commit message. If we want to re-order our last two commits, that can be done using HEAD~2 that allows us to modify them. git rebase --interactive HEAD~2 -> Using Vim, simply reorder the lines, save & quit, and the commits will match the order.","title":"Git Process"},{"location":"DevOps/git-process/#git-process","text":"","title":"Git Process"},{"location":"DevOps/git-process/#initializing-a-repo-and-committing-files","text":"To store a directory under version control you need to create a repository. With Git you initialise a repository in the top-level directory for a project. As this is a new project, a new repository needs to be created. Use the git init command to create a repository. When a directory is part of a repository it is called a Working Directory. A working directory contains the latest downloaded version from the repository together with any changes that have yet to be committed. You can view which files have changed between your working directory and what's been previously committed to the repository using the command git status . To save, or commit, files into your Git repository you first need to add them to the staging area. Git has three areas, - a working directory - a staging area - the repository itself. Use the command git add <file|directory> to add a file or folder to the staging area. Once a file has been added to the staging area it needs to be committed to the repository. Use git commit -m \"<commit message>\" to commit the staged file. Each commit is assigned a SHA-1 hash which enables you to refer back to the commit in other commands. The .gitignore should be committed to the repository to ensure the rules apply across different machines. Pradeeps-MBP:learning-git pradeepgorthi$ git init Initialized empty Git repository in /Users/pradeepgorthi/Documents/github-repos/learning-git/.git/ Pradeeps-MBP:learning-git pradeepgorthi$ git status On branch master No commits yet Untracked files: ( use \"git add <file>...\" to include in what will be committed ) Notes.md nothing added to commit but untracked files present ( use \"git add\" to track ) Pradeeps-MBP:learning-git pradeepgorthi$ git add Notes.md Pradeeps-MBP:learning-git pradeepgorthi$ git status On branch master No commits yet Changes to be committed: ( use \"git rm --cached <file>...\" to unstage ) new file: Notes.md Pradeeps-MBP:learning-git pradeepgorthi$ git commit -m \"learning git initial commit with commit notes\" [ master ( root-commit ) 5e7bc28 ] learning git initial commit with commit notes 1 file changed, 28 insertions ( + ) create mode 100644 Notes.md Pradeeps-MBP:learning-git pradeepgorthi$ git status On branch master nothing to commit, working tree clean","title":"Initializing a repo and committing files"},{"location":"DevOps/git-process/#committing-changes","text":"As discussed in the previous scenario git status allows us to view the changes in the working directory and staging area compared to the repository. The command git diff enables you to compare changes in the working directory against a previously committed version. By default the command compares the working directory and the HEAD commit. If you wish to compare against an older version then provide the commit hash as a parameter, for example git diff <commit> . Comparing against commits will output the changes for all of the files modified. If you want to compare the changes to a single file then provide the name as an argument such as git diff committed.js > git diff diff --git a/committed.js b/committed.js index 12e7e7c..fc77969 100644 --- a/committed.js +++ b/committed.js @@ -1 +1 @@ -console.log ( \"Committed File\" ) +console.log ( \"Demostrating changing a committed file\" ) As in the previous scenario in order to commit a change it must first be staged using git add command. If you rename or delete files then you need to specify these files in the add command for them to be tracked. Alternatives you can use git mv and git rm for git to perform the action and include update the staging area. Once the changes are in the staging area they will not show in the output from git diff . By default, it will only compare the working directory and not the staging area. To compare the changes in the staging area against the previous commit, provide the staged parameter git diff --staged to ensure that you have correctly staged all your changes. > git diff --staged diff --git a/committed.js b/committed.js index 12e7e7c..fc77969 100644 --- a/committed.js +++ b/committed.js @@ -1 +1 @@ -console.log ( \"Committed File\" ) +console.log ( \"Demostrating changing a committed file\" ) diff --git a/untracked.js b/untracked.js new file mode 100644 index 0000000 ..264a5be --- /dev/null +++ b/untracked.js @@ -0,0 +1 @@ +console.log ( \"Untracked File\" ) The command git log allows you to view the history of the repository and the commit log. > git log commit 8e406ea6214f7c89cd732ed8ee43b4e4ee121e24 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Wed Sep 18 15 :44:53 2019 +0000 Changed the output message in committed.js commit 44a3b80bfcdf8a18688cad2149ca8494f7da91a1 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Wed Sep 18 15 :19:35 2019 +0000 Initial Commit The format of the log output is very flexible. > git log --pretty = format: \"%h %an %ar - %s\" 8e406ea Scrapbook Git Tutorial 2 minutes ago - Changed the output message in committed.js 44a3b80 Scrapbook Git Tutorial 27 minutes ago - Initial Commit While git log tells you the commit author and message, to view the changes made in the commit you need to use the the command git show . Use git show <commit-hash> to view older changes. > git show commit 8e406ea6214f7c89cd732ed8ee43b4e4ee121e24 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Wed Sep 18 15 :44:53 2019 +0000 Changed the output message in committed.js diff --git a/committed.js b/committed.js index 12e7e7c..fc77969 100644 --- a/committed.js +++ b/committed.js @@ -1 +1 @@ -console.log ( \"Committed File\" ) +console.log ( \"Demostrating changing a committed file\" ) diff --git a/untracked.js b/untracked.js new file mode 100644 index 0000000 ..264a5be --- /dev/null +++ b/untracked.js @@ -0,0 +1 @@ +console.log ( \"Untracked File\" ) Here is an interesting image to refer to when using git.","title":"Committing Changes"},{"location":"DevOps/git-process/#working-remotely","text":"Remote repositories allow you to share changes from or to your repository. Remote locations are generally a build server, a team members machine or a centralised store such as Github.com. Remotes are added using the git remote command with a friendly name and the remote location, typically a HTTPS URL or a SSH connection. > git remote add test-name /s/remote-project/1 usage: git remote [ -v | --verbose ] or: git remote add [ -t <branch> ] [ -m <master> ] [ -f ] [ --tags | --no-tags ] [ --mirror = <fetch | push> ] <name> <url> or: git remote rename <old> <new> or: git remote remove <name> or: git remote set-head <name> ( -a | --auto | -d | --delete | <branch> ) or: git remote [ -v | --verbose ] show [ -n ] <name> or: git remote prune [ -n | --dry-run ] <name> or: git remote [ -v | --verbose ] update [ -p | --prune ] [( <group> | <remote> ) ... ] or: git remote set-branches [ --add ] <name> <branch>... or: git remote set-url [ --push ] <name> <newurl> [ <oldurl> ] or: git remote set-url --add <name> <newurl> or: git remote set-url --delete <name> <url> -v, --verbose be verbose ; must be placed before a subcommand If you use git clone , then the location being cloned will be automatically added as a remote with the name origin. Pradeeps-MBP:learning-git pradeepgorthi$ git remote add origin https://github.com/deepgorthi/learning-git.git When ready to share the commits, you need to push them to a remote repository via git push and it is followed by two parameters. The first parameter is the friendly name of the remote repository we defined in the first step. The second parameter is the name of the branch. By default all git repositories have a master branch where the code is worked on. Pradeeps-MBP:learning-git pradeepgorthi$ git push -u origin master Username for 'https://github.com' : deepgorthi Password for 'https://deepgorthi@github.com' : Enumerating objects: 6 , done . Counting objects: 100 % ( 6 /6 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 4 /4 ) , done . Writing objects: 100 % ( 6 /6 ) , 1 .62 KiB | 1 .62 MiB/s, done . Total 6 ( delta 1 ) , reused 0 ( delta 0 ) remote: Resolving deltas: 100 % ( 1 /1 ) , done . To https://github.com/deepgorthi/learning-git.git * [ new branch ] master -> master Branch 'master' set up to track remote branch 'master' from 'origin' . A typical Git workflow would be to perform multiple small commits as you complete a task and push to a remote at relevant points, such as when the task is complete, to ensure synchronisation of the code within the team. git pull allows you to sync changes from a remote repository into your local version. Use the command git log --grep=\"#1234\" to find all the commits containing #1234 The command git pull is a combination of two different commands, git pull => git fetch + git merge Fetch downloads the changes from the remote repository into a separate branch named remotes/<remote-name>/<remote-branch-name> . The branch can be accessed using git checkout . Using git fetch is a great way to review the changes without affecting your current branch. The naming format of branches is flexible enough that you can have multiple remotes and branches with the same name and easily switch between them. > git merge remotes/<remote-name>/<remote-branch-name> master You can view a list of all the remote branches using the command > git branch -r","title":"Working Remotely"},{"location":"DevOps/git-process/#undoing-changes","text":"The command git checkout will replace everything in the working directory to the last committed version. If you want to replace all files then use a dot (.) to indicate the current directory, otherwise list the directories/files separated by spaces. If you're in the middle of a commit and have added files to the staging area but then changed your mind then you'll need to use the git reset command. It will move files back from the staging area to the working directory. If you want to reset all files then use a \".\" to indicate current directory, otherwise list the files separated by spaces. $ git add Notes.md $ git add gitimage.jpg $ git reset Notes.md Unstaged changes after reset: M Notes.md $ git commit -m \"Adding git image\" [ master 8eba1eb ] Adding git image 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) create mode 100644 gitimage.jpg $ git push origin master Enumerating objects: 4 , done . Counting objects: 100 % ( 4 /4 ) , done . Delta compression using up to 8 threads Compressing objects: 100 % ( 3 /3 ) , done . Writing objects: 100 % ( 3 /3 ) , 25 .23 KiB | 8 .41 MiB/s, done . Total 3 ( delta 0 ) , reused 0 ( delta 0 ) To https://github.com/deepgorthi/learning-git.git a0d745f..8eba1eb master -> master git reset --hard => git reset + git checkout The result of git reset --hard will be files removed from staging area and the working directory is taken back to the state of the last commit. Using HEAD will clear the state back to the last commit and using git reset --hard <commit-hash> allows you to go back to any commit state. HEAD is an alias for the last commit-hash of the branch. If you have already committed files but realized you made a mistake then the command git revert allows you to undo the commits. The command will create a new commit which has the inverse affect of the commit being reverted. If you haven't pushed your changes then git reset HEAD~1 has the same affect and will remove the last commit. To revert multiple commits at once we use the character ~ to mean minus. For example, HEAD~2 is two commits from the head. This can be combined with the characters ... to say between two commits. Use the command git revert HEAD...HEAD~2 to revert the commits between HEAD and HEAD~2. The command git log --oneline for a quick overview of the commit history.","title":"Undoing Changes"},{"location":"DevOps/git-process/#fixing-merge-conflicts","text":"The git fetch command downloads changes into a separate branch which can be checked out and merge. During a merge, Git will attempt to automatically combine the commits. When no conflicts exist then the merge will be 'fast-forwarded' and you won't have to do anything. If a conflict does exist, then you will retrieve an error and the repository will be in a merging state. > git fetch > git merge origin/master Auto-merging staging.txt CONFLICT ( add/add ) : Merge conflict in staging.txt Automatic merge failed ; fix conflicts and then commit the result. When a conflict occurs the changes from both the local and remote will appear in the same file in the unix diff format. To read the format, the local changes will appear at the top between <<<<<<< HEAD and ======= with the remote changes being underneath between ======= and >>>>>>> remotes/origin/master . <<<<<<< HEAD Fixing Error, Let's Hope No-One Else Does ======= Fixing Previous Error >>>>>>> origin/master The simplest way to fix a conflict is to pick either the local or remote version using git checkout --ours staging.txt or git checkout --theirs staging.txt . If you need to have more control then you can manually edit the file(s) like normal. > git checkout --theirs staging.txt > git add staging.txt > git commit [ master 51ae805 ] Merge remote-tracking branch 'origin/master' If you want to revert in the middle of a merge and try again then use the command git reset --hard HEAD; to go back to your previous state. Use git commit --no-edit when you wish to use the default commit message. To simulate a non-fast forward merge the following has occurred. 1) Developer A pulls the latest changes from Developer B. 2) Developer B commits changes to their local repository. 3) Developer A commits non-conflicting changes to their local repository. 4) Developer A pulls the latest changes from Developer B. In this scenario Git is unable to fast-forward the changes from Developer B because Developer A has made a number of changes. When this happens, Git will attempt to auto-merge the changes. If no conflicts exist then the merge will be completed and a new commit will be created to indicate the merge happening at that point in time. The default commit message for merges is \"Merge branch '' of \". These commits can be useful to indicate synchronization points between repositories but also produce a noisy commit log. > git pull --no-edit origin master remote: Counting objects: 5 , done . remote: Compressing objects: 100 % ( 4 /4 ) , done . remote: Total 4 ( delta 1 ) , reused 0 ( delta 0 ) Unpacking objects: 100 % ( 4 /4 ) , done . From /s/remote-project/1 * branch master -> FETCH_HEAD 418fe6e..095b88a master -> origin/master Merge made by the 'recursive' strategy. new-file-6.txt | 1 + new-file-6a.txt | 1 + 2 files changed, 2 insertions ( + ) create mode 100644 new-file-6.txt create mode 100644 new-file-6a.txt Git Rebase The merge commit messages can be useful to indicate synchronization points but they can also produce a lot of noise. For example, if you're working against local branches and haven't pushed then this additional information is meaningless, and confusing, to other developers looking at the repository. To solve this you can use git rebase instead of git merge. A rebase will unwind the changes you've made and replay the changes in the branch, applying your changes as if they happened all on the same branch. The result is a clean history and graph for the merge. From git-scm : - The easiest way to integrate the branches, as we\u2019ve already covered, is the merge command. It performs a three-way merge between the two latest branch snapshots (C3 and C4) and the most recent common ancestor of the two (C2), creating a new snapshot (and commit). - However, there is another way: you can take the patch of the change that was introduced in C4 and reapply it on top of C3. In Git, this is called rebasing. With the rebase command, you can take all the changes that were committed on one branch and replay them on a different branch. Important: As rebase will replay the changes instead of merging, each commit will have a new hash id. If you, or other developers, have pushed/pulled the repository then changing the history can git to lose commits. As such you shouldn't rebase commits that have been made public, for example pushing commits then rebasing in older commits from a different branch. The result will be previously public commits having different hash ids. More details can be found at The Perils of Rebasing . This approach also applies when working with remote branches and can be applied when issuing a pull request using: git pull --rebase > git pull --rebase There is no tracking information for the current branch. Please specify which branch you want to rebase against. See git-pull ( 1 ) for details git pull <remote> <branch> If you wish to set tracking information for this branch you can do so with: git branch --set-upstream-to = origin/<branch> feature/10 This will act as if you had done a pull request before each of your commits. In general, the way to get the best of both worlds is to rebase local changes you\u2019ve made but haven\u2019t shared yet before you push them in order to clean up your story, but never rebase anything you\u2019ve pushed somewhere.","title":"Fixing Merge Conflicts"},{"location":"DevOps/git-process/#git-branches","text":"A branch allows you to work in a brand new working directory efficiently. The result is that a single Git repository can have multiple different versions of the code-base, each of which can be swapped between without changing directories. The default branch in Git is called master. When you switch a branch, Git changes the contents of the working directory. This means you don't need to change any configurations or settings to reflect different branches or locations. Branches are created based on another branch, generally master. The command git branch <new branch name> <starting branch> takes an existing branch and creates a separate branch to work in. At this point, both branches are identical. To switch to a branch you use the git checkout <new branch name> command. The above two commands can be combined into a single command by doing git checkout -b <new branch name> which will create and checkout the newly created branch and the starting branch defaults to HEAD . > git branch new_branch master > git checkout new_branch Switched to branch 'new_branch' To list all the branches, use the command git branch . The additional argument -a will include remote branches while including -v will include the HEAD commit message for the branch. > git branch -va master 75fa002 First Commit on master new_branch 75fa002 First Commit on master * new_branch_b 1296799 Commit on branch A commit has been made to the new branch. To merge this into master, you would first need to checkout the target branch, in this case master, git checkout master and then use git merge <branch-name> command to merge in the commits from a branch. > git status On branch new_branch nothing to commit, working directory clean > touch test_file > git add test_file > git status On branch new_branch Changes to be committed: ( use \"git reset HEAD <file>...\" to unstage ) new file: test_file > git commit -m \"Adding test file to new_branch\" [ new_branch 0fceb73 ] Adding test file to new_branch 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) create mode 100644 test_file > git checkout master Switched to branch 'master' > git merge new_branch Updating 75fa002..0fceb73 Fast-forward test_file | 0 1 file changed, 0 insertions ( + ) , 0 deletions ( - ) create mode 100644 test_file To push a branch to a remote, use git push <remote_name> <branch_name> > git push origin new_branch Counting objects: 6 , done . Delta compression using up to 12 threads. Compressing objects: 100 % ( 3 /3 ) , done . Writing objects: 100 % ( 6 /6 ) , 524 bytes | 0 bytes/s, done . Total 6 ( delta 0 ) , reused 0 ( delta 0 ) To /s/remote-project/1 * [ new branch ] new_branch -> new_branch Cleaning up branches is important to remove the amount of noise and confusion. To delete a branch, you need to provide the argument -d like git branch -d <branch_name> > git branch -d new_branch Deleted branch new_branch ( was 0fceb73 ) .","title":"Git Branches"},{"location":"DevOps/git-process/#finding-bugs","text":"git diff This command is the simplest to compare what's changed between commits. It will output the differences between the two commits. > git diff HEAD~2 HEAD diff --git a/list.html b/list.html index 96e99d0..9f53aec 100644 --- a/list.html +++ b/list.html @@ -2,4 +2,6 @@ <li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> +<li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> +<li>Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat.</li> </ul> git log While git log helps you see the commit messages but by default it does not output what actually changed. git log --oneline To see the overview of the commits in a short view, use this command. > git log --oneline 43a9071 Final Item 49f29f0 New Item 3c2ef9c Initial commit of the list git log -p To output the commit information with the differences of what changed you need to include the -p prompt > git log -p commit 43a9071cd9f4e4e17dbd1f7ee50020373a04dd6f Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Final Item diff --git a/list.html b/list.html index def310d..9f53aec 100644 --- a/list.html +++ b/list.html @@ -3,4 +3,5 @@ <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> <li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> +<li>Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat.</li> </ul> commit 49f29f018078ec090b466edea6d57ba980e06bd3 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 New Item diff --git a/list.html b/list.html index 96e99d0..def310d 100644 --- a/list.html +++ b/list.html @@ -2,4 +2,5 @@ <li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> +<li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> </ul> commit 3c2ef9c7048a9e98af1679e2b7349cbd53ec17c9 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Initial commit of the list diff --git a/list.html b/list.html new file mode 100644 index 0000000 ..96e99d0 --- /dev/null +++ b/list.html @@ -0,0 +1,5 @@ +<ul> +<li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> +<li>Aliquam tincidunt mauris eu risus.</li> +<li>Vestibulum auctor dapibus neque.</li> +</ul> git log -p -n 2 This will output the entire history. You can filter it with a number of different options. The -n specifies a limit of commits to display from the HEAD. Using '2' displays HEAD and HEAD~1. > git log -p -n 2 commit 43a9071cd9f4e4e17dbd1f7ee50020373a04dd6f Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Final Item diff --git a/list.html b/list.html index def310d..9f53aec 100644 --- a/list.html +++ b/list.html @@ -3,4 +3,5 @@ <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> <li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> +<li>Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat.</li> </ul> commit 49f29f018078ec090b466edea6d57ba980e06bd3 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 New Item diff --git a/list.html b/list.html index 96e99d0..def310d 100644 --- a/list.html +++ b/list.html @@ -2,4 +2,5 @@ <li>Lorem ipsum dolor sit amet, consectetuer adipiscing elit.</li> <li>Aliquam tincidunt mauris eu risus.</li> <li>Vestibulum auctor dapibus neque.</li> +<li>Morbi in sem quis dui placerat ornare. Pellentesque odio nisi, euismod in, pharetra a.</li> </ul> git log --grep=\"Initial\" This will output all the commits which include the word \"Initial\" in their commit message. This is useful if you tag commits with bug-tracking numbers. > git log --grep = \"Initial\" commit 3c2ef9c7048a9e98af1679e2b7349cbd53ec17c9 Author: Scrapbook Git Tutorial <git-tutorial@joinscrapbook.com> Date: Fri Sep 20 17 :34:55 2019 +0000 Initial commit of the list git bisect This command allows you to do a binary search of the repository looking for which commit introduced the problem and the regression. Git bisect takes a number of steps, execute the steps in order to see the results. Steps To enter into bisect mode you use the command git bisect start . Once in bisect mode, define your current checkout as bad using git bisect bad . This indicates that it contains the problem you are searching to see when it was introduced. We have defined where a bad commit happened. Now we need to define when the last known good commit was using git bisect good HEAD~5 . In this case it was five commits ago. The above step will checkout the commit in-between bad and good commits. You can then check the commit, run tests etc to see if the bug exists. In this example, you can check the contents using `cat list.html This commit looks good as everything has correct HTML tags. We tell Git we're happy using git bisect good . This will automatically check out the commit in the middle of the last known good commit, as defined in step 5 and our bad commit. As we did before, we need to check to see if the commit is good or bad using cat list.html This commit has missing HTML tags. Using git bisect bad will end the search and output the the related commit id. The result is that instead of searching five commits, we only searched two. On a much larger timescale, bisect can save you signifant time. git blame It can be useful to know who worked on a certain section of the file to help with improvements in future. This command shows the revision and author who last modified each line of a file. If we know the lines which we're concerned with then we can use the -L parameter to provide a range of lines to output.","title":"Finding Bugs"},{"location":"DevOps/git-process/#being-picky-with-git","text":"Cherry Picking In this scenario, we only care about changes to one of the files but if we merged the branch, then we'd merge all five commits and the unwanted changes. To merge individual commits, we use git cherry-pick <hash-id|ref> command. This behaves in a similar way to merge , if no conflicts exist then the commit will be automatically merged. new_branch~3 refers to the second-to-last commit in the branch. > git log --pretty = oneline --reverse new_branch f3a03a060cd76c4ebd68ba2a35ad9685f041a9b6 Readme File 2be62bb15c92ad4dfc57b62592df09907afc2c5a Initial commit, no items ac4d5aec7ee23de2b6b6756a598dbd5f59f65ac5 Initial list 0e2dc8c0428a3fd5e42af547a97c5eb9828b3c4b Creating Second List 4b4bcb18289c3adf476a97c328caaacfc0f4a9a8 Adding final items to the list > git cherry-pick 2be62bb15c92ad4dfc57b62592df09907afc2c5a [ master 902442e ] Initial commit, no items 1 file changed, 2 insertions ( + ) create mode 100644 list.html > git cherry-pick ac4d5aec7ee23de2b6b6756a598dbd5f59f65ac5 [ master 903c134 ] Initial list 1 file changed, 3 insertions ( + ) > git cherry-pick 4b4bcb18289c3adf476a97c328caaacfc0f4a9a8 [ master e91edc3 ] Adding final items to the list 1 file changed, 1 insertion ( + ) Conflicts can be solved in the same way as with merging a branch - either manually fixing the files or selecting theirs or ours via git checkout . You can stop and revert the pick using git cherry-pick --abort Once the conflicts have been resolved, you can continue with the cherry pick using the command git cherry-pick --continue","title":"Being picky with Git"},{"location":"DevOps/git-process/#re-writing-history-in-git","text":"Re-writing the repositories history is done using git rebase -interactive . This has a total of 6 commands that can be used. > git rebase --interactive --root # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \"squash\", but discard this commit's log message # x, exec = run command (the rest of the line) using shell This will open up a file that can be edited in vim. If we want to correct a type in the commit message, we can change pick to reword and that will result in a new vim file opening. Now, we can change the commit message typo and then save it. > git rebase --interactive --root [ detached HEAD 9eebea9 ] Initial commit of the list 1 file changed, 5 insertions ( + ) create mode 100644 list.html Successfully rebased and updated refs/heads/master git commit --amend can be used to reword the last commit message. If we want to re-order our last two commits, that can be done using HEAD~2 that allows us to modify them. git rebase --interactive HEAD~2 -> Using Vim, simply reorder the lines, save & quit, and the commits will match the order.","title":"Re-writing History in Git"},{"location":"DevOps/google-sre-book-danluu-notes/","text":"Notes on Google SRE \u00b6 Chapter 1: Introduction \u00b6 Traditional Approach Assemble existing components and deploy to produce a service Respond to events and updates as they occur Pros Standard procedure Availability of existing Software Cons Manual intervention for event handling Ops is at odds with dev Google's Approach - SRE Software engineers work on ops Automate tasks Cost effective dev and ops aren't at odds Tenets - SRE team is responsible for: latency performance efficiency change management monitoring emergency response capacity planning Durable focus on Engineering 50% ops cap -> extra ops work is redirected to product teams on overflow. Provides feedback mechanism to product teams as well as keeps load down. Target max 2 events per 8-12 hour on-call shift. Postmortems for all serious incidents, even if they didn\u2019t trigger a page. Blameless postmortems. Move fast without breaking SLO Error budget can be spent on anything: launching features, etc. Error budget allows for discussion about how phased rollouts and 1% experiments can maintain tolerable levels of errors SRE and product devs are incentive aligned to spend the error budget to get maximum feature velocity Build confidence through simulated disasters and other testing Monitoring Automate response when alerts are triggered Never require a human to interpret any part of the alerting domain Three valid kinds of monitoring output Alerts: human needs to take action immediately Tickets: human needs to take action eventually Logging: no action needed Emergency response Reliability is a function of MTTF (mean-time-to-failure) and MTTR (mean-time-to-recovery) Systems that don\u2019t require humans to respond will have higher availability due to lower MTTR Having a \u201cplaybook\u201d produces 3x lower MTTR Change management 70% of outages due to changes in a live system. Mitigation: Implement progressive rollouts Monitoring Rollback Avoid human interaction on repetitive tasks Provisioning Adding capacity is riskier than load shifting as it involves making significant changes to existing systems Expensive enough that it should be done quickly and only when necessary Efficiency and performance Load slows down systems SREs provision to meet capacity target with a specific response time goal Efficiency == costs Chapter 2: Production environment at Google \u00b6 Chapter 3: Embracing risk \u00b6 Managing risk Reliability isn\u2019t linear in cost. It can easily cost 100x more to get one additional increment of reliability Cost associated with redundant equipment Cost of building features for reliability as opposed to \u201cnormal\u201d features Goal : make systems reliable enough, but not too reliable! Measuring service risk Standard practice: identify metric to represent property of system to optimize Possible metric = uptime / (uptime + downtime) Aggregate availability = successful requests / total requests Not all requests are equal, but aggregate availability is an ok first order approximation Usually set quarterly targets Risk tolerance of services Usually not objectively obvious SREs work with product owners to translate business objectives into explicit objectives Identifying risk tolerance of infrastructure services Target availability. Running ex: Bigtable Some consumer services serve data directly from Bigtable -- need low latency and high reliability Some teams use bigtable as a backing store for offline analysis -- care more about throughput than reliability Cost Too expensive to meet all needs generically. Ex: Bigtable instance Low-latency Bigtable user wants low queue depth Throughput oriented Bigtable user wants moderate to high queue depth Success and failure are diametrically opposed in these two cases! Solution Partition infrastructure and offer different levels of service In addition, allows service to externalize the cost of providing different levels of service (e.g., expect latency oriented service to be more expensive than throughput oriented service) Chapter 4: Service Level Objectives \u00b6 Ex: Chubby planned outages Google found that Chubby was consistently over its SLO, and that global Chubby outages would cause unusually bad outages at Google Chubby was so reliable that teams were incorrectly assuming that it would never be down and failing to design systems that account for failures in Chubby Solution: take Chubby down globally when it\u2019s too far above its SLO for a quarter to \u201cshow\u201d teams that Chubby can go down What do you and your users care about? Too many indicators: hard to pay attention Too few indicators: might ignore important behavior Different classes of services should have different indicators User-facing: availability, latency, throughput Storage: latency, availability, durability Big data: throughput, end-to-end latency All systems care about correctness Collecting indicators Can often do naturally from server, but client-side metrics sometimes needed. Aggregation Use distributions and not averages User studies show that people usually prefer slower average with better tail latency Standardize on common defs, e.g., average over 1 minute, average over tasks in cluster, etc. Can have exceptions, but having reasonable defaults makes things easier Choosing targets Don\u2019t pick target based on current performance Current performance may require heroic effort Keep it simple Avoid absolutes Unreasonable to talk about \u201cinfinite\u201d scale or \u201calways\u201d available Minimize number of SLOs Perfection can wait Can always redefine SLOs over time SLOs set expectations Keep a safety margin (internal SLOs can be defined more loosely than external SLOs) Don\u2019t overachieve See Chubby example, above Another example is making sure that the system isn\u2019t too fast under light loads Chapter 5: Eliminating Toil \u00b6 All of these tasks can be useful in eliminating toil Tasks that are not just \u201cwork I don\u2019t want to do\u201d Tasks that are Repetitive Tasks that can be automated No enduring value O(n) with service growth If Toil > 50%, it is a sign that the manager should spread toil load more evenly Chapter 6: Monitoring distributed systems \u00b6 Monitoring can be useful to Analyze long-term trends Compare over time or do experiments Alerting Building dashboards Debugging Setting reasonable expectations Monitoring is non-trivial Number of engineers has decreased over time due to improvements in tooling/libs/centralized monitoring infra General trend towards simpler/faster monitoring systems with better tools Limited success with complex dependency hierarchies (e.g., \u201cif DB slow, alert for DB, otherwise alert for website\u201d). Used mostly for very stable parts of system Rules that generate alerts for humans should be simple to understand and represent a clear failure Four golden signals Latency Traffic Errors Saturation Chapter 7: Evolution of automation at Google \u00b6 Automation is a force multiplier, not a solution for all issues. Value of automation Consistency Extensibility MTTR Faster non-repair actions Time savings Source \u00b6 Danluu's take on SRE book is an amazing resource => Reference Link","title":"Notes on Google SRE"},{"location":"DevOps/google-sre-book-danluu-notes/#notes-on-google-sre","text":"","title":"Notes on Google SRE"},{"location":"DevOps/google-sre-book-danluu-notes/#chapter-1-introduction","text":"Traditional Approach Assemble existing components and deploy to produce a service Respond to events and updates as they occur Pros Standard procedure Availability of existing Software Cons Manual intervention for event handling Ops is at odds with dev Google's Approach - SRE Software engineers work on ops Automate tasks Cost effective dev and ops aren't at odds Tenets - SRE team is responsible for: latency performance efficiency change management monitoring emergency response capacity planning Durable focus on Engineering 50% ops cap -> extra ops work is redirected to product teams on overflow. Provides feedback mechanism to product teams as well as keeps load down. Target max 2 events per 8-12 hour on-call shift. Postmortems for all serious incidents, even if they didn\u2019t trigger a page. Blameless postmortems. Move fast without breaking SLO Error budget can be spent on anything: launching features, etc. Error budget allows for discussion about how phased rollouts and 1% experiments can maintain tolerable levels of errors SRE and product devs are incentive aligned to spend the error budget to get maximum feature velocity Build confidence through simulated disasters and other testing Monitoring Automate response when alerts are triggered Never require a human to interpret any part of the alerting domain Three valid kinds of monitoring output Alerts: human needs to take action immediately Tickets: human needs to take action eventually Logging: no action needed Emergency response Reliability is a function of MTTF (mean-time-to-failure) and MTTR (mean-time-to-recovery) Systems that don\u2019t require humans to respond will have higher availability due to lower MTTR Having a \u201cplaybook\u201d produces 3x lower MTTR Change management 70% of outages due to changes in a live system. Mitigation: Implement progressive rollouts Monitoring Rollback Avoid human interaction on repetitive tasks Provisioning Adding capacity is riskier than load shifting as it involves making significant changes to existing systems Expensive enough that it should be done quickly and only when necessary Efficiency and performance Load slows down systems SREs provision to meet capacity target with a specific response time goal Efficiency == costs","title":"Chapter 1: Introduction"},{"location":"DevOps/google-sre-book-danluu-notes/#chapter-2-production-environment-at-google","text":"","title":"Chapter 2: Production environment at Google"},{"location":"DevOps/google-sre-book-danluu-notes/#chapter-3-embracing-risk","text":"Managing risk Reliability isn\u2019t linear in cost. It can easily cost 100x more to get one additional increment of reliability Cost associated with redundant equipment Cost of building features for reliability as opposed to \u201cnormal\u201d features Goal : make systems reliable enough, but not too reliable! Measuring service risk Standard practice: identify metric to represent property of system to optimize Possible metric = uptime / (uptime + downtime) Aggregate availability = successful requests / total requests Not all requests are equal, but aggregate availability is an ok first order approximation Usually set quarterly targets Risk tolerance of services Usually not objectively obvious SREs work with product owners to translate business objectives into explicit objectives Identifying risk tolerance of infrastructure services Target availability. Running ex: Bigtable Some consumer services serve data directly from Bigtable -- need low latency and high reliability Some teams use bigtable as a backing store for offline analysis -- care more about throughput than reliability Cost Too expensive to meet all needs generically. Ex: Bigtable instance Low-latency Bigtable user wants low queue depth Throughput oriented Bigtable user wants moderate to high queue depth Success and failure are diametrically opposed in these two cases! Solution Partition infrastructure and offer different levels of service In addition, allows service to externalize the cost of providing different levels of service (e.g., expect latency oriented service to be more expensive than throughput oriented service)","title":"Chapter 3: Embracing risk"},{"location":"DevOps/google-sre-book-danluu-notes/#chapter-4-service-level-objectives","text":"Ex: Chubby planned outages Google found that Chubby was consistently over its SLO, and that global Chubby outages would cause unusually bad outages at Google Chubby was so reliable that teams were incorrectly assuming that it would never be down and failing to design systems that account for failures in Chubby Solution: take Chubby down globally when it\u2019s too far above its SLO for a quarter to \u201cshow\u201d teams that Chubby can go down What do you and your users care about? Too many indicators: hard to pay attention Too few indicators: might ignore important behavior Different classes of services should have different indicators User-facing: availability, latency, throughput Storage: latency, availability, durability Big data: throughput, end-to-end latency All systems care about correctness Collecting indicators Can often do naturally from server, but client-side metrics sometimes needed. Aggregation Use distributions and not averages User studies show that people usually prefer slower average with better tail latency Standardize on common defs, e.g., average over 1 minute, average over tasks in cluster, etc. Can have exceptions, but having reasonable defaults makes things easier Choosing targets Don\u2019t pick target based on current performance Current performance may require heroic effort Keep it simple Avoid absolutes Unreasonable to talk about \u201cinfinite\u201d scale or \u201calways\u201d available Minimize number of SLOs Perfection can wait Can always redefine SLOs over time SLOs set expectations Keep a safety margin (internal SLOs can be defined more loosely than external SLOs) Don\u2019t overachieve See Chubby example, above Another example is making sure that the system isn\u2019t too fast under light loads","title":"Chapter 4: Service Level Objectives"},{"location":"DevOps/google-sre-book-danluu-notes/#chapter-5-eliminating-toil","text":"All of these tasks can be useful in eliminating toil Tasks that are not just \u201cwork I don\u2019t want to do\u201d Tasks that are Repetitive Tasks that can be automated No enduring value O(n) with service growth If Toil > 50%, it is a sign that the manager should spread toil load more evenly","title":"Chapter 5: Eliminating Toil"},{"location":"DevOps/google-sre-book-danluu-notes/#chapter-6-monitoring-distributed-systems","text":"Monitoring can be useful to Analyze long-term trends Compare over time or do experiments Alerting Building dashboards Debugging Setting reasonable expectations Monitoring is non-trivial Number of engineers has decreased over time due to improvements in tooling/libs/centralized monitoring infra General trend towards simpler/faster monitoring systems with better tools Limited success with complex dependency hierarchies (e.g., \u201cif DB slow, alert for DB, otherwise alert for website\u201d). Used mostly for very stable parts of system Rules that generate alerts for humans should be simple to understand and represent a clear failure Four golden signals Latency Traffic Errors Saturation","title":"Chapter 6: Monitoring distributed systems"},{"location":"DevOps/google-sre-book-danluu-notes/#chapter-7-evolution-of-automation-at-google","text":"Automation is a force multiplier, not a solution for all issues. Value of automation Consistency Extensibility MTTR Faster non-repair actions Time savings","title":"Chapter 7: Evolution of automation at Google"},{"location":"DevOps/google-sre-book-danluu-notes/#source","text":"Danluu's take on SRE book is an amazing resource => Reference Link","title":"Source"},{"location":"DevOps/k8s/","text":"Kubernetes \u00b6","title":"Kubernetes"},{"location":"DevOps/k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"DevOps/sre-overview/","text":"SRE Overview \u00b6 As quoted in the article and SRE book, Why \u00b6 At their core, the development teams want to launch new features and see them adopted by users. At their core, the ops teams want to make sure the service doesn\u2019t break while they are holding the pager. Because most outages are caused by some kind of change\u2014a new configuration, a new feature launch, or a new type of user traffic\u2014the two teams\u2019 goals are fundamentally in tension. How-to \u00b6 SREs must see the system as a whole and treat its interconnections with as much attention and respect as the components themselves. SREs are software engineers running the products and creating systems to accomplish the work that would otherwise be performed, often manually, by sysadmins. Seven Principles \u00b6 Operations is a software problem Use software engineering approaches to solve that problem (doing operations well) Manage by Service Level Objectives (SLOs) The product team and the SRE team should select an appropriate availability target for the service and its user base, and the service is managed to that SLO. 100% availability is not the goal. Collabaration from business is required. Work to minimize toil Toil is tedious, manual, work. If a machine can perform a desired operation, then a machine often should. Automate this year\u2019s job away Determining what to automate, under what conditions, and how to automate it. Move fast by reducing the cost of failure SREs are specifically charged with improving undesirably late problem discovery. Share ownership with developers Product development and SRE teams should have a holistic view of the stack\u2014the frontend, backend, libraries, storage, kernels, and physical machine. No team should own single components. Use the same tooling, regardless of function or job title There is no good way to manage a service that has one tool for the SREs and another for the product developers. DevOps vs SRE \u00b6 DevOps is in some sense a wider philosophy and culture. Because it effects wider change than does SRE, DevOps is more context-sensitive. SRE has relatively narrowly defined responsibilities and its remit is generally service-oriented (and end-user-oriented) rather than whole-business-oriented. Source \u00b6 Reference Link","title":"SRE Overview"},{"location":"DevOps/sre-overview/#sre-overview","text":"As quoted in the article and SRE book,","title":"SRE Overview"},{"location":"DevOps/sre-overview/#why","text":"At their core, the development teams want to launch new features and see them adopted by users. At their core, the ops teams want to make sure the service doesn\u2019t break while they are holding the pager. Because most outages are caused by some kind of change\u2014a new configuration, a new feature launch, or a new type of user traffic\u2014the two teams\u2019 goals are fundamentally in tension.","title":"Why"},{"location":"DevOps/sre-overview/#how-to","text":"SREs must see the system as a whole and treat its interconnections with as much attention and respect as the components themselves. SREs are software engineers running the products and creating systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.","title":"How-to"},{"location":"DevOps/sre-overview/#seven-principles","text":"Operations is a software problem Use software engineering approaches to solve that problem (doing operations well) Manage by Service Level Objectives (SLOs) The product team and the SRE team should select an appropriate availability target for the service and its user base, and the service is managed to that SLO. 100% availability is not the goal. Collabaration from business is required. Work to minimize toil Toil is tedious, manual, work. If a machine can perform a desired operation, then a machine often should. Automate this year\u2019s job away Determining what to automate, under what conditions, and how to automate it. Move fast by reducing the cost of failure SREs are specifically charged with improving undesirably late problem discovery. Share ownership with developers Product development and SRE teams should have a holistic view of the stack\u2014the frontend, backend, libraries, storage, kernels, and physical machine. No team should own single components. Use the same tooling, regardless of function or job title There is no good way to manage a service that has one tool for the SREs and another for the product developers.","title":"Seven Principles"},{"location":"DevOps/sre-overview/#devops-vs-sre","text":"DevOps is in some sense a wider philosophy and culture. Because it effects wider change than does SRE, DevOps is more context-sensitive. SRE has relatively narrowly defined responsibilities and its remit is generally service-oriented (and end-user-oriented) rather than whole-business-oriented.","title":"DevOps vs SRE"},{"location":"DevOps/sre-overview/#source","text":"Reference Link","title":"Source"},{"location":"DevOps/terraform-basics/","text":"Terraform Basics \u00b6 Features \u00b6 Infrastructure as Code Idempotent: Even if the scripts are run multiple times, nothing will be changed if the configuration is already in place. High-Level Syntax (Hashicorp Configuration Language) Easiily re-usable (with the help of modules) Executioon Plans Shows the inent of the deploy: What is going to be deployed, changed or deleted. Helps ensure everything in the development is intentional. Resource Graph Illustrates all changes and dependencies Use Cases \u00b6 Hybrid Clouds Cloud agnostic Allows deployments to multiple providers simultaneously Multi-tier architecture Allows deployment of several layers of architecture Usually able to automatically deploy in the correct order Software-defined Networking Able to deploy network architecture Used for High-level Infrastructure orchestration tool Not for configuration management Need to be used in conjuctioon with configuration managment tools like Ansible. Provides provisioners that can call tools like Ansible to perform the configuration management process. Installation \u00b6 # Install Docker CE # Install Utils: $ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 # Add the Docker repository: $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # Install Docker CE: $ sudo yum -y install docker-ce # Start Docker and enable it: $ sudo systemctl start docker && sudo systemctl enable docker # Add cloud_user to the docker group: $ sudo usermod -aG docker <user_name> # Test the Docker installation: $ docker --version # Configuring Swarm Manager node # On the manager node, initialize the manager: $ docker swarm init \\ --advertise-addr [ PRIVATE_IP ] # Configure the Swarm Worker node # On the worker node, add the worker to the cluster: $ docker swarm join --token [ TOKEN ] \\ [ PRIVATE_IP ] :2377 # Verify the Swarm cluster # List Swarm nodes: $ docker node ls # Install Terraform # Install Terraform 0.11.13 on the Swarm manager: $ sudo curl -O https://releases.hashicorp.com/terraform/0.12.18/terraform_0.12.18_linux_amd64.zip $ sudo yum install -y unzip $ sudo unzip terraform_0.12.18_linux_amd64.zip -d /usr/local/bin/ # Test the Terraform installation: $ terraform version","title":"Terraform Basics"},{"location":"DevOps/terraform-basics/#terraform-basics","text":"","title":"Terraform Basics"},{"location":"DevOps/terraform-basics/#features","text":"Infrastructure as Code Idempotent: Even if the scripts are run multiple times, nothing will be changed if the configuration is already in place. High-Level Syntax (Hashicorp Configuration Language) Easiily re-usable (with the help of modules) Executioon Plans Shows the inent of the deploy: What is going to be deployed, changed or deleted. Helps ensure everything in the development is intentional. Resource Graph Illustrates all changes and dependencies","title":"Features"},{"location":"DevOps/terraform-basics/#use-cases","text":"Hybrid Clouds Cloud agnostic Allows deployments to multiple providers simultaneously Multi-tier architecture Allows deployment of several layers of architecture Usually able to automatically deploy in the correct order Software-defined Networking Able to deploy network architecture Used for High-level Infrastructure orchestration tool Not for configuration management Need to be used in conjuctioon with configuration managment tools like Ansible. Provides provisioners that can call tools like Ansible to perform the configuration management process.","title":"Use Cases"},{"location":"DevOps/terraform-basics/#installation","text":"# Install Docker CE # Install Utils: $ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 # Add the Docker repository: $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # Install Docker CE: $ sudo yum -y install docker-ce # Start Docker and enable it: $ sudo systemctl start docker && sudo systemctl enable docker # Add cloud_user to the docker group: $ sudo usermod -aG docker <user_name> # Test the Docker installation: $ docker --version # Configuring Swarm Manager node # On the manager node, initialize the manager: $ docker swarm init \\ --advertise-addr [ PRIVATE_IP ] # Configure the Swarm Worker node # On the worker node, add the worker to the cluster: $ docker swarm join --token [ TOKEN ] \\ [ PRIVATE_IP ] :2377 # Verify the Swarm cluster # List Swarm nodes: $ docker node ls # Install Terraform # Install Terraform 0.11.13 on the Swarm manager: $ sudo curl -O https://releases.hashicorp.com/terraform/0.12.18/terraform_0.12.18_linux_amd64.zip $ sudo yum install -y unzip $ sudo unzip terraform_0.12.18_linux_amd64.zip -d /usr/local/bin/ # Test the Terraform installation: $ terraform version","title":"Installation"},{"location":"DevOps/twelve-factor-app/","text":"Twelve Factor App \u00b6 Software is delivered as a service a.k.a web apps . The twelve factor app is a methodology for building SaaS applications by: Minimizing time and cost for new developers joining the project by using declarative formats for setup automation . Offering maximum portability between execution environments. Suitable for deployment on modern cloud platforms . Enabling continuous deployment by minimizing divergence between development and production. Maintaining no change to architecture and tooling while scaling up . Codebase \u00b6 One codebase tracked in revision control, many deploys The app should be tracked in version control system like Git . A copy of the revision tracking database is known as code repo and any single repo or any set of repos sharing a root commit is referred to as codebase . One codebase per app and multiple deploys of the app are possible. A deploy is running instance of the app like prod, staging, local dev environments and so on. The codebase is the same across all deploys , although different versions may be active in each deploy. Multiple codebases translates to distributed system (not an app). Each component in the system is an app. Multiple apps should not share the same code. Factor shared code into libraries. Dependencies \u00b6 Explicitly declare and isolate dependencies The app never relies on implicit existence of system-wide packages. It declares all dependencies via a dependency declaration manifest and uses a dependency isolation tool during execution to remain in a sandbox. Both must be used together. In Ruby, Gemfile is for dependency declaration bundle exec is for dependecy isolation In Python, Pip is for dependency declaration Virtualenv is for dependecy isolation Config \u00b6 Store config in the environment An app\u2019s config is everything that is likely to vary between deploys that includes: - Resource handles to the database and other backing services - Credentials to external services such as Amazon S3 or Twitter - Per-deploy values such as the canonical hostname for the deploy One solution is to use config files which are not checked to version control. But, this is prone to error if checked in. The twelve-factor app stores config in environment variables Sometimes, config can be grouped together to be used in different environments like development , test , etc. This does not scale properly in case new environments enter into the scenario. Backing services \u00b6 Treat backing services as attached resources A backing service is any service (like MySQL or queueing systems) the app consumes over the network as part of its normal operation. The code for a twelve-factor app makes no distinction between local and third party services The app's code should not be changed and be able to swap (URL) between locally managed DB system to a third-party managed DB server. The resource handle in the config should be changed without tweaking the code. Resources can be attached or detached at will. Build, release, run \u00b6 Strictly separate build and run stages A codebase is transformed into a deploy through three stages: - The build stage is a transform which converts a code repo into an executable bundle known as a build by fetching dependencies. - The release stage takes the build produced by the build stage and combines it with the deploy\u2019s current config. - The run (runtime) stage runs the app in the execution environment by launching set of the app's processes. The app must have strict separation between build, release and run stages Every release has a unique Release ID and can be rolled back to a previous working release in case of a failure. Processes \u00b6 Execute the app as one or more stateless processes Any data that needs to persist must be stored in a stateful backing service, typically a database. In a simple scenario, process is launched via CLI in developer's laptop. In a complex scenario, production deploy of a sophisticated app may use many process types. Twelve-factor processes are stateless and share-nothing The twelve-factor app never assumes that anything cached in memory or on disk will be available on a future request or job. Even when running only one process, a restart (triggered by code deploy, config change, or the execution environment relocating the process to a different physical location) will usually wipe out all local (e.g., memory and filesystem) state. Port binding \u00b6 Export services via port binding Web apps are sometimes executed inside a webserver container. The app does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The twelve-factor app is completely self-contained. It exports HTTP as a service by binding to a port, and listening to requests coming in on that port. The port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app. For example, http://localhost:8000/ is a service URL to access the service exported by the app (mkdocs) after building and serving the app using mkdocs serve . Concurrency \u00b6 Scale out via the process model Processes take strong cues from unix process model for running service daemons.Using this model, the developer can architect their app to handle diverse workloads by assigning each type of work to a process type. For example, HTTP requests may be handled by a web process, and long-running background tasks handled by a worker process. In the twelve-factor app, processes are a first class citizen The share-nothing, horizontally partitionable nature of twelve-factor app processes means that adding more concurrency is a simple and reliable operation. The array of process types and number of processes of each type is known as the process formation . Disposability \u00b6 Maximize robustness with fast startup and graceful shutdown The app's processes (disposable) can be started or stopped at a moment's notice resulting in elastic scaling, fast config changes, rapid deployment of code and robustness of production deploys. Processes should minimize startup time. This helps in scalability and agility of the release process. Processes should shut down gracefully when SIGTERM is received. In case of hardware failure, processes should be robust against sudden death. Dev/prod parity \u00b6 Keep development, staging, and production as similar as possible The twelve-factor app is designed for continuous deployment by keeping the gap between development and production to a minimum Gap Issue Continuous Deployment Time Devs code taking weeks to go into prod Devs writing and deploying code in hours/minutes Personnel Devs write code and ops deploy Devs involved in deploying and watching behavior in prod Tools Dev and Ops teams using different tools Keep development and production as similar as possible The twelve-factor developer resists the urge to use different backing services between development and production Lightweight local services are less compelling than they once were. Modern backing services such as Memcached, PostgreSQL and RabbitMQ are not difficult to install and run thanks to modern packaging systems, such as Homebrew and apt-get. Declarative provisioning tools such as Chef and Puppet combined with light-weight virtual environments such as Docker and Vagrant allow developers to run local environments which closely approximate production environments. Logs \u00b6 Treat logs as event streams Logs provide visibility into the behavior of a running app. Logs are the stream of aggregated, time-ordered events collected from the output streams of all running processes and backing services. A twelve-factor app never concerns itself with routing or storage of its output stream Each running process writes event stream to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app\u2019s behavior. In staging or production deploys, each process\u2019 stream will be captured by the execution environment, collated together with all other streams from the app, and routed to one or more final destinations for viewing and long-term archival. The stream can be sent to a log indexing and analysis system that can help in: Finding specific events in the past Large-scale graphing of trends Active alerting as per user-defined heuristics Admin processes \u00b6 Run admin/management tasks as one-off processes The process formation is the array of processes that are used to do the app\u2019s normal operations. There is a need to do other maintenance tasks such as: - Database migrations - Console to run arbitrary code - Running one-time scripts committed into the app\u2019s repo One-off admin processes should be run in an identical environment as the regular long-running processes of the app Admin code must ship with application code to avoid synchronization issues. The same dependency isolation techniques should be used on all process types. A Python program using Virtualenv should use the vendored bin/python for running both the Tornado webserver and any manage.py admin processes.","title":"Twelve Factor App"},{"location":"DevOps/twelve-factor-app/#twelve-factor-app","text":"Software is delivered as a service a.k.a web apps . The twelve factor app is a methodology for building SaaS applications by: Minimizing time and cost for new developers joining the project by using declarative formats for setup automation . Offering maximum portability between execution environments. Suitable for deployment on modern cloud platforms . Enabling continuous deployment by minimizing divergence between development and production. Maintaining no change to architecture and tooling while scaling up .","title":"Twelve Factor App"},{"location":"DevOps/twelve-factor-app/#codebase","text":"One codebase tracked in revision control, many deploys The app should be tracked in version control system like Git . A copy of the revision tracking database is known as code repo and any single repo or any set of repos sharing a root commit is referred to as codebase . One codebase per app and multiple deploys of the app are possible. A deploy is running instance of the app like prod, staging, local dev environments and so on. The codebase is the same across all deploys , although different versions may be active in each deploy. Multiple codebases translates to distributed system (not an app). Each component in the system is an app. Multiple apps should not share the same code. Factor shared code into libraries.","title":"Codebase"},{"location":"DevOps/twelve-factor-app/#dependencies","text":"Explicitly declare and isolate dependencies The app never relies on implicit existence of system-wide packages. It declares all dependencies via a dependency declaration manifest and uses a dependency isolation tool during execution to remain in a sandbox. Both must be used together. In Ruby, Gemfile is for dependency declaration bundle exec is for dependecy isolation In Python, Pip is for dependency declaration Virtualenv is for dependecy isolation","title":"Dependencies"},{"location":"DevOps/twelve-factor-app/#config","text":"Store config in the environment An app\u2019s config is everything that is likely to vary between deploys that includes: - Resource handles to the database and other backing services - Credentials to external services such as Amazon S3 or Twitter - Per-deploy values such as the canonical hostname for the deploy One solution is to use config files which are not checked to version control. But, this is prone to error if checked in. The twelve-factor app stores config in environment variables Sometimes, config can be grouped together to be used in different environments like development , test , etc. This does not scale properly in case new environments enter into the scenario.","title":"Config"},{"location":"DevOps/twelve-factor-app/#backing-services","text":"Treat backing services as attached resources A backing service is any service (like MySQL or queueing systems) the app consumes over the network as part of its normal operation. The code for a twelve-factor app makes no distinction between local and third party services The app's code should not be changed and be able to swap (URL) between locally managed DB system to a third-party managed DB server. The resource handle in the config should be changed without tweaking the code. Resources can be attached or detached at will.","title":"Backing services"},{"location":"DevOps/twelve-factor-app/#build-release-run","text":"Strictly separate build and run stages A codebase is transformed into a deploy through three stages: - The build stage is a transform which converts a code repo into an executable bundle known as a build by fetching dependencies. - The release stage takes the build produced by the build stage and combines it with the deploy\u2019s current config. - The run (runtime) stage runs the app in the execution environment by launching set of the app's processes. The app must have strict separation between build, release and run stages Every release has a unique Release ID and can be rolled back to a previous working release in case of a failure.","title":"Build, release, run"},{"location":"DevOps/twelve-factor-app/#processes","text":"Execute the app as one or more stateless processes Any data that needs to persist must be stored in a stateful backing service, typically a database. In a simple scenario, process is launched via CLI in developer's laptop. In a complex scenario, production deploy of a sophisticated app may use many process types. Twelve-factor processes are stateless and share-nothing The twelve-factor app never assumes that anything cached in memory or on disk will be available on a future request or job. Even when running only one process, a restart (triggered by code deploy, config change, or the execution environment relocating the process to a different physical location) will usually wipe out all local (e.g., memory and filesystem) state.","title":"Processes"},{"location":"DevOps/twelve-factor-app/#port-binding","text":"Export services via port binding Web apps are sometimes executed inside a webserver container. The app does not rely on runtime injection of a webserver into the execution environment to create a web-facing service. The twelve-factor app is completely self-contained. It exports HTTP as a service by binding to a port, and listening to requests coming in on that port. The port-binding approach means that one app can become the backing service for another app, by providing the URL to the backing app as a resource handle in the config for the consuming app. For example, http://localhost:8000/ is a service URL to access the service exported by the app (mkdocs) after building and serving the app using mkdocs serve .","title":"Port binding"},{"location":"DevOps/twelve-factor-app/#concurrency","text":"Scale out via the process model Processes take strong cues from unix process model for running service daemons.Using this model, the developer can architect their app to handle diverse workloads by assigning each type of work to a process type. For example, HTTP requests may be handled by a web process, and long-running background tasks handled by a worker process. In the twelve-factor app, processes are a first class citizen The share-nothing, horizontally partitionable nature of twelve-factor app processes means that adding more concurrency is a simple and reliable operation. The array of process types and number of processes of each type is known as the process formation .","title":"Concurrency"},{"location":"DevOps/twelve-factor-app/#disposability","text":"Maximize robustness with fast startup and graceful shutdown The app's processes (disposable) can be started or stopped at a moment's notice resulting in elastic scaling, fast config changes, rapid deployment of code and robustness of production deploys. Processes should minimize startup time. This helps in scalability and agility of the release process. Processes should shut down gracefully when SIGTERM is received. In case of hardware failure, processes should be robust against sudden death.","title":"Disposability"},{"location":"DevOps/twelve-factor-app/#devprod-parity","text":"Keep development, staging, and production as similar as possible The twelve-factor app is designed for continuous deployment by keeping the gap between development and production to a minimum Gap Issue Continuous Deployment Time Devs code taking weeks to go into prod Devs writing and deploying code in hours/minutes Personnel Devs write code and ops deploy Devs involved in deploying and watching behavior in prod Tools Dev and Ops teams using different tools Keep development and production as similar as possible The twelve-factor developer resists the urge to use different backing services between development and production Lightweight local services are less compelling than they once were. Modern backing services such as Memcached, PostgreSQL and RabbitMQ are not difficult to install and run thanks to modern packaging systems, such as Homebrew and apt-get. Declarative provisioning tools such as Chef and Puppet combined with light-weight virtual environments such as Docker and Vagrant allow developers to run local environments which closely approximate production environments.","title":"Dev/prod parity"},{"location":"DevOps/twelve-factor-app/#logs","text":"Treat logs as event streams Logs provide visibility into the behavior of a running app. Logs are the stream of aggregated, time-ordered events collected from the output streams of all running processes and backing services. A twelve-factor app never concerns itself with routing or storage of its output stream Each running process writes event stream to stdout. During local development, the developer will view this stream in the foreground of their terminal to observe the app\u2019s behavior. In staging or production deploys, each process\u2019 stream will be captured by the execution environment, collated together with all other streams from the app, and routed to one or more final destinations for viewing and long-term archival. The stream can be sent to a log indexing and analysis system that can help in: Finding specific events in the past Large-scale graphing of trends Active alerting as per user-defined heuristics","title":"Logs"},{"location":"DevOps/twelve-factor-app/#admin-processes","text":"Run admin/management tasks as one-off processes The process formation is the array of processes that are used to do the app\u2019s normal operations. There is a need to do other maintenance tasks such as: - Database migrations - Console to run arbitrary code - Running one-time scripts committed into the app\u2019s repo One-off admin processes should be run in an identical environment as the regular long-running processes of the app Admin code must ship with application code to avoid synchronization issues. The same dependency isolation techniques should be used on all process types. A Python program using Virtualenv should use the vendored bin/python for running both the Tornado webserver and any manage.py admin processes.","title":"Admin processes"},{"location":"Interview%20Notes/ansible/","text":"Ansible \u00b6","title":"Ansible"},{"location":"Interview%20Notes/ansible/#ansible","text":"","title":"Ansible"},{"location":"Interview%20Notes/aws/","text":"AWS \u00b6","title":"AWS"},{"location":"Interview%20Notes/aws/#aws","text":"","title":"AWS"},{"location":"Interview%20Notes/behavorial-questions/","text":"Behavorial Questions \u00b6","title":"Behavorial Questions"},{"location":"Interview%20Notes/behavorial-questions/#behavorial-questions","text":"","title":"Behavorial Questions"},{"location":"Interview%20Notes/docker/","text":"Docker \u00b6","title":"Docker"},{"location":"Interview%20Notes/docker/#docker","text":"","title":"Docker"},{"location":"Interview%20Notes/jenkins/","text":"Jenkins \u00b6","title":"Jenkins"},{"location":"Interview%20Notes/jenkins/#jenkins","text":"","title":"Jenkins"},{"location":"Interview%20Notes/k8s/","text":"Kubernetes \u00b6","title":"Kubernetes"},{"location":"Interview%20Notes/k8s/#kubernetes","text":"","title":"Kubernetes"},{"location":"Interview%20Notes/python/","text":"Python \u00b6","title":"Python"},{"location":"Interview%20Notes/python/#python","text":"","title":"Python"},{"location":"Interview%20Notes/terraform/","text":"Terraform \u00b6 null_resource \u00b6 1 A null_resource behaves exactly like any other resource, so you configure provisioners, connection details, and other meta-parameters in the same way you would on any other resource. This is a null_resource to execute seeding logic after the database is created and for one-time use. variable \"password\" { default = \"password123\" } resource \"aws_db_instance\" \"example\" { allocated_storage = 10 storage_type = \"gp2\" engine = \"postgres\" instance_class = \"db.t2.micro\" name = \"example\" username = \"user\" password = var . password } resource \"null_resource\" \"seed\" { provisioner \"local-exec\" { command = \"PGPASSWORD = ${var.password} psql --host = ${aws_db_instance.example.address} --port = ${aws_db_instance.example.port} --username = ${aws_db_instance.example.username} --dbname = ${ aws_db_instance . example . name } < seed . sql\" } } depends_on \u00b6 1 Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure. To create resources in a specific order when needed, we can use depends_on property. resource \"null_resource\" \"first\" { provisioner \"local-exec\" { command = \"echo 'first'\" } } resource \"null_resource\" \"second\" { depends_on = [ \"null_resource.first\" ] provisioner \"local-exec\" { command = \"echo 'second'\" } } resource \"null_resource\" \"third\" { depends_on = [ \"null_resource.second\" ] provisioner \"local-exec\" { command = \"echo 'third'\" } } Targeted Changes \u00b6 1 As your infrastructure gets more sophisticated and number of Terraform resources increases, we do not want to touch DNS or other sensitive resources while terraform apply everytime. To reduce the scope of apply, the -target option can be used to focus Terraform's attention on only a subset of resources. This is useful when using terraform with automated deployment/delivery. When deploying multiple resources in each release, we can use null_resource along with depends_on to specify a singular resource and use terraform apply -target = null_resource.deployment Terraform State \u00b6 Reference \u21a9 \u21a9 \u21a9","title":"Terraform"},{"location":"Interview%20Notes/terraform/#terraform","text":"","title":"Terraform"},{"location":"Interview%20Notes/terraform/#null_resource","text":"1 A null_resource behaves exactly like any other resource, so you configure provisioners, connection details, and other meta-parameters in the same way you would on any other resource. This is a null_resource to execute seeding logic after the database is created and for one-time use. variable \"password\" { default = \"password123\" } resource \"aws_db_instance\" \"example\" { allocated_storage = 10 storage_type = \"gp2\" engine = \"postgres\" instance_class = \"db.t2.micro\" name = \"example\" username = \"user\" password = var . password } resource \"null_resource\" \"seed\" { provisioner \"local-exec\" { command = \"PGPASSWORD = ${var.password} psql --host = ${aws_db_instance.example.address} --port = ${aws_db_instance.example.port} --username = ${aws_db_instance.example.username} --dbname = ${ aws_db_instance . example . name } < seed . sql\" } }","title":"null_resource"},{"location":"Interview%20Notes/terraform/#depends_on","text":"1 Terraform builds a graph of all your resources, and parallelizes the creation and modification of any non-dependent resources. Because of this, Terraform builds infrastructure as efficiently as possible, and operators get insight into dependencies in their infrastructure. To create resources in a specific order when needed, we can use depends_on property. resource \"null_resource\" \"first\" { provisioner \"local-exec\" { command = \"echo 'first'\" } } resource \"null_resource\" \"second\" { depends_on = [ \"null_resource.first\" ] provisioner \"local-exec\" { command = \"echo 'second'\" } } resource \"null_resource\" \"third\" { depends_on = [ \"null_resource.second\" ] provisioner \"local-exec\" { command = \"echo 'third'\" } }","title":"depends_on"},{"location":"Interview%20Notes/terraform/#targeted-changes","text":"1 As your infrastructure gets more sophisticated and number of Terraform resources increases, we do not want to touch DNS or other sensitive resources while terraform apply everytime. To reduce the scope of apply, the -target option can be used to focus Terraform's attention on only a subset of resources. This is useful when using terraform with automated deployment/delivery. When deploying multiple resources in each release, we can use null_resource along with depends_on to specify a singular resource and use terraform apply -target = null_resource.deployment","title":"Targeted Changes"},{"location":"Interview%20Notes/terraform/#terraform-state","text":"Reference \u21a9 \u21a9 \u21a9","title":"Terraform State"},{"location":"Linux/support/","text":"Support Documentation \u00b6","title":"Support Documentation"},{"location":"Linux/support/#support-documentation","text":"","title":"Support Documentation"},{"location":"Projects/project1-ci-cd-pipeline-website/","text":"CI/CD Pipeline for Website \u00b6","title":"CI/CD Pipeline for Website"},{"location":"Projects/project1-ci-cd-pipeline-website/#cicd-pipeline-for-website","text":"","title":"CI/CD Pipeline for Website"},{"location":"Projects/project2-elk-aws-ecs/","text":"ELK on AWS ECS \u00b6","title":"ELK on AWS ECS"},{"location":"Projects/project2-elk-aws-ecs/#elk-on-aws-ecs","text":"","title":"ELK on AWS ECS"},{"location":"Projects/project3-video-sharing/","text":"Video Sharing Website \u00b6 Tags: Lambda, Python, Boto3, Terraform","title":"Video Sharing Website"},{"location":"Projects/project3-video-sharing/#video-sharing-website","text":"Tags: Lambda, Python, Boto3, Terraform","title":"Video Sharing Website"},{"location":"Python/key-points/","text":"Notes \u00b6 Key Points About Python: \u00b6 Object-Oriented Scripting Language. Dynamic & strong typing system. Dynamic types are checked at runtime Strong types don\u2019t change implicitly, can\u2019t add 1 and \"something\". Supports functional concepts like map, reduce, filter, and list comprehension. Whitespace delimited (no { or } around code blocks) Pseudo-code like syntax Extremely popular language used across many different disciplines (academia, data science, scripting, web development, etc.). Large open source community and public package index (Pypi). Runs on all major operating systems (historically more of a pain to run on Windows than Unix systems). Pre-installed on most *NIX systems. Supported by large companies such as Google & YouTube. REPL \u00b6 Python is an interpreted language , and the code is evaluated line-by-line . Since each line can be evaluated by itself, the time between evaluating each line doesn\u2019t matter, and this allows us to have a REPL. REPL stands for: Read, Evaluate, Print, Loop. Each line is read, evaluated, the return value is then printed to the screen, and then the process repeats. Python ships with a REPL, and you can access it by running python3.6 from your terminal. $ python3.6 Python 3 .6.4 ( default, Jan 5 2018 , 20 :24:27 ) [ GCC 4 .8.5 20150623 ( Red Hat 4 .8.5-16 )] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information. Some Comments \u00b6 #!/usr/bin/env python3.6 print ( \"Hello, World\" ) $ chmod u+x hello Run the script by using ./hello and we\u2019ll see the same result. Let\u2019s create a bin directory and move the script: $ mkdir ~/bin $ mv hello ~/bin/ To add this directory to the $PATH in .bashrc: $ export PATH = $HOME /bin: $PATH $ hello # this will run the script from anywhere Triple-quoted string is a multi-line string that can functionally work like comments, but they will still be allocated into memory. \"\"\" This is not a block comment, but it will work when you need for some lines of code to not execute. \"\"\" Str and Numeric types \u00b6 Some useful snippets on Python: >>> \"double\" . find ( 's' ) - 1 >>> \"double\" . find ( 'u' ) 2 >>> \"double\" . find ( 'bl' ) 3 >>> \"PassWord123\" . lower () 'password123' >>> print ( \"Tab \\t Delimited\" ) Tab Delimited >>> print ( \"New \\n Line\" ) New Line >>> print ( \"Slash \\\\ Character\" ) Slash \\ Character >>> print ( \"'Single' in Double\" ) 'Single' in Double >>> print ( '\"Double\" in Single' ) \"Double\" in Single >>> print ( \" \\\" Double \\\" in Double\" ) \"Double\" in Double >>> 2 + 2 # Addition 4 >>> 10 - 4 # Subtraction 6 >>> 3 * 9 # Multiplication 27 >>> 5 / 3 # Division 1.66666666666667 >>> 5 // 3 # Floor division, always returns a number without a remainder 1 >>> 8 % 3 # Modulo division, returns the remainder 2 >>> 2 ** 3 # Exponent 8 >>> str ( 1.1 ) '1.1' >>> int ( \"10\" ) 10 >>> int ( 5.99999 ) 5 >>> float ( \"5.6\" ) 5.6 >>> float ( 5 ) 5.0 >>> float ( \"1.1 things\" ) Traceback ( most recent call last ): File \"\" , line 1 , in ValueError : could not convert string to float : '1.1 things'","title":"Notes"},{"location":"Python/key-points/#notes","text":"","title":"Notes"},{"location":"Python/key-points/#key-points-about-python","text":"Object-Oriented Scripting Language. Dynamic & strong typing system. Dynamic types are checked at runtime Strong types don\u2019t change implicitly, can\u2019t add 1 and \"something\". Supports functional concepts like map, reduce, filter, and list comprehension. Whitespace delimited (no { or } around code blocks) Pseudo-code like syntax Extremely popular language used across many different disciplines (academia, data science, scripting, web development, etc.). Large open source community and public package index (Pypi). Runs on all major operating systems (historically more of a pain to run on Windows than Unix systems). Pre-installed on most *NIX systems. Supported by large companies such as Google & YouTube.","title":"Key Points About Python:"},{"location":"Python/key-points/#repl","text":"Python is an interpreted language , and the code is evaluated line-by-line . Since each line can be evaluated by itself, the time between evaluating each line doesn\u2019t matter, and this allows us to have a REPL. REPL stands for: Read, Evaluate, Print, Loop. Each line is read, evaluated, the return value is then printed to the screen, and then the process repeats. Python ships with a REPL, and you can access it by running python3.6 from your terminal. $ python3.6 Python 3 .6.4 ( default, Jan 5 2018 , 20 :24:27 ) [ GCC 4 .8.5 20150623 ( Red Hat 4 .8.5-16 )] on linux Type \"help\" , \"copyright\" , \"credits\" or \"license\" for more information.","title":"REPL"},{"location":"Python/key-points/#some-comments","text":"#!/usr/bin/env python3.6 print ( \"Hello, World\" ) $ chmod u+x hello Run the script by using ./hello and we\u2019ll see the same result. Let\u2019s create a bin directory and move the script: $ mkdir ~/bin $ mv hello ~/bin/ To add this directory to the $PATH in .bashrc: $ export PATH = $HOME /bin: $PATH $ hello # this will run the script from anywhere Triple-quoted string is a multi-line string that can functionally work like comments, but they will still be allocated into memory. \"\"\" This is not a block comment, but it will work when you need for some lines of code to not execute. \"\"\"","title":"Some Comments"},{"location":"Python/key-points/#str-and-numeric-types","text":"Some useful snippets on Python: >>> \"double\" . find ( 's' ) - 1 >>> \"double\" . find ( 'u' ) 2 >>> \"double\" . find ( 'bl' ) 3 >>> \"PassWord123\" . lower () 'password123' >>> print ( \"Tab \\t Delimited\" ) Tab Delimited >>> print ( \"New \\n Line\" ) New Line >>> print ( \"Slash \\\\ Character\" ) Slash \\ Character >>> print ( \"'Single' in Double\" ) 'Single' in Double >>> print ( '\"Double\" in Single' ) \"Double\" in Single >>> print ( \" \\\" Double \\\" in Double\" ) \"Double\" in Double >>> 2 + 2 # Addition 4 >>> 10 - 4 # Subtraction 6 >>> 3 * 9 # Multiplication 27 >>> 5 / 3 # Division 1.66666666666667 >>> 5 // 3 # Floor division, always returns a number without a remainder 1 >>> 8 % 3 # Modulo division, returns the remainder 2 >>> 2 ** 3 # Exponent 8 >>> str ( 1.1 ) '1.1' >>> int ( \"10\" ) 10 >>> int ( 5.99999 ) 5 >>> float ( \"5.6\" ) 5.6 >>> float ( 5 ) 5.0 >>> float ( \"1.1 things\" ) Traceback ( most recent call last ): File \"\" , line 1 , in ValueError : could not convert string to float : '1.1 things'","title":"Str and Numeric types"},{"location":"Python/oops-python/","text":"Python Object-Oriented Programming \u00b6 Classes and Instances \u00b6 Self can be described as the instance reference in the class. Each method in the class automatically takes the instance as the first argument. Even though self is the standard, anything can be used. __int__ is the constructor declaration for a class. Instead of initializing each object of the class, we are using self as the reference to the object instance. If we are not declaring the object instance when defining the class, then we need to instantiate every property of that instance manually. emp1.fullname() is the same as Employee.fullname(emp1) ########################################## # Manual Object instantiation ########################################## class Employee : pass emp1 . first = 'First' emp1 . last = 'Example' emp1 . pay = 1000 emp1 . email = 'First.Example@example.com' emp2 . first = 'Second' emp2 . last = 'Example' emp2 . pay = 1000 emp2 . email = 'Second.Example@example.com' ########################################## ########################################## # Using self for creating object instance ########################################## class Employee : raise_amount = 1.04 num_of_emps = 0 def __init__ ( self , first , last , pay ): self . first = first self . last = last self . pay = pay self . email = first + '.' + last + '@example.com' # This can also be something like self.fname = first Employee . num_of_emps += 1 def fullname ( self ): return f ' { self . first } { self . last } ' def apply_raise ( self ): self . pay = int ( self . pay * self . raise_amount ) @classmethod def set_raise_amount ( cls , amount ): cls . raise_amount = amount # To change the raise_amount for all the class instances instead of just the instance, we can use: Employee . set_raise_amount ( 1.05 ) emp1 = Employee ( 'First' , 'Example' , 1000 ) emp2 = Employee ( 'Second' , 'Example' , 1000 ) print ( emp1 . email ) print ( emp1 . fullname ()) print ( Employee . fullname ( emp1 )) print ( f \"Pay before raise - { emp1 . pay } \" ) emp1 . apply_raise () print ( f \"Pay after raise - { emp1 . pay } \" ) print ( emp1 . __dict__ ) print ( Employee . num_of_emps ) ########################################## Class Variables and Instance Variables \u00b6 Class variables are variables shared among all instances of the class. Instance variables are unique to each instance of that class. To access the namespace of the class instance, we can use print ( emp1 . __dict__ ) # Output # {'first': 'First', 'last': 'Example', 'pay': 1040, 'email': 'First.Example@example.com'} To change a variable for a class and not any instance, we can use: Employee . num_of_emps += 1 Regular Methods, Class Methods and Static Methods \u00b6 Regular methods in a class automatically take the instance as the first argument. Class methods in a class automatically take the class as the first argument. Adding a decorator ( @classmethod ) before the method definition in the class makes it a class method. The decorator is altering the functionality of the method. @classmethod def set_raise_amount ( cls , amount ): cls . raise_amount = amount # To change the raise_amount for all the class instances instead of just the instance, we can use: Employee . set_raise_amount ( 1.05 ) # We can also call the classmethod using the instance of that class and it will have the same effect on all the instances of that class. emp1 . set_raise_amount ( 1.05 ) Using Class methods as Alternative Constructors We can use this class methods in order to provide multiple ways of creating our objects. If there is a repeating part of the code that we are running in multiple instances of the class, we can define class methods to define that particular code and this is referred to as alternative constructor. Static Methods do not pass anything as the first argument. While regular methods pass the instance of the class and class methods pass the class as the first argument, static methods do not expect the first argument to be either the class or the instance of that class. @staticmethod def is_workday ( day ): if day . weekday () == 5 or if day . weekday () == 6 : return False return True Inheritance - Creating Subclasses \u00b6","title":"Python Object-Oriented Programming"},{"location":"Python/oops-python/#python-object-oriented-programming","text":"","title":"Python Object-Oriented Programming"},{"location":"Python/oops-python/#classes-and-instances","text":"Self can be described as the instance reference in the class. Each method in the class automatically takes the instance as the first argument. Even though self is the standard, anything can be used. __int__ is the constructor declaration for a class. Instead of initializing each object of the class, we are using self as the reference to the object instance. If we are not declaring the object instance when defining the class, then we need to instantiate every property of that instance manually. emp1.fullname() is the same as Employee.fullname(emp1) ########################################## # Manual Object instantiation ########################################## class Employee : pass emp1 . first = 'First' emp1 . last = 'Example' emp1 . pay = 1000 emp1 . email = 'First.Example@example.com' emp2 . first = 'Second' emp2 . last = 'Example' emp2 . pay = 1000 emp2 . email = 'Second.Example@example.com' ########################################## ########################################## # Using self for creating object instance ########################################## class Employee : raise_amount = 1.04 num_of_emps = 0 def __init__ ( self , first , last , pay ): self . first = first self . last = last self . pay = pay self . email = first + '.' + last + '@example.com' # This can also be something like self.fname = first Employee . num_of_emps += 1 def fullname ( self ): return f ' { self . first } { self . last } ' def apply_raise ( self ): self . pay = int ( self . pay * self . raise_amount ) @classmethod def set_raise_amount ( cls , amount ): cls . raise_amount = amount # To change the raise_amount for all the class instances instead of just the instance, we can use: Employee . set_raise_amount ( 1.05 ) emp1 = Employee ( 'First' , 'Example' , 1000 ) emp2 = Employee ( 'Second' , 'Example' , 1000 ) print ( emp1 . email ) print ( emp1 . fullname ()) print ( Employee . fullname ( emp1 )) print ( f \"Pay before raise - { emp1 . pay } \" ) emp1 . apply_raise () print ( f \"Pay after raise - { emp1 . pay } \" ) print ( emp1 . __dict__ ) print ( Employee . num_of_emps ) ##########################################","title":"Classes and Instances"},{"location":"Python/oops-python/#class-variables-and-instance-variables","text":"Class variables are variables shared among all instances of the class. Instance variables are unique to each instance of that class. To access the namespace of the class instance, we can use print ( emp1 . __dict__ ) # Output # {'first': 'First', 'last': 'Example', 'pay': 1040, 'email': 'First.Example@example.com'} To change a variable for a class and not any instance, we can use: Employee . num_of_emps += 1","title":"Class Variables and Instance Variables"},{"location":"Python/oops-python/#regular-methods-class-methods-and-static-methods","text":"Regular methods in a class automatically take the instance as the first argument. Class methods in a class automatically take the class as the first argument. Adding a decorator ( @classmethod ) before the method definition in the class makes it a class method. The decorator is altering the functionality of the method. @classmethod def set_raise_amount ( cls , amount ): cls . raise_amount = amount # To change the raise_amount for all the class instances instead of just the instance, we can use: Employee . set_raise_amount ( 1.05 ) # We can also call the classmethod using the instance of that class and it will have the same effect on all the instances of that class. emp1 . set_raise_amount ( 1.05 ) Using Class methods as Alternative Constructors We can use this class methods in order to provide multiple ways of creating our objects. If there is a repeating part of the code that we are running in multiple instances of the class, we can define class methods to define that particular code and this is referred to as alternative constructor. Static Methods do not pass anything as the first argument. While regular methods pass the instance of the class and class methods pass the class as the first argument, static methods do not expect the first argument to be either the class or the instance of that class. @staticmethod def is_workday ( day ): if day . weekday () == 5 or if day . weekday () == 6 : return False return True","title":"Regular Methods, Class Methods and Static Methods"},{"location":"Python/oops-python/#inheritance-creating-subclasses","text":"","title":"Inheritance - Creating Subclasses"},{"location":"Python/python-virtual-env/","text":"VirtualEnv for Python \u00b6 Install Python3 \u00b6 Important : make altinstall causes it to not replace the built in python executable when installing with python source code. Ensure that secure_path in /etc/sudoers file includes /usr/local/bin. The line should look something like this: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin pipenv (Preferred) \u00b6 Coming from pip and virtualenvs, and how they allow us to manage our dependency versions, for a development project, pipenv is a new tool to manage the project\u2019s virtualenv and install dependencies. Rather than creating a requirements.txt file for us, pipenv creates a Pipfile that it will use to store virtualenv and dependency information. Full guide to pipenv from Real Python # install pipenv $ pip3 install pipenv # activate our new virtualenv $ pipenv shell # Install 3rd party package $ pipenv install numpy # Install dependency for development only $ pipenv install pytest --dev # To show dependency graph $ pipenv graph # deactivate the virtualenv $ exit To use pipenv with specific versions of either python or 3 rd party packges: # create a virtualenv with specific python version $ pipenv --python $( which python3 ) # Install 3rd party package with specific version $ pipenv install flask == 0 .12.1 # Install requests library directly from git $ pipenv install -e 'git+https://github.com/requests/requests.git#egg=requests' Once everthing is working in the local environment and ready to push to production, our environment must be locked to ensure you have the same one in production: $ pipenv lock To install last successful environment, we need to use Pipfile.lock instead of Pipfile for a production environment: $ pipenv install --ignore-pipfile To install all dependencies and to modify prod code after downloading it to a different dev environment: # Prod environment $ pipenv install # Dev environment with dev dependencies $ pipenv install --dev To uninstall either one unwanted package or all packages: # Uninstall 3rd party package $ pipenv uninstall numpy # Uninstall All packages $ pipenv uninstall --all venv vs virtualenv \u00b6 The venv module was added to the standard library in Python3.3. The pyvenv command is a wrapper around the venv module, and you should strongly consider avoiding the wrapper and just using the module directly, since it solves so many problems inherent with wrapper scripts, particularly when you have multiple versions of Python installed. venv, which is part of Python itself, has access to internals of Python. It can do things the right way with far fewer hacks. For example, virtualenv has to copy the Python interpreter binary into the virtual environment to trick it into thinking it's isolated, whereas venv can just use a configuration file that is read by the Python binary in its normal location for it to know it's supposed to act like it's in a virtual environment. If you are running a version of Python that has venv in the standard library, you can use it without having to install anything, which is another added bonus. Venv for python3 \u00b6 Virtualenvs allow us to create sandboxed Python environments. In Python2, we need to install the virtualenv package to do this, but with Python3 it has been worked in under the module name of venv. To create a virtualenv, use the following command: # python3 -m venv <PATH FOR VIRTUALENV> # -m flag loads a module as a script. $ mkdir venvs $ python3 -m venv venvs/experiment This command creates a directory called env, which contains a directory structure similar to this: \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 activate \u2502 \u251c\u2500\u2500 activate.csh \u2502 \u251c\u2500\u2500 activate.fish \u2502 \u251c\u2500\u2500 easy_install \u2502 \u251c\u2500\u2500 easy_install-3.7 \u2502 \u251c\u2500\u2500 pip \u2502 \u251c\u2500\u2500 pip3 \u2502 \u251c\u2500\u2500 pip3.5 \u2502 \u251c\u2500\u2500 python -> python3.7 \u2502 \u251c\u2500\u2500 python3 -> python3.7 \u2502 \u2514\u2500\u2500 python3.5 -> /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 \u251c\u2500\u2500 include \u251c\u2500\u2500 lib \u2502 \u2514\u2500\u2500 python3.7 \u2502 \u2514\u2500\u2500 site-packages \u2514\u2500\u2500 pyvenv.cfg Virtualenvs are local Python installations with their own site-packages. To use a virtualenv, we need to activate it by sourcing an activate file in the virtualenv\u2019s bin directory: $ source venvs/experiment/bin/activate ( experiment ) ~ $ With the virtualenv activated, the python and pip binaries point to the local Python 3 variations, so we don\u2019t need to append the 3.7 to all of our commands. ( experiment ) ~ $ echo $PATH /home/user/venvs/experiment/bin:/home/user/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin ( experiment ) ~ $ which python ~/venvs/experiment/bin/python ( experiment ) ~ $ pip list Package Version ---------- ------- pip 19 .3.1 setuptools 42 .0.2 To remove the virtualenv\u2019s contents from our $PATH, we will utilize the deactivate script that the virtualenv provided. ( experiment ) ~ $ deactivate Virtualenvwrapper \u00b6 # install $ pip install virtualenvwrapper # setup workon area $ mkdir -p ~/.virtualenvs $ export WORKON_HOME = ~/.virtualenvs # add wrapper functions $ source ~/.local/bin/virtualenvwrapper.sh # using virtualenvwrapper $ mkvirtualenv mynewproj $ workon mynewproj # example of python3 project (virtualenvwrappers) $ mkvirtualenv --python = $( which python2 ) site2 $ workon site2 $ pip install django == 2 .1.3 DirEnv \u00b6 We can use a tool like dirEnv to automate virtualenv or virtualenvwrapper tools when we enter a project directory. For installation on macOS: $ brew install direnv Setup and configuring DirEnv for a python environment in bash: cat <<-'DIRENV_CONFIG' > ~/.direnvrc layout_virtualenv() { local venv_path=\"$1\" source ${venv_path}/bin/activate } layout_virtualenvwrapper() { local venv_path=\"${WORKON_HOME}/$1\" layout_virtualenv $venv_path } DIRENV_CONFIG eval \" $( direnv hook bash ) \" # setup $ export WORKON_HOME = ~/.virtualenvs $ source ~/.local/bin/virtualenvwrapper.sh # create virtualenv $ mkvirtualenv deployprojs # configuring and allowing direnv $ mkdir -p myweb && cd myweb $ echo 'layout virtualenv $HOME/.virtualenvs/deployprojs' > .envrc $ direnv allow Save the packages in a package manifests for future use on another machine, we can run: $ pip freeze > requirements.txt This will capture our packages for future use in a requirements.txt file. On a new system, we can switch to the appropriate virtualenv, and then install the packages again: $ pip install -r requirements.txt","title":"VirtualEnv for Python"},{"location":"Python/python-virtual-env/#virtualenv-for-python","text":"","title":"VirtualEnv for Python"},{"location":"Python/python-virtual-env/#install-python3","text":"Important : make altinstall causes it to not replace the built in python executable when installing with python source code. Ensure that secure_path in /etc/sudoers file includes /usr/local/bin. The line should look something like this: Defaults secure_path = /sbin:/bin:/usr/sbin:/usr/bin:/usr/local/bin","title":"Install Python3"},{"location":"Python/python-virtual-env/#pipenv-preferred","text":"Coming from pip and virtualenvs, and how they allow us to manage our dependency versions, for a development project, pipenv is a new tool to manage the project\u2019s virtualenv and install dependencies. Rather than creating a requirements.txt file for us, pipenv creates a Pipfile that it will use to store virtualenv and dependency information. Full guide to pipenv from Real Python # install pipenv $ pip3 install pipenv # activate our new virtualenv $ pipenv shell # Install 3rd party package $ pipenv install numpy # Install dependency for development only $ pipenv install pytest --dev # To show dependency graph $ pipenv graph # deactivate the virtualenv $ exit To use pipenv with specific versions of either python or 3 rd party packges: # create a virtualenv with specific python version $ pipenv --python $( which python3 ) # Install 3rd party package with specific version $ pipenv install flask == 0 .12.1 # Install requests library directly from git $ pipenv install -e 'git+https://github.com/requests/requests.git#egg=requests' Once everthing is working in the local environment and ready to push to production, our environment must be locked to ensure you have the same one in production: $ pipenv lock To install last successful environment, we need to use Pipfile.lock instead of Pipfile for a production environment: $ pipenv install --ignore-pipfile To install all dependencies and to modify prod code after downloading it to a different dev environment: # Prod environment $ pipenv install # Dev environment with dev dependencies $ pipenv install --dev To uninstall either one unwanted package or all packages: # Uninstall 3rd party package $ pipenv uninstall numpy # Uninstall All packages $ pipenv uninstall --all","title":"pipenv (Preferred)"},{"location":"Python/python-virtual-env/#venv-vs-virtualenv","text":"The venv module was added to the standard library in Python3.3. The pyvenv command is a wrapper around the venv module, and you should strongly consider avoiding the wrapper and just using the module directly, since it solves so many problems inherent with wrapper scripts, particularly when you have multiple versions of Python installed. venv, which is part of Python itself, has access to internals of Python. It can do things the right way with far fewer hacks. For example, virtualenv has to copy the Python interpreter binary into the virtual environment to trick it into thinking it's isolated, whereas venv can just use a configuration file that is read by the Python binary in its normal location for it to know it's supposed to act like it's in a virtual environment. If you are running a version of Python that has venv in the standard library, you can use it without having to install anything, which is another added bonus.","title":"venv vs virtualenv"},{"location":"Python/python-virtual-env/#venv-for-python3","text":"Virtualenvs allow us to create sandboxed Python environments. In Python2, we need to install the virtualenv package to do this, but with Python3 it has been worked in under the module name of venv. To create a virtualenv, use the following command: # python3 -m venv <PATH FOR VIRTUALENV> # -m flag loads a module as a script. $ mkdir venvs $ python3 -m venv venvs/experiment This command creates a directory called env, which contains a directory structure similar to this: \u251c\u2500\u2500 bin \u2502 \u251c\u2500\u2500 activate \u2502 \u251c\u2500\u2500 activate.csh \u2502 \u251c\u2500\u2500 activate.fish \u2502 \u251c\u2500\u2500 easy_install \u2502 \u251c\u2500\u2500 easy_install-3.7 \u2502 \u251c\u2500\u2500 pip \u2502 \u251c\u2500\u2500 pip3 \u2502 \u251c\u2500\u2500 pip3.5 \u2502 \u251c\u2500\u2500 python -> python3.7 \u2502 \u251c\u2500\u2500 python3 -> python3.7 \u2502 \u2514\u2500\u2500 python3.5 -> /Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 \u251c\u2500\u2500 include \u251c\u2500\u2500 lib \u2502 \u2514\u2500\u2500 python3.7 \u2502 \u2514\u2500\u2500 site-packages \u2514\u2500\u2500 pyvenv.cfg Virtualenvs are local Python installations with their own site-packages. To use a virtualenv, we need to activate it by sourcing an activate file in the virtualenv\u2019s bin directory: $ source venvs/experiment/bin/activate ( experiment ) ~ $ With the virtualenv activated, the python and pip binaries point to the local Python 3 variations, so we don\u2019t need to append the 3.7 to all of our commands. ( experiment ) ~ $ echo $PATH /home/user/venvs/experiment/bin:/home/user/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/user/.local/bin:/home/user/bin ( experiment ) ~ $ which python ~/venvs/experiment/bin/python ( experiment ) ~ $ pip list Package Version ---------- ------- pip 19 .3.1 setuptools 42 .0.2 To remove the virtualenv\u2019s contents from our $PATH, we will utilize the deactivate script that the virtualenv provided. ( experiment ) ~ $ deactivate","title":"Venv for python3"},{"location":"Python/python-virtual-env/#virtualenvwrapper","text":"# install $ pip install virtualenvwrapper # setup workon area $ mkdir -p ~/.virtualenvs $ export WORKON_HOME = ~/.virtualenvs # add wrapper functions $ source ~/.local/bin/virtualenvwrapper.sh # using virtualenvwrapper $ mkvirtualenv mynewproj $ workon mynewproj # example of python3 project (virtualenvwrappers) $ mkvirtualenv --python = $( which python2 ) site2 $ workon site2 $ pip install django == 2 .1.3","title":"Virtualenvwrapper"},{"location":"Python/python-virtual-env/#direnv","text":"We can use a tool like dirEnv to automate virtualenv or virtualenvwrapper tools when we enter a project directory. For installation on macOS: $ brew install direnv Setup and configuring DirEnv for a python environment in bash: cat <<-'DIRENV_CONFIG' > ~/.direnvrc layout_virtualenv() { local venv_path=\"$1\" source ${venv_path}/bin/activate } layout_virtualenvwrapper() { local venv_path=\"${WORKON_HOME}/$1\" layout_virtualenv $venv_path } DIRENV_CONFIG eval \" $( direnv hook bash ) \" # setup $ export WORKON_HOME = ~/.virtualenvs $ source ~/.local/bin/virtualenvwrapper.sh # create virtualenv $ mkvirtualenv deployprojs # configuring and allowing direnv $ mkdir -p myweb && cd myweb $ echo 'layout virtualenv $HOME/.virtualenvs/deployprojs' > .envrc $ direnv allow Save the packages in a package manifests for future use on another machine, we can run: $ pip freeze > requirements.txt This will capture our packages for future use in a requirements.txt file. On a new system, we can switch to the appropriate virtualenv, and then install the packages again: $ pip install -r requirements.txt","title":"DirEnv"},{"location":"Python/python-webscraper/","text":"Web Scraping \u00b6 Creating a folder and initializing the project with pipenv install --python python3.7 scrapy $ pipenv install --python python3.7 scrapy Virtualenv already exists! Removing existing virtualenv\u2026 Creating a virtualenv for this project\u2026 Pipfile: /Users/pradeepgorthi/Documents/github-repos/webscraper-python/Pipfile Using /usr/local/bin/python3.7 ( 3 .7.5 ) to create virtualenv\u2026 \u2807 Creating virtual environment...Already using interpreter /usr/local/opt/python/bin/python3.7 Using base prefix '/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7' New python executable in /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD-/bin/python3.7 Also creating executable in /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD-/bin/python Installing setuptools, pip, wheel... done . Running virtualenv with interpreter /usr/local/bin/python3.7 \u2714 Successfully created virtual environment! Virtualenv location: /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD- Installing scrapy\u2026 Adding scrapy to Pipfile 's [packages]\u2026 \u2714 Installation Succeeded Pipfile.lock not found, creating\u2026 Locking [dev-packages] dependencies\u2026 Locking [packages] dependencies\u2026 \u2714 Success! Updated Pipfile.lock (00acc2)! Installing dependencies from Pipfile.lock (00acc2)\u2026 \ud83d\udc0d \u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589 25/25 \u2014 00:00:03 To activate this project' s virtualenv, run pipenv shell. Alternatively, run a command inside the virtualenv with pipenv run. Activating the pipenv shell using: $ pipenv shell Launching subshell in virtual environment\u2026 bash-5.0$ . /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD-/bin/activate ( webscraper-python ) bash-5.0$","title":"Web Scraping"},{"location":"Python/python-webscraper/#web-scraping","text":"Creating a folder and initializing the project with pipenv install --python python3.7 scrapy $ pipenv install --python python3.7 scrapy Virtualenv already exists! Removing existing virtualenv\u2026 Creating a virtualenv for this project\u2026 Pipfile: /Users/pradeepgorthi/Documents/github-repos/webscraper-python/Pipfile Using /usr/local/bin/python3.7 ( 3 .7.5 ) to create virtualenv\u2026 \u2807 Creating virtual environment...Already using interpreter /usr/local/opt/python/bin/python3.7 Using base prefix '/usr/local/Cellar/python/3.7.5/Frameworks/Python.framework/Versions/3.7' New python executable in /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD-/bin/python3.7 Also creating executable in /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD-/bin/python Installing setuptools, pip, wheel... done . Running virtualenv with interpreter /usr/local/bin/python3.7 \u2714 Successfully created virtual environment! Virtualenv location: /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD- Installing scrapy\u2026 Adding scrapy to Pipfile 's [packages]\u2026 \u2714 Installation Succeeded Pipfile.lock not found, creating\u2026 Locking [dev-packages] dependencies\u2026 Locking [packages] dependencies\u2026 \u2714 Success! Updated Pipfile.lock (00acc2)! Installing dependencies from Pipfile.lock (00acc2)\u2026 \ud83d\udc0d \u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589\u2589 25/25 \u2014 00:00:03 To activate this project' s virtualenv, run pipenv shell. Alternatively, run a command inside the virtualenv with pipenv run. Activating the pipenv shell using: $ pipenv shell Launching subshell in virtual environment\u2026 bash-5.0$ . /Users/pradeepgorthi/.local/share/virtualenvs/webscraper-python-TotDRkD-/bin/activate ( webscraper-python ) bash-5.0$","title":"Web Scraping"},{"location":"Python/useful-python-code/","text":"Useful Python Code \u00b6 Anagram \u00b6 # An anagram is a word or phrase formed by rearranging the letters of a different word or phrase. # If the Counter objects of two strings are equal, then they are anagrams. from collections import Counter str_1 , str_2 , str_3 = \"acbde\" , \"abced\" , \"abcda\" cnt_1 , cnt_2 , cnt_3 = Counter ( str_1 ), Counter ( str_2 ), Counter ( str_3 ) if cnt_1 == cnt_2 : print ( '1 and 2 anagram' ) if cnt_1 == cnt_3 : print ( '1 and 3 anagram' ) Convert Title case \u00b6 my_string = \"test scenario again\" # using the title() function of string class new_string = my_string . title () print ( new_string ) Counter \u00b6 ## Python counter keeps track of the frequency of each element in the container. Counter() returns a dictionary with elements as keys and frequency as values. # finding frequency of each element in a list from collections import Counter my_list = [ 'a' , 'a' , 'b' , 'b' , 'b' , 'c' , 'd' , 'd' , 'd' , 'd' , 'd' ] count = Counter ( my_list ) # defining a counter object print ( count ) # Of all elements # Counter({'d': 5, 'b': 3, 'a': 2, 'c': 1}) print ( count [ 'b' ]) # of individual element # 3 print ( count . most_common ( 1 )) # most frequent element # [('d', 5)] Digitize \u00b6 import time num = 123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456 # using map start_time_1 = time . time () list_of_digits = list ( map ( int , str ( num ))) end_time_1 = time . time () time_taken_in_micro_1 = ( end_time_1 - start_time_1 ) * ( 10 ** 6 ) print ( f \"This is the map value: { map ( int , str ( num )) } \" ) print ( list_of_digits ) print ( f \"Time taken using map: { time_taken_in_micro_1 } \" ) # [1, 2, 3, 4, 5, 6] start_time_2 = time . time () # using list comprehension list_of_digits = [ int ( x ) for x in str ( num )] end_time_2 = time . time () time_taken_in_micro_2 = ( end_time_2 - start_time_2 ) * ( 10 ** 6 ) print ( list_of_digits ) print ( f \"Time taken using for loop: { time_taken_in_micro_2 } \" ) # [1, 2, 3, 4, 5, 6] Enumerate \u00b6 # The following script uses enumerate to iterate through values in a list along with their indices. my_list = [ 'a' , 'b' , 'c' , 'd' , 'e' ] for index , value in enumerate ( my_list ): # Two different formats to print print ( ' {0} : {1} ' . format ( index , value )) print ( f \" { index } : { value } \" ) # 0: a # 1: b # 2: c # 3: d # 4: e Flattening Lists \u00b6 # Need to install the package - iteration_utilities using # ```$ pip3 install iteration_utilities``` from iteration_utilities import deepflatten # if you only have one depth nested_list, use this def flatten ( l ): return [ item for sublist in l for item in sublist ] l = [[ 1 , 2 , 3 ],[ 3 ]] print ( flatten ( l )) # [1, 2, 3, 3] # if you don't know how deep the list is nested l = [[ 1 , 2 , 3 ],[ 4 ,[ 5 ],[ 6 , 7 ]],[ 8 ,[ 9 ,[ 10 ]]]] print ( list ( deepflatten ( l , depth = 3 ))) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] Memory Usage \u00b6 import sys num = 21 print ( sys . getsizeof ( num )) Dict Merge \u00b6 dict_1 = { 'apple' : 9 , 'banana' : 6 } dict_2 = { 'banana' : 4 , 'orange' : 8 } combined_dict = { ** dict_1 , ** dict_2 } print ( combined_dict ) # Values from the second dictionary are used in case of intersections. # Output # {'apple': 9, 'banana': 4, 'orange': 8} List Multiplication \u00b6 # Multiplying each element in a list by 2 original_list = [ 1 , 2 , 3 , 4 ] new_list = [ 2 * x for x in original_list ] print ( new_list ) # [2,4,6,8] Palindrome \u00b6 my_string = \"abcba\" if my_string == my_string [:: - 1 ]: print ( \"palindrome\" ) else : print ( \"not palindrome\" ) Reverse String \u00b6 # Reversing a string using slicing my_string = \"Test Scenario\" reversed_string = my_string [:: - 1 ] print ( reversed_string ) Snake Case \u00b6 # https://python.30secondsofcode.org/list # Converts a string to snake case. Break the string into words and combine them adding _ as a separator, using a regexp. import re def snake ( s ): return '_' . join ( re . sub ( '([A-Z][a-z]+)' , r ' \\1' , re . sub ( '([A-Z]+)' , r ' \\1' , s . replace ( '-' , ' ' ))) . split ()) . lower () # EXAMPLES # snake('camelCase') # 'camel_case' # snake('some text') # 'some_text' # snake('some-mixed_string With spaces_underscores-and-hyphens') # 'some_mixed_string_with_spaces_underscores_and_hyphens' # snake('AllThe-small Things') # \"all_the_smal_things\" Exec time \u00b6 import time start_time = time . time () # Code to check a , b = 1 , 2 c = a + b # Code to check end_time = time . time () time_taken_in_micro = ( end_time - start_time ) * ( 10 ** 6 ) print ( f \"Time taken in micro_seconds: { time_taken_in_micro } ms\" ) Exception Handling \u00b6 a , b = 1 , 1 try : print ( a / b ) # exception raised when b is 0 except ZeroDivisionError : print ( \"division by zero\" ) # else statement is run when there is no exception raised in the try block. else : print ( \"no exceptions raised\" ) # If you need to run something irrespective of exception, use finally. finally : print ( \"Run this always\" ) Unique Elements \u00b6 my_string = \"aabbbbbbccccddddeee\" # converting the string to a set temp_set = set ( my_string ) # all elements in a set are unique. # stitching set into a string using join new_string = '' . join ( temp_set ) print ( new_string )","title":"Useful Python Code"},{"location":"Python/useful-python-code/#useful-python-code","text":"","title":"Useful Python Code"},{"location":"Python/useful-python-code/#anagram","text":"# An anagram is a word or phrase formed by rearranging the letters of a different word or phrase. # If the Counter objects of two strings are equal, then they are anagrams. from collections import Counter str_1 , str_2 , str_3 = \"acbde\" , \"abced\" , \"abcda\" cnt_1 , cnt_2 , cnt_3 = Counter ( str_1 ), Counter ( str_2 ), Counter ( str_3 ) if cnt_1 == cnt_2 : print ( '1 and 2 anagram' ) if cnt_1 == cnt_3 : print ( '1 and 3 anagram' )","title":"Anagram"},{"location":"Python/useful-python-code/#convert-title-case","text":"my_string = \"test scenario again\" # using the title() function of string class new_string = my_string . title () print ( new_string )","title":"Convert Title case"},{"location":"Python/useful-python-code/#counter","text":"## Python counter keeps track of the frequency of each element in the container. Counter() returns a dictionary with elements as keys and frequency as values. # finding frequency of each element in a list from collections import Counter my_list = [ 'a' , 'a' , 'b' , 'b' , 'b' , 'c' , 'd' , 'd' , 'd' , 'd' , 'd' ] count = Counter ( my_list ) # defining a counter object print ( count ) # Of all elements # Counter({'d': 5, 'b': 3, 'a': 2, 'c': 1}) print ( count [ 'b' ]) # of individual element # 3 print ( count . most_common ( 1 )) # most frequent element # [('d', 5)]","title":"Counter"},{"location":"Python/useful-python-code/#digitize","text":"import time num = 123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456123456 # using map start_time_1 = time . time () list_of_digits = list ( map ( int , str ( num ))) end_time_1 = time . time () time_taken_in_micro_1 = ( end_time_1 - start_time_1 ) * ( 10 ** 6 ) print ( f \"This is the map value: { map ( int , str ( num )) } \" ) print ( list_of_digits ) print ( f \"Time taken using map: { time_taken_in_micro_1 } \" ) # [1, 2, 3, 4, 5, 6] start_time_2 = time . time () # using list comprehension list_of_digits = [ int ( x ) for x in str ( num )] end_time_2 = time . time () time_taken_in_micro_2 = ( end_time_2 - start_time_2 ) * ( 10 ** 6 ) print ( list_of_digits ) print ( f \"Time taken using for loop: { time_taken_in_micro_2 } \" ) # [1, 2, 3, 4, 5, 6]","title":"Digitize"},{"location":"Python/useful-python-code/#enumerate","text":"# The following script uses enumerate to iterate through values in a list along with their indices. my_list = [ 'a' , 'b' , 'c' , 'd' , 'e' ] for index , value in enumerate ( my_list ): # Two different formats to print print ( ' {0} : {1} ' . format ( index , value )) print ( f \" { index } : { value } \" ) # 0: a # 1: b # 2: c # 3: d # 4: e","title":"Enumerate"},{"location":"Python/useful-python-code/#flattening-lists","text":"# Need to install the package - iteration_utilities using # ```$ pip3 install iteration_utilities``` from iteration_utilities import deepflatten # if you only have one depth nested_list, use this def flatten ( l ): return [ item for sublist in l for item in sublist ] l = [[ 1 , 2 , 3 ],[ 3 ]] print ( flatten ( l )) # [1, 2, 3, 3] # if you don't know how deep the list is nested l = [[ 1 , 2 , 3 ],[ 4 ,[ 5 ],[ 6 , 7 ]],[ 8 ,[ 9 ,[ 10 ]]]] print ( list ( deepflatten ( l , depth = 3 ))) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]","title":"Flattening Lists"},{"location":"Python/useful-python-code/#memory-usage","text":"import sys num = 21 print ( sys . getsizeof ( num ))","title":"Memory Usage"},{"location":"Python/useful-python-code/#dict-merge","text":"dict_1 = { 'apple' : 9 , 'banana' : 6 } dict_2 = { 'banana' : 4 , 'orange' : 8 } combined_dict = { ** dict_1 , ** dict_2 } print ( combined_dict ) # Values from the second dictionary are used in case of intersections. # Output # {'apple': 9, 'banana': 4, 'orange': 8}","title":"Dict Merge"},{"location":"Python/useful-python-code/#list-multiplication","text":"# Multiplying each element in a list by 2 original_list = [ 1 , 2 , 3 , 4 ] new_list = [ 2 * x for x in original_list ] print ( new_list ) # [2,4,6,8]","title":"List Multiplication"},{"location":"Python/useful-python-code/#palindrome","text":"my_string = \"abcba\" if my_string == my_string [:: - 1 ]: print ( \"palindrome\" ) else : print ( \"not palindrome\" )","title":"Palindrome"},{"location":"Python/useful-python-code/#reverse-string","text":"# Reversing a string using slicing my_string = \"Test Scenario\" reversed_string = my_string [:: - 1 ] print ( reversed_string )","title":"Reverse String"},{"location":"Python/useful-python-code/#snake-case","text":"# https://python.30secondsofcode.org/list # Converts a string to snake case. Break the string into words and combine them adding _ as a separator, using a regexp. import re def snake ( s ): return '_' . join ( re . sub ( '([A-Z][a-z]+)' , r ' \\1' , re . sub ( '([A-Z]+)' , r ' \\1' , s . replace ( '-' , ' ' ))) . split ()) . lower () # EXAMPLES # snake('camelCase') # 'camel_case' # snake('some text') # 'some_text' # snake('some-mixed_string With spaces_underscores-and-hyphens') # 'some_mixed_string_with_spaces_underscores_and_hyphens' # snake('AllThe-small Things') # \"all_the_smal_things\"","title":"Snake Case"},{"location":"Python/useful-python-code/#exec-time","text":"import time start_time = time . time () # Code to check a , b = 1 , 2 c = a + b # Code to check end_time = time . time () time_taken_in_micro = ( end_time - start_time ) * ( 10 ** 6 ) print ( f \"Time taken in micro_seconds: { time_taken_in_micro } ms\" )","title":"Exec time"},{"location":"Python/useful-python-code/#exception-handling","text":"a , b = 1 , 1 try : print ( a / b ) # exception raised when b is 0 except ZeroDivisionError : print ( \"division by zero\" ) # else statement is run when there is no exception raised in the try block. else : print ( \"no exceptions raised\" ) # If you need to run something irrespective of exception, use finally. finally : print ( \"Run this always\" )","title":"Exception Handling"},{"location":"Python/useful-python-code/#unique-elements","text":"my_string = \"aabbbbbbccccddddeee\" # converting the string to a set temp_set = set ( my_string ) # all elements in a set are unique. # stitching set into a string using join new_string = '' . join ( temp_set ) print ( new_string )","title":"Unique Elements"},{"location":"Support/grav-website/","text":"Grav Website Setup \u00b6 Setup from GitHub \u00b6 Clone the Grav repository from https://github.com/getgrav/grav to a folder in the webroot of your server, e.g. ~/webroot/grav . Launch a terminal or console and navigate to the webroot folder: $ cd grav-resume $ git clone https://github.com/getgrav/grav.git Install the plugin and theme dependencies by using the Grav CLI application bin/grav : $ brew install php $ cd grav-resume/grav $ bin/grav install Check out the install procedures for more information. To have launchd start php now and restart at login: $ brew services start php Use the Grav Package Manager or GPM: # display all the available plugins $ bin/gpm index # install one or more themes $ bin/gpm install <plugin/theme> Maintaining Grav \u00b6 # To update Grav: $ bin/gpm selfupgrade # To update plugins and themes: $ bin/gpm update","title":"Grav Website Setup"},{"location":"Support/grav-website/#grav-website-setup","text":"","title":"Grav Website Setup"},{"location":"Support/grav-website/#setup-from-github","text":"Clone the Grav repository from https://github.com/getgrav/grav to a folder in the webroot of your server, e.g. ~/webroot/grav . Launch a terminal or console and navigate to the webroot folder: $ cd grav-resume $ git clone https://github.com/getgrav/grav.git Install the plugin and theme dependencies by using the Grav CLI application bin/grav : $ brew install php $ cd grav-resume/grav $ bin/grav install Check out the install procedures for more information. To have launchd start php now and restart at login: $ brew services start php Use the Grav Package Manager or GPM: # display all the available plugins $ bin/gpm index # install one or more themes $ bin/gpm install <plugin/theme>","title":"Setup from GitHub"},{"location":"Support/grav-website/#maintaining-grav","text":"# To update Grav: $ bin/gpm selfupgrade # To update plugins and themes: $ bin/gpm update","title":"Maintaining Grav"},{"location":"Support/hugo-website/","text":"Hugo Website Setup \u00b6 Installation \u00b6 Install Hugo using: $ brew install hugo Once Hugo is installed, new site can be created using: $ hugo new site hugo-resume To apply a specific theme, like devresume, in the root of Hugo webroot folder, $ cd themes $ git clone https://github.com/deepgorthi/hugo-resume-custom.git Once that is done, copy config.toml to the webroot folder and change the data as needed: $ cp hugo-resume-custom/exampleSite/config.toml Build and Serve \u00b6 After the site is configured, it can be served using: hugo server The site can be accessed locally using: http://localhost:1313 Deployment \u00b6 To deploy the site to GH pages, we can run this: $ ./publish.sh For further configuration and other support related documentaiton, here is the best source","title":"Hugo Website Setup"},{"location":"Support/hugo-website/#hugo-website-setup","text":"","title":"Hugo Website Setup"},{"location":"Support/hugo-website/#installation","text":"Install Hugo using: $ brew install hugo Once Hugo is installed, new site can be created using: $ hugo new site hugo-resume To apply a specific theme, like devresume, in the root of Hugo webroot folder, $ cd themes $ git clone https://github.com/deepgorthi/hugo-resume-custom.git Once that is done, copy config.toml to the webroot folder and change the data as needed: $ cp hugo-resume-custom/exampleSite/config.toml","title":"Installation"},{"location":"Support/hugo-website/#build-and-serve","text":"After the site is configured, it can be served using: hugo server The site can be accessed locally using: http://localhost:1313","title":"Build and Serve"},{"location":"Support/hugo-website/#deployment","text":"To deploy the site to GH pages, we can run this: $ ./publish.sh For further configuration and other support related documentaiton, here is the best source","title":"Deployment"},{"location":"Support/jekyll-website/","text":"Jekyll Website Setup \u00b6 Jekyll Website \u00b6 [CloudFormation Template] |---------------------------------------------------------------------| | (Resolve Domain name) | | |-->[Amazon Route53] | | | (Get contents) | | | |-->[Amazon S3 contents Bucket] | | Users---|-->[AWS CloudFront]--| | | | |-->[Amazon S3 Logging Bucket] | | | (Save Logs) | | |-->[AWS Certificate Manager] | |---------------------------------------------------------------------| AWS CI/CD Pipeline \u00b6 [CloudFormation Template] |---------------------------------------------------------------------------| |[NewCodeChange] --> [Pushed to master branch] --> [AWS CodePipeline] | | | | | | | | Published to S3 <-- Build Jekyll Website <-- [AWS CodeBuild] | | | | | | | | Lambda function triggered to invalidate CloudFront Distribution | | (Old files removed from CloudFront edge before the actual expiration) | |---------------------------------------------------------------------------| Running Jekyll in Docker container \u00b6 Jekyll might require a different version of ruby and gem than the one installed on your computer and when working on MacOSX, it is difficult to work with a different version than the one already pre-installed. Rather than corrupting the PATH and ruby versions, it is best to run jekyll in a container. Here are the following steps to setup and work with jekyll in a container. $ echo 'JEKYLL_VERSION=4.0' >> ~/.bash_profile Create a project directory to hold the files and directories for jekyll. $ mkdir -p ~/jekyll-dev/new-blog $ cd ~/jekyll-dev/new-blog Run docker using the following command: $ docker run --rm --volume = \" $PWD :/srv/jekyll\" -it jekyll/jekyll: $JEKYLL_VERSION jekyll new . To build the site, use the following command: $ docker run --rm --volume = \" $PWD :/srv/jekyll\" -it jekyll/jekyll: $JEKYLL_VERSION jekyll build To run the site locally: $ docker run --name deepgorthi --volume = \" $PWD :/srv/jekyll\" -p 4000 :4000 -it jekyll/jekyll: $JEKYLL_VERSION jekyll serve --watch --drafts The website can be locally accessed using http://localhost:4000 To execute commands in the container: $ docker exec -ti deepgorthi gem install \"jekyll-theme-example\" Executing a shell in the container: $ docker exec -ti deepgorthi /bin/sh Removing container: docker rm -f deepgorthi After making any changes to the jekyll website, you can either build jekyll within docker using: $ docker run --name deepgorthi --volume = \" $PWD :/srv/jekyll\" -p 4000 :4000 -it jekyll/jekyll: $JEKYLL_VERSION jekyll serve --watch --drafts Or, you can restart docker container using: $ docker restart deepgorthi","title":"Jekyll Website Setup"},{"location":"Support/jekyll-website/#jekyll-website-setup","text":"","title":"Jekyll Website Setup"},{"location":"Support/jekyll-website/#jekyll-website","text":"[CloudFormation Template] |---------------------------------------------------------------------| | (Resolve Domain name) | | |-->[Amazon Route53] | | | (Get contents) | | | |-->[Amazon S3 contents Bucket] | | Users---|-->[AWS CloudFront]--| | | | |-->[Amazon S3 Logging Bucket] | | | (Save Logs) | | |-->[AWS Certificate Manager] | |---------------------------------------------------------------------|","title":"Jekyll Website"},{"location":"Support/jekyll-website/#aws-cicd-pipeline","text":"[CloudFormation Template] |---------------------------------------------------------------------------| |[NewCodeChange] --> [Pushed to master branch] --> [AWS CodePipeline] | | | | | | | | Published to S3 <-- Build Jekyll Website <-- [AWS CodeBuild] | | | | | | | | Lambda function triggered to invalidate CloudFront Distribution | | (Old files removed from CloudFront edge before the actual expiration) | |---------------------------------------------------------------------------|","title":"AWS CI/CD Pipeline"},{"location":"Support/jekyll-website/#running-jekyll-in-docker-container","text":"Jekyll might require a different version of ruby and gem than the one installed on your computer and when working on MacOSX, it is difficult to work with a different version than the one already pre-installed. Rather than corrupting the PATH and ruby versions, it is best to run jekyll in a container. Here are the following steps to setup and work with jekyll in a container. $ echo 'JEKYLL_VERSION=4.0' >> ~/.bash_profile Create a project directory to hold the files and directories for jekyll. $ mkdir -p ~/jekyll-dev/new-blog $ cd ~/jekyll-dev/new-blog Run docker using the following command: $ docker run --rm --volume = \" $PWD :/srv/jekyll\" -it jekyll/jekyll: $JEKYLL_VERSION jekyll new . To build the site, use the following command: $ docker run --rm --volume = \" $PWD :/srv/jekyll\" -it jekyll/jekyll: $JEKYLL_VERSION jekyll build To run the site locally: $ docker run --name deepgorthi --volume = \" $PWD :/srv/jekyll\" -p 4000 :4000 -it jekyll/jekyll: $JEKYLL_VERSION jekyll serve --watch --drafts The website can be locally accessed using http://localhost:4000 To execute commands in the container: $ docker exec -ti deepgorthi gem install \"jekyll-theme-example\" Executing a shell in the container: $ docker exec -ti deepgorthi /bin/sh Removing container: docker rm -f deepgorthi After making any changes to the jekyll website, you can either build jekyll within docker using: $ docker run --name deepgorthi --volume = \" $PWD :/srv/jekyll\" -p 4000 :4000 -it jekyll/jekyll: $JEKYLL_VERSION jekyll serve --watch --drafts Or, you can restart docker container using: $ docker restart deepgorthi","title":"Running Jekyll in Docker container"},{"location":"Support/mac-setup/","text":"MacOS Setup \u00b6 # Make sure Xcode Command Line tools are installed $ xcode-select --install # Install Homebrew by running this command $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" # Add the following line to ~/.profile $ echo \"export PATH='/usr/local/opt/python/libexec/bin: $PATH '\" >> ~/.profile ; source ~/.profile # Install Docker $ brew install docker # Install Python3 $ brew install python3 # Install Pipenv $ pip install --user pipenv # When you cd into a directory containing a .env, direnv automagically activates the environment. $ brew install direnv # Install wget with IRI support. $ brew install wget --with-iri # Install awscli and configure AWS with Secret Access Keys $ pip3 install awscli $ aws configure # Install boto3 $ pip3 install boto3 # Install [AWS Shell](https://github.com/awslabs/aws-shell) $ pip3 install aws-shell Virtualenv \u00b6 Virtualenv is a tool that creates an isolated Python environment for each of your projects. For a particular project, instead of installing required packages globally, it is best to install them in an isolated folder in the project (say a folder named venv), that will be managed by virtualenv. The advantage is that different projects might require different versions of packages, and it would be hard to manage that if you install packages globally. Installation $ pip3 install virtualenv Usage Let's say you have a project in a directory called myproject. To set up virtualenv for that project: cd myproject/ virtualenv venv --distribute If you want your virtualenv to also inherit globally installed packages (like IPython or Numpy mentioned above), use: virtualenv venv --distribute --system-site-packages These commands create a venv subdirectory in your project where everything is installed. You need to activate it first though (in every terminal where you are working on your project): source venv/bin/activate You should see a (venv) appear at the beginning of your terminal prompt indicating that you are working inside the virtualenv. Now when you install something: pip3 install <package> It will get installed in the venv folder, and not conflict with other projects. Important : Remember to add venv to your project's .gitignore file so you don't include all of that in your source code! Virtualenvwrapper \u00b6 Virtualenvwrapper is a set of extensions that includes wrappers for creating and deleting virtual environments and otherwise managing your development workflow, making it easier to work on more than one project at a time without introducing conflicts in their dependencies. Main features include: Organizes all of your virtual environments in one place. Wrappers for managing your virtual environments (create, delete, copy). Use a single command to switch between environments. Tab completion for commands that take a virtual environment as argument. Installation pip3 install virtualenvwrapper Usage Create a new virtual environment. When you create a new environment it automatically becomes the active environment: mkvirtualenv [env name] Remove an existing virtual environment. The environment must be deactivated (see below) before it can be removed: rmvirtualenv [env name] Activate a virtual environment. Will also list all existing virtual environments if no argument is passed: workon [env name] Deactivate the currently active virtual environment. Note that workonwill automatically deactivate the current environment before activating a new one: deactivate Adding SSH key to GitHub \u00b6 # To generate and add SSH keys to github, first create a ssh key with passphrase and it will be placed in ~/.ssh $ ssh-keygen -t rsa -b 4096 -C \"my@email.com\" # Run `ssh-agent` in the background $ eval \" $( ssh-agent -s ) \" # Create a config file and add the following lines to the file for automatically loading keys and storing passphrase in keychain $ vi ~/.ssh/config Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa # Add SSH private key to ssh-agent $ ssh-add -K ~/.ssh/id_rsa Enter passphrase for /Users/pradeepgorthi/.ssh/id_rsa: Identity added: /Users/pradeepgorthi/.ssh/id_rsa # Copy the public key to add it to GitHub account via the website in the Settings section. $ pbcopy < ~/.ssh/id_rsa.pub # Check if it is working $ ssh -T git@github.com The authenticity of host 'github.com (192.30.253.112)' can ' t be established. RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8. Are you sure you want to continue connecting ( yes/no ) ? yes","title":"MacOS Setup"},{"location":"Support/mac-setup/#macos-setup","text":"# Make sure Xcode Command Line tools are installed $ xcode-select --install # Install Homebrew by running this command $ /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" # Add the following line to ~/.profile $ echo \"export PATH='/usr/local/opt/python/libexec/bin: $PATH '\" >> ~/.profile ; source ~/.profile # Install Docker $ brew install docker # Install Python3 $ brew install python3 # Install Pipenv $ pip install --user pipenv # When you cd into a directory containing a .env, direnv automagically activates the environment. $ brew install direnv # Install wget with IRI support. $ brew install wget --with-iri # Install awscli and configure AWS with Secret Access Keys $ pip3 install awscli $ aws configure # Install boto3 $ pip3 install boto3 # Install [AWS Shell](https://github.com/awslabs/aws-shell) $ pip3 install aws-shell","title":"MacOS Setup"},{"location":"Support/mac-setup/#virtualenv","text":"Virtualenv is a tool that creates an isolated Python environment for each of your projects. For a particular project, instead of installing required packages globally, it is best to install them in an isolated folder in the project (say a folder named venv), that will be managed by virtualenv. The advantage is that different projects might require different versions of packages, and it would be hard to manage that if you install packages globally. Installation $ pip3 install virtualenv Usage Let's say you have a project in a directory called myproject. To set up virtualenv for that project: cd myproject/ virtualenv venv --distribute If you want your virtualenv to also inherit globally installed packages (like IPython or Numpy mentioned above), use: virtualenv venv --distribute --system-site-packages These commands create a venv subdirectory in your project where everything is installed. You need to activate it first though (in every terminal where you are working on your project): source venv/bin/activate You should see a (venv) appear at the beginning of your terminal prompt indicating that you are working inside the virtualenv. Now when you install something: pip3 install <package> It will get installed in the venv folder, and not conflict with other projects. Important : Remember to add venv to your project's .gitignore file so you don't include all of that in your source code!","title":"Virtualenv"},{"location":"Support/mac-setup/#virtualenvwrapper","text":"Virtualenvwrapper is a set of extensions that includes wrappers for creating and deleting virtual environments and otherwise managing your development workflow, making it easier to work on more than one project at a time without introducing conflicts in their dependencies. Main features include: Organizes all of your virtual environments in one place. Wrappers for managing your virtual environments (create, delete, copy). Use a single command to switch between environments. Tab completion for commands that take a virtual environment as argument. Installation pip3 install virtualenvwrapper Usage Create a new virtual environment. When you create a new environment it automatically becomes the active environment: mkvirtualenv [env name] Remove an existing virtual environment. The environment must be deactivated (see below) before it can be removed: rmvirtualenv [env name] Activate a virtual environment. Will also list all existing virtual environments if no argument is passed: workon [env name] Deactivate the currently active virtual environment. Note that workonwill automatically deactivate the current environment before activating a new one: deactivate","title":"Virtualenvwrapper"},{"location":"Support/mac-setup/#adding-ssh-key-to-github","text":"# To generate and add SSH keys to github, first create a ssh key with passphrase and it will be placed in ~/.ssh $ ssh-keygen -t rsa -b 4096 -C \"my@email.com\" # Run `ssh-agent` in the background $ eval \" $( ssh-agent -s ) \" # Create a config file and add the following lines to the file for automatically loading keys and storing passphrase in keychain $ vi ~/.ssh/config Host * AddKeysToAgent yes UseKeychain yes IdentityFile ~/.ssh/id_rsa # Add SSH private key to ssh-agent $ ssh-add -K ~/.ssh/id_rsa Enter passphrase for /Users/pradeepgorthi/.ssh/id_rsa: Identity added: /Users/pradeepgorthi/.ssh/id_rsa # Copy the public key to add it to GitHub account via the website in the Settings section. $ pbcopy < ~/.ssh/id_rsa.pub # Check if it is working $ ssh -T git@github.com The authenticity of host 'github.com (192.30.253.112)' can ' t be established. RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8. Are you sure you want to continue connecting ( yes/no ) ? yes","title":"Adding SSH key to GitHub"},{"location":"Support/mkdocs-website/","text":"MkDocs Website Setup \u00b6 Installation \u00b6 Install MkDocs using: pip3 install mkdocs Once MkDocs is installed, this material theme can be installed using: pip install mkdocs mkdocs-material pymdown-extensions pygments mkdocs-git-revision-date-localized-plugin mkdocs-minify-plugin Alternatively, we can use Docker by pulling the image with all dependencies included from Docker repo and running the Docker image locally in the root folder of the project: docker pull squidfunk/mkdocs-material docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material Build and Serve \u00b6 After the site is configured, it can be served using: mkdocs serve The site can be accessed locally using: http://localhost:8000 Deployment \u00b6 To deploy the site to GH pages, we can run this: mkdocs gh-deploy For further configuration and other support related documentaiton, here is the best source TravisCI \u00b6 The site is built using TravisCI. language : python python : 3.6 branches : master install : - pip install mkdocs mkdocs-material pymdown-extensions pygments mkdocs-git-revision-date-localized-plugin mkdocs-minify-plugin # Install the required dependencies script : true before_deploy : - mkdocs build --verbose --clean --strict deploy : provider : pages skip_cleanup : true github_token : $github_token local_dir : site on : branch : master","title":"MkDocs Website Setup"},{"location":"Support/mkdocs-website/#mkdocs-website-setup","text":"","title":"MkDocs Website Setup"},{"location":"Support/mkdocs-website/#installation","text":"Install MkDocs using: pip3 install mkdocs Once MkDocs is installed, this material theme can be installed using: pip install mkdocs mkdocs-material pymdown-extensions pygments mkdocs-git-revision-date-localized-plugin mkdocs-minify-plugin Alternatively, we can use Docker by pulling the image with all dependencies included from Docker repo and running the Docker image locally in the root folder of the project: docker pull squidfunk/mkdocs-material docker run --rm -it -p 8000:8000 -v ${PWD}:/docs squidfunk/mkdocs-material","title":"Installation"},{"location":"Support/mkdocs-website/#build-and-serve","text":"After the site is configured, it can be served using: mkdocs serve The site can be accessed locally using: http://localhost:8000","title":"Build and Serve"},{"location":"Support/mkdocs-website/#deployment","text":"To deploy the site to GH pages, we can run this: mkdocs gh-deploy For further configuration and other support related documentaiton, here is the best source","title":"Deployment"},{"location":"Support/mkdocs-website/#travisci","text":"The site is built using TravisCI. language : python python : 3.6 branches : master install : - pip install mkdocs mkdocs-material pymdown-extensions pygments mkdocs-git-revision-date-localized-plugin mkdocs-minify-plugin # Install the required dependencies script : true before_deploy : - mkdocs build --verbose --clean --strict deploy : provider : pages skip_cleanup : true github_token : $github_token local_dir : site on : branch : master","title":"TravisCI"},{"location":"Support/ssh-key-pair/","text":"SSH Key pair \u00b6 This is on Linux. Create SSH key \u00b6 $ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key ( /home/demo/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /home/demo/.ssh/id_rsa. Your public key has been saved in /home/demo/.ssh/id_rsa.pub. The key fingerprint is: 4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@a The key ' s randomart image is: +-- [ RSA 2048 ] ----+ | .oo. | | . o.E | | + . o | | . = = . | | = S = . | | o + = + | | . o + o . | | . o | | | +-----------------+ The public key is now located in /home/demo/.ssh/id_rsa.pub. The private key (identification) is now located in /home/demo/.ssh/id_rsa. Copy Public Key \u00b6 $ ssh-copy-id <remote-user>@<remote-server-IP> /usr/bin/ssh-copy-id: INFO: Source of key ( s ) to be installed: \"~/.ssh/id_rsa.pub\" /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key ( s ) , to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key ( s ) remain to be installed -- if you are prompted now it is to install the new keys Password: Number of key ( s ) added: 1 Now try logging into the machine, with: \"ssh <remote-user>@<remote-server-IP>\" and check to make sure that only the key ( s ) you wanted were added.","title":"SSH Key pair"},{"location":"Support/ssh-key-pair/#ssh-key-pair","text":"This is on Linux.","title":"SSH Key pair"},{"location":"Support/ssh-key-pair/#create-ssh-key","text":"$ ssh-keygen -t rsa Generating public/private rsa key pair. Enter file in which to save the key ( /home/demo/.ssh/id_rsa ) : Enter passphrase ( empty for no passphrase ) : Enter same passphrase again: Your identification has been saved in /home/demo/.ssh/id_rsa. Your public key has been saved in /home/demo/.ssh/id_rsa.pub. The key fingerprint is: 4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@a The key ' s randomart image is: +-- [ RSA 2048 ] ----+ | .oo. | | . o.E | | + . o | | . = = . | | = S = . | | o + = + | | . o + o . | | . o | | | +-----------------+ The public key is now located in /home/demo/.ssh/id_rsa.pub. The private key (identification) is now located in /home/demo/.ssh/id_rsa.","title":"Create SSH key"},{"location":"Support/ssh-key-pair/#copy-public-key","text":"$ ssh-copy-id <remote-user>@<remote-server-IP> /usr/bin/ssh-copy-id: INFO: Source of key ( s ) to be installed: \"~/.ssh/id_rsa.pub\" /usr/bin/ssh-copy-id: INFO: attempting to log in with the new key ( s ) , to filter out any that are already installed /usr/bin/ssh-copy-id: INFO: 1 key ( s ) remain to be installed -- if you are prompted now it is to install the new keys Password: Number of key ( s ) added: 1 Now try logging into the machine, with: \"ssh <remote-user>@<remote-server-IP>\" and check to make sure that only the key ( s ) you wanted were added.","title":"Copy Public Key"}]}